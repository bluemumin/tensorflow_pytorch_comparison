{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ae783b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T08:27:49.047321Z",
     "start_time": "2024-02-19T08:27:42.124670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n",
      "torch 2.2.0+cpu\n",
      "numpy 1.19.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print('torch', torch.__version__)\n",
    "print('numpy', np.__version__)\n",
    "\n",
    "import itertools\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from text_loader import TextDataset\n",
    "\n",
    "#따로 생성된 py파일\n",
    "import seq2seq_models as sm\n",
    "from seq2seq_models import str2tensor, EOS_token, SOS_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803e3b8",
   "metadata": {},
   "source": [
    "# seq to seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c9dda7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T07:53:43.733253Z",
     "start_time": "2024-02-19T07:53:43.729488Z"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "N_LAYERS = 1\n",
    "BATCH_SIZE = 3\n",
    "N_EPOCH = 20\n",
    "N_CHARS = 128  # ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf17a6ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T07:53:44.088399Z",
     "start_time": "2024-02-19T07:53:44.070508Z"
    }
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    word_input = str2tensor('hello')\n",
    "    encoder_outputs, encoder_hidden = encoder(word_input, encoder_hidden)\n",
    "    print(encoder_outputs)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    word_target = str2tensor('pytorch')\n",
    "    for cc in range(len(word_target)):\n",
    "        decoder_output, decoder_hidden = decoder(word_target[cc], decoder_hidden)\n",
    "    print(decoder_output.size(), decoder_hidden.size())\n",
    "    \n",
    "def train(src, target):\n",
    "    src_var = str2tensor(src)\n",
    "    target_var = str2tensor(target, eos = True)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n",
    "    \n",
    "    hidden = encoder_hidden\n",
    "    loss = 0\n",
    "    \n",
    "    for cc in range(len(target_var)):\n",
    "        token = target_var[cc - 1] if cc else str2tensor(SOS_token)\n",
    "        output, hidden = decoder(token, hidden)\n",
    "        \n",
    "        \n",
    "        predicted_char_index = torch.argmax(output)\n",
    "        target_char_index = torch.tensor(target_var[cc])\n",
    "        loss += criterion(output.view(1, -1), target_char_index.view(1))\n",
    "        \n",
    "        decoder_in = target_char_index\n",
    "        \n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item() / len(target_var)  # loss.item()을 사용하여 손실 값을 가져옴\n",
    "        \n",
    "#         loss += criterion(output, target_var[cc])\n",
    "        \n",
    "#     encoder.zero_grad()\n",
    "#     decoder.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     return loss.data[0] / len(target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6d3eb8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T07:53:44.325257Z",
     "start_time": "2024-02-19T07:53:44.308785Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate(enc_input = 'example.sentence.', predict_len = 100, temperature = 0.9):\n",
    "    input_var = str2tensor(enc_input)\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n",
    "    \n",
    "    hidden = encoder_hidden\n",
    "    \n",
    "    predicted = ''\n",
    "    dec_input = str2tensor(SOS_token)\n",
    "    for cc in range(predict_len):\n",
    "        output, hidden = decoder(dec_input, hidden)\n",
    "        \n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        if top_i == EOS_token:\n",
    "            break\n",
    "            \n",
    "        predicted_char = chr(top_i.item())\n",
    "        predicted += predicted_char\n",
    "        \n",
    "        dec_input = str2tensor(predicted_char)\n",
    "        \n",
    "    return enc_input, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2fb05c5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T07:53:44.702195Z",
     "start_time": "2024-02-19T07:53:44.672573Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(128, 128)\n",
      "  (gru): GRU(128, 128)\n",
      ") DecoderRNN(\n",
      "  (embedding): Embedding(128, 128)\n",
      "  (gru): GRU(128, 128)\n",
      "  (out): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "tensor([[[ 0.1351,  0.0793,  0.2339,  0.3450, -0.3748,  0.1893,  0.0755,\n",
      "          -0.3821, -0.1120, -0.3857,  0.0443, -0.1588,  0.0250, -0.1579,\n",
      "          -0.1055,  0.3679,  0.1870,  0.1374, -0.5314,  0.4405,  0.2066,\n",
      "           0.1151,  0.0812, -0.0775, -0.4187,  0.0508,  0.2170, -0.2497,\n",
      "           0.2012, -0.1753,  0.1420, -0.3278, -0.1177, -0.0975, -0.2767,\n",
      "          -0.0385, -0.0407, -0.5524,  0.0590, -0.2769, -0.2081,  0.2547,\n",
      "           0.3303,  0.0886,  0.1457, -0.4725,  0.1074, -0.0768,  0.1089,\n",
      "           0.1945, -0.0785, -0.4157, -0.3862, -0.4529,  0.5077,  0.2001,\n",
      "           0.1002,  0.0997, -0.1742, -0.3428,  0.1969, -0.1498,  0.2929,\n",
      "          -0.1584,  0.0887,  0.3462,  0.1456,  0.1522,  0.1640, -0.0381,\n",
      "           0.3080, -0.0349, -0.2373, -0.1280, -0.3739,  0.2466,  0.4036,\n",
      "          -0.3744,  0.0658, -0.3853,  0.4096,  0.2465, -0.3177,  0.1404,\n",
      "           0.2885,  0.3089,  0.2193, -0.3923, -0.1083,  0.3908,  0.4861,\n",
      "          -0.0482,  0.3104,  0.0467,  0.0071, -0.2943, -0.2199, -0.3232,\n",
      "           0.0678, -0.1257, -0.2179,  0.2063, -0.1945, -0.1235,  0.0083,\n",
      "          -0.1303,  0.0082, -0.3938, -0.1178,  0.0332, -0.0233,  0.0177,\n",
      "           0.2130,  0.0362, -0.0128, -0.1435, -0.0190, -0.1345,  0.0573,\n",
      "           0.1969,  0.1237, -0.0286, -0.1722, -0.1201, -0.0910,  0.1246,\n",
      "           0.2857, -0.0775]],\n",
      "\n",
      "        [[ 0.0372,  0.1742,  0.1080, -0.0917, -0.1546,  0.3353,  0.2333,\n",
      "          -0.3446, -0.1550, -0.2906,  0.1524, -0.0041, -0.0514, -0.1294,\n",
      "           0.0074,  0.4905, -0.0404,  0.1124, -0.2102,  0.4747, -0.0385,\n",
      "           0.1974,  0.1389, -0.2158, -0.0676,  0.3478, -0.0578, -0.2356,\n",
      "          -0.0089, -0.3320,  0.5136, -0.5012, -0.0535, -0.0693, -0.5245,\n",
      "           0.2433, -0.1792, -0.0695,  0.0624, -0.0026, -0.1817, -0.2080,\n",
      "           0.2156, -0.1559,  0.3401, -0.5929,  0.1332, -0.3383,  0.4202,\n",
      "          -0.0552, -0.1106, -0.2400, -0.0771, -0.1444,  0.2890, -0.2217,\n",
      "          -0.4826, -0.3684,  0.4102, -0.2390, -0.0910, -0.0251,  0.3448,\n",
      "           0.0198, -0.0967, -0.2605,  0.2952, -0.1308, -0.1174, -0.3866,\n",
      "           0.3837,  0.0751, -0.0491,  0.1856,  0.3761, -0.1936,  0.3401,\n",
      "          -0.2738, -0.0897, -0.2582,  0.3968,  0.3141,  0.1026,  0.1588,\n",
      "           0.4018, -0.0910,  0.3118, -0.2400,  0.2118,  0.3767,  0.5627,\n",
      "          -0.3345,  0.1579, -0.2164, -0.0166, -0.2733,  0.0644, -0.3380,\n",
      "          -0.0033,  0.0176, -0.3879, -0.1395,  0.0765, -0.2635,  0.3750,\n",
      "           0.1068,  0.2873, -0.0900,  0.1632, -0.1308, -0.1565, -0.1302,\n",
      "           0.2632, -0.1968, -0.0691,  0.0068, -0.1363, -0.4615,  0.1879,\n",
      "           0.0388,  0.0568, -0.3689, -0.0408, -0.3242, -0.0071, -0.1741,\n",
      "          -0.0486,  0.1437]],\n",
      "\n",
      "        [[ 0.5301,  0.3846,  0.4758, -0.1247,  0.3035, -0.1103,  0.2994,\n",
      "          -0.4205, -0.5172, -0.2137,  0.3135, -0.4239,  0.1451, -0.1791,\n",
      "          -0.0084, -0.5293,  0.0173, -0.1444,  0.4009,  0.2591, -0.0331,\n",
      "           0.4197,  0.1867, -0.5130, -0.0331,  0.1824, -0.0691, -0.1288,\n",
      "          -0.1662,  0.2632,  0.4274,  0.2162, -0.0286, -0.2159, -0.6241,\n",
      "          -0.3261, -0.2998,  0.1116, -0.1081, -0.0103, -0.3253, -0.2120,\n",
      "           0.0937, -0.3359,  0.2571, -0.4759,  0.3505, -0.3966,  0.1823,\n",
      "           0.1022, -0.3419, -0.1619,  0.0935, -0.3280,  0.0372,  0.2799,\n",
      "          -0.5269,  0.0617, -0.2108, -0.4723,  0.2829, -0.1509,  0.4316,\n",
      "           0.4078,  0.2965, -0.2736,  0.0102, -0.1548, -0.1904, -0.5252,\n",
      "          -0.2255,  0.3201, -0.0066, -0.0170,  0.4550, -0.2058,  0.1904,\n",
      "           0.5796, -0.1213,  0.0862,  0.2955,  0.0657, -0.3174, -0.1541,\n",
      "           0.3069,  0.1933,  0.3504, -0.3043,  0.1159, -0.2777,  0.2382,\n",
      "          -0.2141, -0.2001,  0.3934,  0.0581, -0.4591, -0.2256, -0.3389,\n",
      "          -0.2210, -0.2293,  0.4808, -0.2399,  0.2613, -0.1797,  0.2482,\n",
      "           0.3033,  0.3788,  0.1909, -0.1461,  0.2573, -0.2146,  0.4837,\n",
      "          -0.0053, -0.1549,  0.1406,  0.2282,  0.1242, -0.0255,  0.0461,\n",
      "           0.0851, -0.3178, -0.4621,  0.2093, -0.3717,  0.0426, -0.2474,\n",
      "           0.3948,  0.3531]],\n",
      "\n",
      "        [[ 0.6230,  0.4774,  0.5839, -0.1054,  0.5159, -0.3321,  0.4015,\n",
      "          -0.4855, -0.6422, -0.2355,  0.3540, -0.5940,  0.2825, -0.2092,\n",
      "          -0.0258, -0.7210,  0.0086, -0.2938,  0.6530,  0.0117, -0.0715,\n",
      "           0.5338,  0.2329, -0.6606, -0.0426,  0.0886, -0.1004, -0.0860,\n",
      "          -0.2455,  0.5022,  0.3886,  0.5491, -0.0935, -0.3264, -0.6906,\n",
      "          -0.4276, -0.3921,  0.1541, -0.2414, -0.1477, -0.4217, -0.2250,\n",
      "           0.0658, -0.3793,  0.1155, -0.3677,  0.4889, -0.4399,  0.0471,\n",
      "           0.2114, -0.4814, -0.1406,  0.1589, -0.4613, -0.0943,  0.4327,\n",
      "          -0.5396,  0.2362, -0.5038, -0.5798,  0.3950, -0.1198,  0.4908,\n",
      "           0.6233,  0.4429, -0.2199, -0.1117, -0.1432, -0.2354, -0.5651,\n",
      "          -0.4271,  0.3727,  0.0143, -0.1881,  0.5168, -0.2172,  0.1526,\n",
      "           0.6778, -0.1296,  0.3059,  0.1762, -0.0097, -0.4911, -0.2238,\n",
      "           0.2790,  0.3435,  0.3587, -0.3591,  0.0479, -0.5347,  0.1090,\n",
      "           0.0688, -0.3791,  0.6060,  0.0882, -0.5028, -0.4305, -0.3463,\n",
      "          -0.3710, -0.3575,  0.6660, -0.3016,  0.3625, -0.1042,  0.1295,\n",
      "           0.4511,  0.4375,  0.3295, -0.1699,  0.4665, -0.2375,  0.6787,\n",
      "          -0.0432, -0.1119,  0.1650,  0.3629,  0.1626,  0.1072, -0.0627,\n",
      "           0.1468, -0.4506, -0.5011,  0.3018, -0.4342,  0.0608, -0.3257,\n",
      "           0.6272,  0.4894]],\n",
      "\n",
      "        [[ 0.3655,  0.6609,  0.4341, -0.3845,  0.3238, -0.3538,  0.4963,\n",
      "          -0.2999, -0.2700,  0.0947,  0.3710, -0.6070,  0.1323, -0.3817,\n",
      "          -0.3487, -0.2922, -0.3539, -0.2438,  0.2451,  0.1439,  0.0244,\n",
      "           0.1873,  0.2638, -0.4998,  0.4927,  0.1665, -0.4813,  0.0772,\n",
      "          -0.2685,  0.3245,  0.2058,  0.3293, -0.1944, -0.5210, -0.5729,\n",
      "          -0.5894,  0.4165, -0.0897,  0.1907,  0.0779, -0.4450, -0.0158,\n",
      "           0.1060, -0.2264,  0.2730,  0.1516,  0.5728, -0.0871,  0.1562,\n",
      "           0.2700, -0.2733,  0.0689,  0.4744, -0.3004, -0.0485,  0.7400,\n",
      "          -0.5485, -0.1040, -0.0686,  0.1249,  0.3423, -0.2073,  0.1375,\n",
      "           0.2611,  0.0987, -0.1981, -0.2623, -0.3817, -0.4645, -0.2635,\n",
      "           0.0075,  0.6803,  0.1013, -0.2422,  0.4587, -0.0129,  0.0087,\n",
      "           0.1252,  0.2473,  0.0962,  0.0526, -0.0422,  0.0224, -0.6390,\n",
      "           0.1197,  0.2874,  0.4716, -0.3167,  0.1559, -0.5228,  0.4602,\n",
      "           0.3851, -0.4832,  0.4493,  0.0546, -0.2136, -0.2196, -0.0321,\n",
      "          -0.5701,  0.0813,  0.2162, -0.5367, -0.0792,  0.1022, -0.2147,\n",
      "           0.4440,  0.0913,  0.2620,  0.0255,  0.4012, -0.1819,  0.0242,\n",
      "          -0.1417,  0.1242,  0.0917,  0.2615,  0.2977, -0.0391, -0.3854,\n",
      "           0.1647,  0.0111, -0.2046, -0.1892, -0.4114, -0.2531, -0.6591,\n",
      "           0.2605, -0.0610]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 128]) torch.Size([1, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\n",
    "decoder = sm.DecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    decoder.cuda()\n",
    "    encoder.cuda()\n",
    "print(encoder, decoder)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e84a78ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T08:03:14.780961Z",
     "start_time": "2024-02-19T07:53:45.036020Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 20 epochs...\n",
      "[(1 5%) 4.8377]\n",
      "('godgrantwenevermayhaveneedofyou!', '(]\\x1dl\\x12,{Bq1r1D\\x190\\x0eRUB0foC\\x05=Po^G<L\\x07\\x0cTcK.5j)jVT\\x15\\tO`\\r?x/_C}\\x08#!#B2U2g\\t,@\\x15\\x0bQl\\x0bgO\\x08IGd-+\\x1bMTkF_H\\x7fYaHx\\x7fjW\\x003\\x1eYx\\x10') \n",
      "\n",
      "('example.sentence.', '@M\\x18\\x7f\\x05\\nR!t,M>7pz') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10331\\AppData\\Local\\Temp/ipykernel_16308/2324192785.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_char_index = torch.tensor(target_var[cc])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1 5%) 2.5302]\n",
      "(\"myworkhathyetnotwarm'dme:fareyouwell:\", 'I\\x05O wed.') \n",
      "\n",
      "('example.sentence.', 'TIe oome qhou nagur') \n",
      "\n",
      "[(1 5%) 2.8125]\n",
      "('thirdcitizen:', 'OOer llaat wonnt sis misacc in ofy gYes.') \n",
      "\n",
      "('example.sentence.', 'or me,') \n",
      "\n",
      "[(1 5%) 2.5918]\n",
      "('inallhisowndesires;nay,lethimchoose', 'Bo the nisot dest inane thae brtnse,s thehou.') \n",
      "\n",
      "('example.sentence.', 'Mom of mo fe fous shrin faris ond isch, wer the by fors force eas the ons') \n",
      "\n",
      "[(1 5%) 2.3654]\n",
      "('iwouldwithsuchperfectiongovern,sir,', 'Wourou, tond,') \n",
      "\n",
      "('example.sentence.', 'The Wouris bee,') \n",
      "\n",
      "[(1 5%) 2.1924]\n",
      "('nowprisonertothepalsy,chastisethee', 'Wreat werast wis.irst wore dit.') \n",
      "\n",
      "('example.sentence.', 'Tdere,') \n",
      "\n",
      "[(1 5%) 1.9845]\n",
      "('andyetnotmany.', 'Depy hesthawe theat ame delath') \n",
      "\n",
      "('example.sentence.', 'And your wiwlld.') \n",
      "\n",
      "[(1 5%) 2.5288]\n",
      "('andgeorge,ofclarence:warwick,asourself,', \"Lone herco comes'sn tarou heas hingst,\") \n",
      "\n",
      "('example.sentence.', \"Pet I my e'st an, bretinge, kor Byout the wCand thy zoudils foordst hik le it forlling the shis grad\") \n",
      "\n",
      "[(1 5%) 2.1101]\n",
      "('morethanihavesaid,lovingcountrymen,', 'Fo done pan ke iter and the nothahtoze.') \n",
      "\n",
      "('example.sentence.', 'In feun awt and tell') \n",
      "\n",
      "[(1 5%) 2.0752]\n",
      "('notsick,althoughihavetodowithdeath,', 'u the hess? your so eny whet?') \n",
      "\n",
      "('example.sentence.', 'Th hathens bece, bame por anaf') \n",
      "\n",
      "[(1 5%) 2.0295]\n",
      "('whathaveyoudone?behold,theheavensdoope,', 'Whe the of iliver mose, don erporce') \n",
      "\n",
      "('example.sentence.', 'her thou the sond are hased!') \n",
      "\n",
      "[(1 5%) 2.7227]\n",
      "('forconsulships?', 'Wor shit ay be of browngr:') \n",
      "\n",
      "('example.sentence.', 'or sinter her stshen! speans,') \n",
      "\n",
      "[(1 5%) 2.0321]\n",
      "(\"forthatourkingdom'searthshouldnotbesoil'd\", 'Hellemen it freder to nose tons icle; for seame; enters.') \n",
      "\n",
      "('example.sentence.', \"In winspas, to were my hea'mn fno.\") \n",
      "\n",
      "[(1 5%) 2.0636]\n",
      "(\"ifonthefirst,howheinouse'eritbe,\", 'But do love comas in pits as breates forink fyo! oon:') \n",
      "\n",
      "('example.sentence.', 'GELI an, ake thou hostiar the to tod,') \n",
      "\n",
      "[(1 5%) 1.8460]\n",
      "('iwarrant,anishouldliveathousandyears,', 'I And loks dee om sher ane felly east wond low') \n",
      "\n",
      "('example.sentence.', 'Maven suter I streed the Reass our oll weir') \n",
      "\n",
      "[(1 5%) 1.9913]\n",
      "('iprithee,givenolimitstomytongue:', 'Pealinguchent, hatrane bepange,') \n",
      "\n",
      "('example.sentence.', 'Wetbous, to galus,e, cosett andbonmar,') \n",
      "\n",
      "[(1 5%) 2.0465]\n",
      "('barkloughlycastlecalltheythisathand?', \"A's his in mind all not wither thou with, Corperne; thou\") \n",
      "\n",
      "('example.sentence.', 'the Heirant nerartad--op;') \n",
      "\n",
      "[(1 5%) 1.6978]\n",
      "('tosunderhisthatwasthineenemy?', 'To thy all that one onel:') \n",
      "\n",
      "('example.sentence.', 'Whtin y in a his of ford.') \n",
      "\n",
      "[(1 5%) 2.5901]\n",
      "('takecold.holla,ho!curtis.', 'And him.') \n",
      "\n",
      "('example.sentence.', \"I's haade and pinsh soughe age hibad\\x15;\") \n",
      "\n",
      "[(1 5%) 2.4407]\n",
      "('andoccupationsperish!', 'And uple nine,') \n",
      "\n",
      "('example.sentence.', \"Weret of tay Afe trane wings pmo'dight dause\") \n",
      "\n",
      "[(1 5%) 1.6737]\n",
      "('why,thenminehonestyshallbemydower;', 'Whembler ssient form ouched Beat, I beets to benter;') \n",
      "\n",
      "('example.sentence.', \"Wert an there would 'teesitus Cather weld\") \n",
      "\n",
      "[(1 5%) 1.9640]\n",
      "('volumnia:', 'DDUCENTINCINA\\x10EL:') \n",
      "\n",
      "('example.sentence.', 'EStest tell ad sonrs dgofe wen Goo,') \n",
      "\n",
      "[(1 5%) 1.7130]\n",
      "('tobetterpurpose.', 'YoNk, his to the me it to way Slambvents,') \n",
      "\n",
      "('example.sentence.', \"my ply's hy praing\") \n",
      "\n",
      "[(1 5%) 1.6890]\n",
      "(\"for'tisthemindthatmakesthebodyrich;\", \"Fore I the lard, ashat of Seeme's a'\") \n",
      "\n",
      "('example.sentence.', 'Mieck!') \n",
      "\n",
      "[(1 5%) 1.7498]\n",
      "('hehathnofriendsbutwhoarefriendsforfear.', 'He a with the dighter it enare our and tood Cdood.') \n",
      "\n",
      "('example.sentence.', 'Have the livends:') \n",
      "\n",
      "[(1 5%) 1.6858]\n",
      "('ithankyou,sir.', 'It not goln blodsermends sosanced') \n",
      "\n",
      "('example.sentence.', 'Ed mang is all yoursedy') \n",
      "\n",
      "[(1 5%) 1.8788]\n",
      "('sopartwesadlyinthistroublousworld,', 'Son jurisy the Ands me the to thiss se plombwer') \n",
      "\n",
      "('example.sentence.', 'From.') \n",
      "\n",
      "[(1 5%) 2.0643]\n",
      "('toeslookthroughtheover-leather.', 'To wikown we now you hain.') \n",
      "\n",
      "('example.sentence.', 'Beet the wime beast my wapter.') \n",
      "\n",
      "[(1 5%) 2.0287]\n",
      "('callitnotpatience,gaunt;itisdespair:', 'Cind net the viman:') \n",
      "\n",
      "('example.sentence.', 'Moust thee in ery ablemen;') \n",
      "\n",
      "[(1 5%) 2.0718]\n",
      "('persuadethisrudewretchwillinglytodie.', 'Se will would beseliter, hrour Lenatoh.') \n",
      "\n",
      "('example.sentence.', 'entigemina.') \n",
      "\n",
      "[(1 5%) 1.7745]\n",
      "('andmakesomeprettymatchwithsheddingtears?', 'As diserst, penrace play this to soul') \n",
      "\n",
      "('example.sentence.', 'Ellad; all the coll,') \n",
      "\n",
      "[(1 5%) 2.0437]\n",
      "(\"that'only'camewellin.sir,listtome:\", 'LRORINGEE:') \n",
      "\n",
      "('example.sentence.', 'En; hemess golt the froor make?') \n",
      "\n",
      "[(1 5%) 1.0225]\n",
      "('queenmargaret:', 'UIt heave.') \n",
      "\n",
      "('example.sentence.', 'net,') \n",
      "\n",
      "[(1 5%) 0.7900]\n",
      "('duchessofyork:', 'DUCHES COIF YORK:') \n",
      "\n",
      "('example.sentence.', 'Evy be candy,') \n",
      "\n",
      "[(1 5%) 1.9584]\n",
      "('mayichangethesegarments?', 'Mersen hour. Yore is poth, curcined my ware.') \n",
      "\n",
      "('example.sentence.', 'Yiut. Bearld,') \n",
      "\n",
      "[(1 5%) 1.4638]\n",
      "('sinkintheground?ithoughtitwouldhavemounted.', 'Sintle con the endengy wourthn hercingely, and your says.') \n",
      "\n",
      "('example.sentence.', 'Ens your artiugh;') \n",
      "\n",
      "[(1 5%) 1.4179]\n",
      "('hisistheright,andthereforepardonme.', 'His no art will dou jucune all spear thentrod, be shime,') \n",
      "\n",
      "('example.sentence.', 'Your Kord your from to pons.') \n",
      "\n",
      "[(1 5%) 0.3459]\n",
      "('gloucester:', 'GLYUS_ES:') \n",
      "\n",
      "('example.sentence.', 'Sengess, my veake:') \n",
      "\n",
      "[(1 5%) 1.8270]\n",
      "('sir,hisstoutness', 'Scouen leed our hant bimfan; Yould Dreast the crave') \n",
      "\n",
      "('example.sentence.', 'uppearp? And there') \n",
      "\n",
      "[(1 5%) 1.8054]\n",
      "('forhere,ihope,beginsourlastingjoy.', \"Frone'll w lown in and warectuely coulp dis brone!\") \n",
      "\n",
      "('example.sentence.', 'Warn this loven your lord me!') \n",
      "\n",
      "[(1 5%) 0.8479]\n",
      "('queenmargaret:', 'QUEENG ELWANUS:') \n",
      "\n",
      "('example.sentence.', \"puty's petrorch; the rain talt.\") \n",
      "\n",
      "[(1 5%) 1.4154]\n",
      "('thesameyouarenot,which,foryourbestends,', \"The gence, no ravet thee mer'd make God air\") \n",
      "\n",
      "('example.sentence.', 'Erad, be rest, you would with be provess') \n",
      "\n",
      "[(1 5%) 1.9662]\n",
      "(\"somelayindeadmen'sskulls;and,inthoseholes\", 'sing your himers of in') \n",
      "\n",
      "('example.sentence.', 'beaces end') \n",
      "\n",
      "[(1 5%) 1.8228]\n",
      "('hasdeservedprison,thenaboundintears', 'Hath them and suinf his noles grace.') \n",
      "\n",
      "('example.sentence.', 'uf so, ofler knone;') \n",
      "\n",
      "[(1 5%) 1.7162]\n",
      "('marchon,andmarkkingrichardhowhelooks.', 'Most prow the ency lied you his headsed.') \n",
      "\n",
      "('example.sentence.', \"Exempence's to you and thy pasent'\") \n",
      "\n",
      "[(1 5%) 1.4767]\n",
      "('toknowourfurtherpleasureinthiscase,', 'To pooder with, Tuts when it.') \n",
      "\n",
      "('example.sentence.', 'Emen, tHere') \n",
      "\n",
      "[(1 5%) 0.5882]\n",
      "('autolycus:', 'AUTESTIO:') \n",
      "\n",
      "('example.sentence.', 'Evive to betand his, your that mers;') \n",
      "\n",
      "[(1 5%) 1.3984]\n",
      "('holdyourpeaces.', \"Hone the conged our ucable words and witle and make the lay'd.\") \n",
      "\n",
      "('example.sentence.', 'EScton intabley of blose,') \n",
      "\n",
      "[(1 5%) 1.7235]\n",
      "('letthemaccusemebyinvention,i', 'Pmyell,--if fathat your someny, aill; boding her,') \n",
      "\n",
      "('example.sentence.', 'ay soom? therese, holds') \n",
      "\n",
      "[(1 5%) 1.4915]\n",
      "('thatsheshallstillbecurstincompany.', 'That extrime; and to for fife,') \n",
      "\n",
      "('example.sentence.', 'Entray stramenor thou a bou, know ho man for') \n",
      "\n",
      "[(1 5%) 1.7187]\n",
      "('forcedbythetidetocombatwiththewind;', 'For youl how highman may be your loke! be in you hes?') \n",
      "\n",
      "('example.sentence.', 'E! yet strely, in longes, lot?') \n",
      "\n",
      "[(1 5%) 0.7506]\n",
      "('miranda:', 'MINCENTIO:') \n",
      "\n",
      "('example.sentence.', 'En, How would, no stand;') \n",
      "\n",
      "[(1 5%) 1.5414]\n",
      "('edwardandclarence.o,whatcausehavei,', 'uncure knebie have not?') \n",
      "\n",
      "('example.sentence.', 'Ay, heng.') \n",
      "\n",
      "[(1 5%) 1.4507]\n",
      "('withpromiseofhissister,andwhatelse,', 'With god diso crue') \n",
      "\n",
      "('example.sentence.', 'Edwer! I death a pracuse these:') \n",
      "\n",
      "[(1 5%) 1.6622]\n",
      "('suchadependencyofthingonthing,', 'Suncreciuns out lord will sed did allow to and your carest wils,') \n",
      "\n",
      "('example.sentence.', 'Eepal am the pawnes and will') \n",
      "\n",
      "[(1 5%) 0.2926]\n",
      "('vincentio:', 'VINCENTET:') \n",
      "\n",
      "('example.sentence.', 'ED her sut san and by my light,') \n",
      "\n",
      "[(1 5%) 2.1102]\n",
      "('theeffectsofhisfondjealousiessogrieving', 'The nike and you by to him.') \n",
      "\n",
      "('example.sentence.', \"Eant's the drack heng wone the sserjent a reak'd:\") \n",
      "\n",
      "[(1 5%) 1.6980]\n",
      "('hortensiowillbequitwiththeebychanging.', 'His with you vose he my greats conue the face.') \n",
      "\n",
      "('example.sentence.', 'Enes, My so jother.') \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1 5%) 2.7324]\n",
      "('apothecary:', 'As grove thou know no take exwerger,') \n",
      "\n",
      "('example.sentence.', \"Eform and unkingwen blood freate's the would\") \n",
      "\n",
      "[(1 5%) 0.3548]\n",
      "('warwick:', 'WARDID:') \n",
      "\n",
      "('example.sentence.', 'deasty;') \n",
      "\n",
      "[(1 5%) 0.2634]\n",
      "('kingrichardii:', 'KING ERI EVINIO:') \n",
      "\n",
      "('example.sentence.', 'Metterime han butter') \n",
      "\n",
      "[(1 5%) 1.7410]\n",
      "('now,lords,takeleaveuntilwemeetagain,', 'Not make be blodch a our my fatigst netction jay.') \n",
      "\n",
      "('example.sentence.', 'neter as stracts now stand other') \n",
      "\n",
      "[(1 5%) 1.4609]\n",
      "('ifimightdiewithinthishour,ihavelived', 'I wones Ttrocire hime thut') \n",
      "\n",
      "('example.sentence.', 'my of washen,') \n",
      "\n",
      "[(1 5%) 1.5561]\n",
      "('andbetruekingindeed,thoubuttheshadow.', 'And beds, will a mutly one cather') \n",
      "\n",
      "('example.sentence.', 'Eence, and your fare mory') \n",
      "\n",
      "[(1 5%) 1.7577]\n",
      "('speak,goodcominius:', 'Sour Buck the mile bothy dose:') \n",
      "\n",
      "('example.sentence.', 'EDe mess tureers, genocker selve.') \n",
      "\n",
      "[(1 5%) 1.7021]\n",
      "('anymanthatcanwritemayansweraletter.', 'And the all son, be good me all more not a good?') \n",
      "\n",
      "('example.sentence.', 'Ekesal of your livess,') \n",
      "\n",
      "[(1 5%) 1.6939]\n",
      "(\"clearthemo'thecity.formyself,i'llput\", 'Coundy: I your dustle: wature.') \n",
      "\n",
      "('example.sentence.', 'and a come of the laws your cum.') \n",
      "\n",
      "[(1 5%) 1.1951]\n",
      "('perdita:', 'PERUVIOLAND:') \n",
      "\n",
      "('example.sentence.', 'are down. Than we picer you broan.') \n",
      "\n",
      "[(1 5%) 1.3818]\n",
      "('tallfellowofthyhandsandthatthouwiltbe', 'Tracy thou attenguit son the to your prody') \n",
      "\n",
      "('example.sentence.', 'Ecect, I whiscer.') \n",
      "\n",
      "[(1 5%) 0.1971]\n",
      "('warwick:', 'WARWIUS:') \n",
      "\n",
      "('example.sentence.', 'elbuindage the propt-ord,') \n",
      "\n",
      "[(1 5%) 2.0711]\n",
      "('thedustonantiquetimewouldlieunswept,', 'This resome it and hat thire yetter my fire') \n",
      "\n",
      "('example.sentence.', 'a sed? you should you are chawly lear.') \n",
      "\n",
      "[(1 5%) 0.0917]\n",
      "('dukevincentio:', 'DUKE OF IRAN:') \n",
      "\n",
      "('example.sentence.', 'Ene waretiond') \n",
      "\n",
      "[(1 5%) 0.4743]\n",
      "('claudio:', 'CLAUCIO:') \n",
      "\n",
      "('example.sentence.', 'Ear, and, as have Banry;') \n",
      "\n",
      "[(1 5%) 1.7723]\n",
      "(\"asheisproudtodo't.\", 'As to withy olding endery,') \n",
      "\n",
      "('example.sentence.', 'unalle;') \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16308/439165380.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_EPOCH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msrcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrcs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mii\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16308/2324192785.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(src, target)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcc\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr2tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSOS_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\only_studying\\seq2seq_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# input shape: S(=1) x B (=1) x I (input size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# Note: we run this one step at a time. (Sequence size = 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr = 0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(dataset = TextDataset(),\n",
    "                         batch_size = BATCH_SIZE,\n",
    "                         shuffle = True,\n",
    "                         )\n",
    "\n",
    "print(\"training for %d epochs...\" % N_EPOCH)\n",
    "for epoch in range(1, N_EPOCH + 1):\n",
    "    for ii, (srcs, targets) in enumerate(train_loader):\n",
    "        train_loss = train(srcs[0], targets[0])\n",
    "        \n",
    "        if ii % 100 == 0:\n",
    "            print('[(%d %d%%) %.4f]' %\n",
    "                  (epoch, epoch / N_EPOCH * 100, train_loss))\n",
    "            print(translate(srcs[0]), '\\n')\n",
    "            print(translate(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250cad3",
   "metadata": {},
   "source": [
    "# seq to seq with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed53835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T08:27:55.151606Z",
     "start_time": "2024-02-19T08:27:55.146619Z"
    }
   },
   "outputs": [],
   "source": [
    "from seq2seq_models import cuda_variable, str2tensor, EOS_token, SOS_token\n",
    "\n",
    "N_LAYERS = 1\n",
    "BATCH_SIZE = 3*64\n",
    "N_EPOCH = 20\n",
    "N_CHARS = 128  # ASCII\n",
    "HIDDEN_SIZE = N_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e6e3b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T08:27:55.502370Z",
     "start_time": "2024-02-19T08:27:55.482396Z"
    }
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    encoder_test = sm.EncoderRNN(10, 10, 2)\n",
    "    decoder_test = sm.AttnDecoderRNN(10,10,2)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        encoder_test.cuda()\n",
    "        decoder_test.cuda()\n",
    "        \n",
    "    encoder_hidden = encoder_test.init_hidden()\n",
    "    word_input = cuda.variable(torch.LongTensor([1,2,3]))\n",
    "    encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "    print(encoder_outputs,size())\n",
    "    \n",
    "    word_target = cuda_variable(torch.LongTensor([1,2,3]))\n",
    "    decoder_attns = torch.zeros(1,3,3)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for cc in ragne(len(word_target)):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder_test(word_target[cc],\n",
    "                                                                   decoder_hidden,\n",
    "                                                                   encoder_outputs)\n",
    "        \n",
    "        print(decoder_output.size(), decoder_hidden.size(), decoder_attn.size())\n",
    "        decoder_attns[0, cc] = decoder_attn.squeeze(0).cpu().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfccb3b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T08:27:55.918910Z",
     "start_time": "2024-02-19T08:27:55.885985Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(src, target):\n",
    "    loss = 0 \n",
    "    \n",
    "    src_var = str2tensor(src)\n",
    "    target_var = str2tensor(target, eos=True)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n",
    "    \n",
    "    hidden = encoder_hidden\n",
    "    \n",
    "    for cc in range(len(target_var)):\n",
    "        \n",
    "        token = target_var[cc - 1] if cc else str2tensor(SOS_token)\n",
    "        output, hidden, attention = decoder(token, hidden, encoder_outputs)\n",
    "        target_char_index = torch.tensor(target_var[cc])\n",
    "        loss += criterion(output.view(1, -1), target_char_index.view(1))\n",
    "        \n",
    "        decoder_in = target_char_index\n",
    "        \n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    return loss.item() / len(target_var)  # loss.item()을 사용하여 손실 값을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7895c784",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T08:27:56.318775Z",
     "start_time": "2024-02-19T08:27:56.296470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Translate the given input\n",
    "def translate(enc_input='example.sentence.', predict_len=100, temperature=0.9):\n",
    "    input_var = str2tensor(enc_input)\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n",
    "\n",
    "    hidden = encoder_hidden\n",
    "\n",
    "    predicted = ''\n",
    "    dec_input = str2tensor(SOS_token)\n",
    "    attentions = []\n",
    "    for cc in range(predict_len):\n",
    "        output, hidden, attention = decoder(dec_input, hidden, encoder_outputs)\n",
    "        # Sample from the network as a multi nominal distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        attentions.append(attention.view(-1).data.cpu().numpy().tolist())\n",
    "\n",
    "        # Stop at the EOS\n",
    "        if top_i is EOS_token:\n",
    "            break\n",
    "\n",
    "        predicted_char = chr(top_i)\n",
    "        predicted += predicted_char\n",
    "\n",
    "        dec_input = str2tensor(predicted_char)\n",
    "\n",
    "    return predicted, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ef70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53410c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa79b0e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T09:15:30.211925Z",
     "start_time": "2024-02-19T08:43:51.099619Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(128, 128)\n",
      "  (gru): GRU(128, 128)\n",
      ") AttnDecoderRNN(\n",
      "  (attn): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (embedding): Embedding(128, 128)\n",
      "  (gru): GRU(128, 128, dropout=0.1)\n",
      "  (out): Linear(in_features=256, out_features=128, bias=True)\n",
      ")\n",
      "Training for 20 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10331\\AppData\\Local\\Temp/ipykernel_18484/1417949097.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_char_index = torch.tensor(target_var[cc])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1/20 0.00%) 4.8780]\n",
      "dukevincentio: ^ii\u0007L,w\u0005Q3\u0011\u0011\u0019.J%;mPxo\u001alO\u0012\u0019\f",
      "o<Fg8<@2t\u001fo^\u0003m{92^ma\u0012O33CyU#DL\n",
      "\u000fX\u001bq\u0012I{5\u0002$\u001a\u001c",
      "\u0019uDY\u0011PO|,Q@7H-\u00144f\u000f\u0000>W \\m\n",
      "\u0007. \u001b>v\u0005@;_\f",
      "\u001e",
      "Z2 \u0002\u0004,f\u001365\u0002\f",
      "kv7`:%b`k9\u0011\u0000FYIKz\u0005|nQSJ9O\u00034}GU\u0016p\u0007&+~\u0002\t\n",
      "\n",
      "[(1/20 58.48%) 2.7747]\n",
      "thegodsforbid! phts to , dorn,r oor faes coo ta ghuwiwercccWe ss t ayt cekrtso sor ara, fi he ve t\u0001 cren hceusTo!\u0001  \\m\n",
      "example.sentence nhe tpger ofd ous o  olee y henc aidin\u0001\u0001fu tic iyrcn aher t tero nryou tesecocheP\u0001,\u0001txSW\u0001I te he  oo \n",
      "\n",
      "[(2/20 0.00%) 2.4903]\n",
      "whichne'ercamefromthelungs,buteventhus-- Whaade\ts thie, uaits\u0001thes,\u0001e nedegy ust som toy veuC\u0001ke ingofesty lch ;oyche'nd!\u0001' sind- an, -u sgof \\m\n",
      "example.sentence SLor:\u0001O meayecl, ind ther heved fatfnt saand ofeas have.\u0001\u0001hve th hwe\u0001shqincde, her these cong anos:\u0001 \n",
      "\n",
      "[(2/20 58.48%) 2.4151]\n",
      "cansetthedukeupindespiteofme. Hwe hed the s hithr aor thaves hes thoand ang he'ne whe, sies dond ithse wat  tofuw< bur knesr'd dii \\m\n",
      "example.sentence He row stor hen oll, and.\u0001 onl mes pont cend Rpays slo wer, dir co wast ark lorel afre ourn wd aroh  \n",
      "\n",
      "[(3/20 0.00%) 2.1588]\n",
      "andwhenthelionfawnsuponthelamb, Antes beroull ghel nall is thive mes:\u0001\u0001nlel hE yifim him\u0001.\u0001 hendipd w'dsedane:\u0001,\u0001 wim noupen,; bay,  \\m\n",
      "example.sentence  there anowor:\u0001 stecie'itd pegimes:\u0001 hast.\u0001\u0001; are.\u0001\u0001's thue, patte inf noomus weare hrery hie noupS: \n",
      "\n",
      "[(3/20 58.48%) 2.3708]\n",
      "forthemostgentleclaudio. Fon my, foche gontt.\u0001pinced:\u0001y, forYles fore tare in, shat\u0001njot artede\u0001el as ponds.\u0001steapore! heand  \\m\n",
      "example.sentence there ot dey.\u0001\u0001,\u0001 sathe.\u0001 thes nidiss yonronissanoge no the be,\u0001s the tha:\u0001 wo ardy\u0001, no! th fisgins \n",
      "\n",
      "[(4/20 0.00%) 2.5956]\n",
      "queenelizabeth: MOLESENI TEN:\u0001 oud wegh aut anop I miy, be\u0001s so lost bon themy, wisll she, Mi, limle nould acy:\u0001\u0001.\u0001, \\m\n",
      "example.sentence Ant an hou maf ly, foswl\u0001ell my doris our and mod, kin wou, het, ance inTher ine slelm mudt,\u0001, man i \n",
      "\n",
      "[(4/20 58.48%) 2.3441]\n",
      "thisotherday,becauseiwasnogentlemanborn. this I thet surs:\u0001\u0001t tomeckha coworither cowist st the my theer thou st withe ble thoted nome ford c \\m\n",
      "example.sentence Nemot my stes of wald lepntset; gaiechet spover's?\u0001-usfant use o wou' neve:\u0001\u0001s kill\u0001.\u0001\u0001 stoble guese \n",
      "\n",
      "[(5/20 0.00%) 1.9643]\n",
      "nornoonehere;forcursesneverpass vour wentondl to wastare\u0001r chever tlere worll sears laprer yours worsing beis whams sufepler wome th \\m\n",
      "example.sentence Uomhmors thener well su?\u0001nce\u0001f lenthe;\u0001 suinek nole chatsentstes thePse pirs?\u0001 thest wir st.\u0001\u0001 E:\u0001ce \n",
      "\n",
      "[(5/20 58.48%) 2.1970]\n",
      "tocounterfeitoppressionofsuchgrief pours hip as migh sale silo oned to beour gomo netie lat to sto sons:\u0001 wife to hissarsotht and his m \\m\n",
      "example.sentence Thell themme she ake pent ince thelte simpebes inte nad is thus all! tetisus touser enceave shenve t \n",
      "\n",
      "[(6/20 0.00%) 1.6898]\n",
      "thankfultotheeshallbemystudy,andmyprofit The foor forbO:\u0001, ngee?\u0001 shall these sese rodand and so he thoul\u0001th'l what mady, wand hard 3rorbesti \\m\n",
      "example.sentence I neme.\u0001N\u0001nce thence, have cersseste!\u0001Ence, Yeam diceance.\u0001\u00017 neng meT teence, lextimece.\u0001R ence.\u0001\u0001  \n",
      "\n",
      "[(6/20 58.48%) 1.5431]\n",
      "hastings: HAs hat ings:\u0001SEng:\u0001\u0001t bes surns thingtinge:\u0001ang\u0001s:\u0001-natI anle.\u0001\u0001 Cangy s!\u0001 as glpe\u0001ng gang sing:\u0001\u0001  \\m\n",
      "example.sentence Fo we.\u0001Tence.\u0001\u0001 se-elot.\u0001PRET:\u0001. Tence. Jice.\u0001:\u0001\u0001te ine.\u0001L:\u0001\u0001TO I:\u0001\u0001 me eneence.\u0001\u0001:\u0001\u0001 inece:\u0001 sot ir \n",
      "\n",
      "[(7/20 0.00%) 1.3380]\n",
      "ourdreadfulmarchestodelightfulmeasures. Pourd -urdeam amas fod the wes marves,\u0001 ams arCodfins.\u0001 Pim praares. \u0001ses worye.\u0001 Meste lies.\u0001\u0001 syau \\m\n",
      "example.sentence D tsence fime .e Bince.\u0001\u0001. Berim ame .tait onesenTtenesence me.\u0001t ence.\u0001\u0001ws soence.\u0001\u0001Csence seencenc \n",
      "\n",
      "[(7/20 58.48%) 0.9522]\n",
      "fourthcitizen: Four thcit I senen\u0001:\u0001:\u0001rth?e in I it ith\u0001 CI\u0001 Mith:\u0001\u0001g:\u0001rto'd hin mine: hiven:\u0001:\u0001nNe'nt:\u0001 sI neancE  \\m\n",
      "example.sentence I coner.\u0001\u0001 ne.\u0001\u0001. Meseontente\u0001. I mes. fosenkpe\u0001.\u0001m.\u0001\u0001 NesteTe.\u0001\u0001N I mampe. I ephe.\u0001:\u0001\u0001\u0001 ne.\u0001\u0001e he n \n",
      "\n",
      "[(8/20 0.00%) 0.4821]\n",
      "gloucester: MESTER:\u0001:\u0001,\u0001\u0001r:\u0001 cergcest t, boucer:\u0001cker:\u0001 CErmprouceprercest er:\u0001\u0001r\u0001 gucester: loul cestert :\u0001t er \\m\n",
      "example.sentence Puprleample maple. Clesesence.\u0001e gence!\u0001 ce. but ence.\u0001\u0001 CenaNle.\u0001NCslence.\u0001.\u0001\u0001  VPetence.\u0001\u0001, mople. \n",
      "\n",
      "[(8/20 58.48%) 0.5045]\n",
      "ifearyounot. If'ary oun ot.\u0001\u0001\u0001 tary on ta not. onot.\u0001\u0001.\u0001\u0001 Toun o tke akry\u0001 you.\u0001\u0001 t.\u0001LEA DR:\u0001\u0001\u0001t.\u0001 beary akn ot.\u0001 \\m\n",
      "example.sentence dewnt ence lencest tencencexce.\u0001\u0001.\u0001.\u0001 ETencestencencencet mence.\u0001.\u0001.\u0001.\u0001\u0001t encengten: I ble sercencet \n",
      "\n",
      "[(9/20 0.00%) 0.6175]\n",
      "gloucester: GOUSTER:\u0001I Cer's therles ter:\u0001r:\u0001 rer's:\u0001 rrer:\u0001rces\u0001:\u0001r: pare: er::\u0001r er'Tite as pert:\u0001ter\u0001@r er's: \\m\n",
      "example.sentence I I mlentence.\u0001'n ceam pllen?\u0001.R.\u0001\u0001nce.\u0001.\u0001\u0001.\u0001\u0001xcE.\u0001nce.\u0001.\u0001.\u0001Been\u0001\u0001tence.\u0001\u0001.\u0001\u0001.\u0001\u0001tence.\u0001\u0001le sentence. \n",
      "\n",
      "[(9/20 58.48%) 0.1696]\n",
      "gloucester: GLOUCESTER:\u0001\u0001r:\u0001r:\u0001cCEst:\u0001r:\u0001rcest:\u0001r:\u0001r:\u0001r:\u0001:\u0001ster:\u0001r:\u0001rg loucest:\u0001r:\u0001\u0001r:\u0001r:\u0001r:\u0001r:\u0001r:\u0001r:\u0001rar:\u0001\u0001r:\u0001r \\m\n",
      "example.sentence I gentence. CpEnce.\u0001Lce.\u0001.\u0001.\u0001.\u0001\u0001.\u0001ce.\u0001. Comtplenten\u0001ce.\u0001.\u0001\u0001nce.\u0001\u0001ce.\u0001.\u0001\u0001, nce.\u0001.\u0001\u0001w IN Centence.\u0001\u0001te \n",
      "\n",
      "[(10/20 0.00%) 0.5150]\n",
      "what,inthemidstofthestreet? Whas t, in the estole thes to thest reetidst net!\u0001\u0001thes toof the strer offthe.\u0001\u0001d\u0001 thes to foftthes  \\m\n",
      "example.sentence Niw Ple. Clence, tence\u0001 cuence.\u0001 cence.\u0001\u0001nce.\u0001 Cuce. Cence.\u0001 nce. Cence lence.\u0001 nce.\u0001ce. cence.\u0001nce. \n",
      "\n",
      "[(10/20 58.48%) 0.8826]\n",
      "slandermyselfasfalsetoedward'sbed; Slander myself ase to et o eard's falsetoe to efall salu sbeped; ward'se to elf allse to to sal set  \\m\n",
      "example.sentence E lee?\u0001 sentence. nce.\u0001\u0001\u0001m bajle.\u0001s tence\u0001stenceencete nce.\u0001 Centence,\u0001 nce.\u0001\u0001 tence.\u0001n ce.\u0001 Mle dee \n",
      "\n",
      "[(11/20 0.00%) 0.2089]\n",
      "coriolanus: CORONUOLANHAN:\u0001:\u0001s asus:\u0001:\u0001n: slus:\u0001:\u0001s lanus: weul anus:\u0001:\u0001\u0001 sulan ous:\u0001: s:\u0001\u0001r Iollan us: AnUs:\u0001:\u0001 \\m\n",
      "example.sentence ENIA mple. iple tencey.\u0001\u0001nce.\u0001 .\u0001.\u0001\u0001 am ple.\u0001 fence.\u0001\u0001nce.\u0001.\u0001\u0001n Eenet entes entence.\u0001\u0001n ce.\u0001\u0001\u0001 mLer  \n",
      "\n",
      "[(11/20 58.48%) 0.6082]\n",
      "astheantipodesareuntous, Ast wh ip od o desare utous,\u0001s,\u0001s,\u0001 de odes tous,\u0001 do desareuntou s,\u0001s, are untous,\u0001 desareun tous,\u0001 \\m\n",
      "example.sentence Eu ve-ample.N coen ce.\u0001\u0001xample.\u0001\u0001splence.\u0001\u0001 ample.\u0001\u0001xA mle.\u0001\u0001 ample.\u0001?\u0001.\u0001 I Amplew le sente ence amp \n",
      "\n",
      "[(12/20 0.00%) 0.5750]\n",
      "yetthatthybrazengatesofheavenmayope, Yet that a thy bracen mayop e!\u0001aven May oppe,\u0001t thave nen cay ope,\u0001n may ope, day ope,\u0001n mayo\u0001 heave \\m\n",
      "example.sentence Se jample.\u0001 xlele.\u0001 senten: senterce.\u0001ncet ence.\u0001'nce.\u0001. Ence hle.\u0001\u0001nce.\u0001\u0001nce.\u0001 .\u0001\u0001xce.\u0001. I kentence \n",
      "\n",
      "[(12/20 58.48%) 0.7127]\n",
      "iwouldyourspiritwereeasierforadvice, I would your spirt wes ier forad vice,\u0001\u0001,\u0001 I twereasi\u0001\u0001 werte asie ritwere as iit 'f orad vice,\u0001\u0001,\u0001  \\m\n",
      "example.sentence Hexd ample.\u0001\u0001nce.\u0001.\u0001\u0001nteen ce.\u0001\u0001\u0001 Ence.\u0001\u0001s Encestence.\u0001\u0001nptenxce.\u0001\u0001nce.\u0001bnce.\u0001\u0001nce dence.\u0001\u0001\u0001nce ampl \n",
      "\n",
      "[(13/20 0.00%) 0.1750]\n",
      "thisgentleman,theprince'snearally, This gentleman, the prinpince\u0001y, the prince's near ally,\u0001, the prince'sne in cearly,\u0001l al y, , near  \\m\n",
      "example.sentence A mple xucten ce.\u0001\u0001 Ence. Ce.\u0001\u0001nce.\u0001\u0001\u0001!\u0001\u0001nce.\u0001,\u0001nl cemnce.\u0001\u0001r Ce.\u0001\u0001tence,\u0001\u0001n ce.\u0001\u0001\u0001 knce.\u0001.\u0001 IZe .\u0001  \n",
      "\n",
      "[(13/20 58.48%) 0.5955]\n",
      "soasthoulivestinpeace,diefreefromstrife: Soast houll I Vefrefrefrefrefrefrefrefreffrefdrefrefrefrefrefoms trife:\u0001\u0001n preace, diefrefhree frefr \\m\n",
      "example.sentence exuampleWsence. Bxleia mple.\u0001\u0001'mpole.\u0001\u0001nce.\u0001. I Xe.\u0001.\u0001.\u0001\u0001ce.\u0001\u0001.\u0001\u0001 nCebustence, tence.\u0001.\u0001\u0001\u0001\u0001 .e .ce.\u0001 \n",
      "\n",
      "[(14/20 0.00%) 0.3192]\n",
      "mybanquetistocloseourstomachsup, My banque tis to closeto mach sup,\u0001 pJ,\u0001 p, ach sup,\u0001 pup,\u0001 p, ach\u0001s\u0001up,\u0001 proosto machs up,\u0001 p, sach \\m\n",
      "example.sentence Am pled sence.\u0001\u0001\u0001 \u0001nce.\u0001\u0001. Refce.\u0001\u0001\u0001te nce.\u0001.\u0001.\u0001\u0001\u0001nce.\u0001.\u0001.\u0001.\u0001'ce.\u0001\u0001.\u0001\u0001'fce.\u0001\u0001\u0001 Ence.\u0001.\u0001\u0001.\u0001.\u0001\u0001\u0001nCe.\u0001\u0001 \n",
      "\n",
      "[(14/20 58.48%) 0.7607]\n",
      "nothingbutsecrecy.letmysheepgo:come,good Manoth in gbut secrecy. ly.\u0001W o: let my she po: good dood , od thing butsecry.\u0001Wo-d good do: od do:  \\m\n",
      "example.sentence A mp le. sentence.\u0001\u0001.\u0001\u0001 nce.\u0001\u0001\u0001 dence.\u0001.\u0001.\u0001.\u0001\u0001E Ncestence.\u0001.\u0001\u0001.\u0001\u0001n Cexstentence.\u0001 Ence.\u0001\u0001\u0001. AMple.\u0001\u0001 \n",
      "\n",
      "[(15/20 0.00%) 0.2641]\n",
      "thisenemytown.i'llenter:ifheslayme, This ene my to wn.\u0001i'llenter: I fhes lay me,\u0001,\u0001 lay may me,\u0001, lay me,\u0001, lay me,\u0001, lay me,\u0001 lay ge-me \\m\n",
      "example.sentence eie nce .Nce.\u0001\u0001 tence.\u0001 ENCEce.\u0001\u0001\u0001. Ample.\u0001\u0001nce. ENCETETE VPACETESence.\u0001.\u0001\u0001sentence.\u0001.\u0001.\u0001\u0001\u0001 Ence.\u0001\u0001. \n",
      "\n",
      "[(15/20 58.48%) 0.1227]\n",
      "clarence: CLANENCE:\u0001:\u0001\u0001:\u0001\u0001:\u0001:\u0001\u0001 nce::\u0001:\u0001:\u0001\u0001:\u0001\u0001 ce:\u0001:\u0001\u0001 ce:\u0001:\u0001 vence:\u0001:\u0001\u0001O:\u0001\u0001:\u0001: nce:\u0001'nce:\u0001:\u0001\u0001:\u0001 :\u0001\u0001mlarence:\u0001 \\m\n",
      "example.sentence EEBextence. ImPle.\u0001.\u0001\u0001 nce.\u0001\u0001\u0001 nce.\u0001.\u0001\u0001.\u0001 VAmPle.\u0001\u0001\u0001 ny ce.\u0001\u0001.\u0001\u0001\u0001 nce.\u0001 .\u0001Bsentence.\u0001\u0001. Ence.\u0001.\u0001\u0001\u0001 n \n",
      "\n",
      "[(16/20 0.00%) 0.4188]\n",
      "prithee,nomore:thoudosttalknothingtome. Prithe ee, no more: thingtoud ost alk, nothing to me.\u0001.\u0001\u0001d os talk not hing to me.\u0001\u0001. M.\u0001 Fost a lkn \\m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example.sentence Ex mle tencie.\u0001\u0001.\u0001\u0001. ENce.\u0001 .\u0001.\u0001.\u0001ble.\u0001\u0001 iplessence.\u0001.\u0001\u0001.\u0001\u0001. ce.\u0001.\u0001 ENce.\u0001\u0001.\u0001.\u0001.\u0001.\u0001.\u0001\u0001. Imple.\u0001.\u0001\u0001\u0001. \n",
      "\n",
      "[(16/20 58.48%) 0.5316]\n",
      "andforthymaintenancecommitshisbody And for thy maintencecamitss body\u0001d yod so\u0001d yod\u0001od yorthy maits bod\u0001\u0001d\u0001s body yor mits\u0001b ody\u0001d\u0001 yor \\m\n",
      "example.sentence ExU mle serce.\u0001.\u0001\u0001'mple\u0001 jample.\u0001\u0001NCE.\u0001\u0001\u0001En ce.\u0001\u0001ze.\u0001.\u0001\u0001ble.\u0001\u0001rce.\u0001.\u0001\u0001\u0001 A mle.\u0001 I mle.\u0001\u0001\u0001nce.\u0001.\u0001.\u0001.\u0001 \n",
      "\n",
      "[(17/20 0.00%) 0.2372]\n",
      "andwringtheawfulsceptrefromhisfist, And wring theaw fulsceptrefromhis fist, st,\u0001 cepptrefrom his fist,\u0001\u0001\u0001 peptrefromhis fist,\u0001,\u0001 cepe pt \\m\n",
      "example.sentence Edextence.\u0001\u0001nce?\u0001.\u0001\u0001\u0001nce.\u0001\u0001\u0001n\u0001ce.\u0001.\u0001 EnCe.\u0001.\u0001.\u0001\u0001nce,\u0001\u0001\u0001n ce.\u0001\u0001\u0001 NCETEEN CENTence.\u0001\u0001 I Plence.\u0001.\u0001\u0001 bl \n",
      "\n",
      "[(17/20 58.48%) 0.2545]\n",
      "youhaveundoneamanoffourscorethree, You have und on eam anof four scoreth Ree,\u0001,\u0001 hore,\u0001\u0001,\u0001\u0001n of four scoreth tree,\u0001\u0001,\u0001 hree,\u0001,\u0001\u0001e,\u0001 hre \\m\n",
      "example.sentence extexce ce.\u0001\u0001 sence.\u0001.\u0001\u0001nce.N\u0001. EN RETn ce. Ble.s\u0001nce.\u0001.\u0001\u0001nce.\u0001.\u0001.\u0001\u0001nce.\u0001\u0001 sence.\u0001.\u0001\u0001nce.\u0001\u0001t ENce.\u0001\u0001 \n",
      "\n",
      "[(18/20 0.00%) 1.2312]\n",
      "friarlaurence: Friarl aurence: I te:\u0001\u0001-ce:\u0001: bence:\u0001\u0001: in e: cee:\u0001\u0001:\u0001\u0001:\u0001 uren ce:\u0001:\u0001n E: ence:\u0001:\u0001 I Burence:\u0001:\u0001\u0001\u0001\u0001: \\m\n",
      "example.sentence Ex a ble. Le sen ten tence. TEn EN CEESTentence.\u0001. Ente ne.\u0001\u0001 mle. sentence.\u0001\u0001. TENTe ften ce..\u0001 En  \n",
      "\n",
      "[(18/20 58.48%) 0.4225]\n",
      "peter: PETER:\u0001R:\u0001:\u0001\u0001r:\u0001:\u0001\u0001\u0001r:\u0001:\u0001\u0001r:\u0001: 'ter:\u0001r:\u0001:\u0001\u0001r:\u0001\u0001,\u0001\u0001 R:\u0001:\u0001\u0001r:\u0001:\u0001 R:'ter:\u0001\u0001r:\u0001:\u0001r:\u0001't h:\u0001r:\u0001'ter:\u0001\u0001r:\u0001\u0001 \\m\n",
      "example.sentence Example.Sentelententenentens enten tenententene desentene entenententene ne.\u0001- le.\u0001n tEnesteen tenet \n",
      "\n",
      "[(19/20 0.00%) 0.5529]\n",
      "eventhenthatsunshinebrew'dashowerforhim, EVe vove ve veven that sun she brew'd ashower for him,\u0001\u0001 dashowerfory him,\u0001\u0001 dashower for him,\u0001,\u0001d a \\m\n",
      "example.sentence Eex Ample. senten ce.\u0001.\u0001 Ententence.\u0001\u0001n Ce.\u0001.\u0001\u0001n cemle, sententence.\u0001\u0001n mce.\u0001.\u0001\u0001n Ce.\u0001EN\u0001n Ce.\u0001. .NT \n",
      "\n",
      "[(19/20 58.48%) 0.1476]\n",
      "suchnews,mylord,asgrievesmetounfold. Such news, my lord, as grieves me tou noo d.\u0001 lof\u0001ld. Lood .\u0001Ld.\u0001 Etoun fold.\u0001 COLDUTYASGRDUNfold.\u0001L \\m\n",
      "example.sentence Ere xam ple.\u0001 sentence.\u0001 Dente nce.\u0001.\u0001\u0001 nce.\u0001\u0001.\u0001\u0001nce.\u0001. NTentence. Theim ple. Entenceu.\u0001\u0001 Veenten te \n",
      "\n",
      "[(20/20 0.00%) 0.5627]\n",
      "andnotobedienttohishonestwill, And no bed in to bed in to bedient this honesst will,\u0001\u0001 ll,\u0001\u0001 nestw ill,\u0001 fill,\u0001\u0001 ll,\u0001\u0001 honestwill,\u0001 \\m\n",
      "example.sentence EXexte I mle. sentence\u0001. EnTence?\u0001.\u0001\u0001\u0001n ce.\u0001\u0001 I mple.\u0001nce.\u0001.\u0001\u0001nce.\u0001.\u0001N tence. Nice.\u0001\u0001 Ence.\u0001-nce.\u0001\u0001  \n",
      "\n",
      "[(20/20 58.48%) 0.3108]\n",
      "whywillyoumewherup, Why will you me wherup,\u0001ll you me mewher,\u0001 parup,\u0001\u0001lilly ou\u0001me wher up,\u0001\u0001ll\u0001\u0001ou\u0001 mewherup,\u0001\u0001l\u0001\u0001 wher \\m\n",
      "example.sentence Example. Sentence.\u0001USE:\u0001 nCe.\u0001\u0001 mle.\u0001 Sentence.\u0001\u0001nce.\u0001\u0001\u0001nce.\u0001\u0001 Ence.\u0001.\u0001 Sentence.\u0001\u0001nce.\u0001.\u0001nce.\u0001TE NT \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    encoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\n",
    "    decoder = sm.AttnDecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        decoder.cuda()\n",
    "        encoder.cuda()\n",
    "    print(encoder, decoder)\n",
    "    \n",
    "    params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr = 0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_loader = DataLoader(dataset=TextDataset(),\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=2)\n",
    "    \n",
    "    print(\"Training for %d epochs...\" % N_EPOCH)\n",
    "    \n",
    "    for epoch in range(1, N_EPOCH + 1):\n",
    "        for ii, (srcs, targets) in enumerate(train_loader):\n",
    "            train_loss = train(srcs[0], targets[0])\n",
    "            \n",
    "            if ii % 100 == 0 : \n",
    "                print('[(%d/%d %.2f%%) %.4f]' %\n",
    "                      (epoch, N_EPOCH, (ii * len(srcs)) * 100 / (len(train_loader) * len(srcs)), train_loss))\n",
    "#                 print('[(%d/%d %d%%) %.4f]' %\n",
    "#                       (epoch, N_EPOCH, ii * len(srcs) * 100 / len(train_loader), train_loss)) \n",
    "#해당 코드 사용시 100% 넘어가서 위에로 수정함\n",
    "                output, _ = translate(srcs[0])\n",
    "                print(srcs[0], output, '\\m')\n",
    "                \n",
    "                output, attentions = translate()\n",
    "                print('example.sentence', output, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07a0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810cec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e66cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c07e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
