{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681e7416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T01:41:44.607915Z",
     "start_time": "2024-02-19T01:41:40.291968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n",
      "torch 2.2.0+cpu\n",
      "numpy 1.19.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print('torch', torch.__version__)\n",
    "print('numpy', np.__version__)\n",
    "\n",
    "# from torch.autograd import Variable (torch.Tensor로 통합됨)\n",
    "\n",
    "import time\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from name_dataset import NameDataset #py파일 (names_train.csv 처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c291cd46",
   "metadata": {},
   "source": [
    "# classification basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7666913f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T07:54:34.790006Z",
     "start_time": "2024-02-14T07:54:34.785020Z"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "N_CHARS = 128\n",
    "N_CLASSES = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7e62ebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T08:16:05.959026Z",
     "start_time": "2024-02-14T08:16:05.951987Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        batch_size = input.size(0) # input = B x S . size(0) = B\n",
    "        \n",
    "        input = input.t()  # input:  B x S  -- (transpose) --> S x B\n",
    "        \n",
    "        print(\" input\", input.size())\n",
    "        embedded = self.embedding(input)\n",
    "        print(\" embedding\", embedded.size())\n",
    "        \n",
    "        #hidden = self._init_hidden(batch_size)\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        \n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        print(\" gru hidden output\", hidden.size())\n",
    "        \n",
    "        fc_output = self.fc(hidden)\n",
    "        print(\" fc output\", fc_output.size())\n",
    "        return fc_output\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3955728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T08:16:06.684298Z",
     "start_time": "2024-02-14T08:16:06.677768Z"
    }
   },
   "outputs": [],
   "source": [
    "def str2ascii_arr(msg):\n",
    "    arr = [ord(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    return seq_tensor\n",
    "\n",
    "def make_variables(names):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "408df812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T08:17:10.734389Z",
     "start_time": "2024-02-14T08:17:10.703277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input torch.Size([6, 1])\n",
      " embedding torch.Size([6, 1, 100])\n",
      " gru hidden output torch.Size([1, 1, 100])\n",
      " fc output torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 6]) out torch.Size([1, 1, 18])\n",
      " input torch.Size([5, 1])\n",
      " embedding torch.Size([5, 1, 100])\n",
      " gru hidden output torch.Size([1, 1, 100])\n",
      " fc output torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 5]) out torch.Size([1, 1, 18])\n",
      " input torch.Size([4, 1])\n",
      " embedding torch.Size([4, 1, 100])\n",
      " gru hidden output torch.Size([1, 1, 100])\n",
      " fc output torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 4]) out torch.Size([1, 1, 18])\n",
      " input torch.Size([3, 1])\n",
      " embedding torch.Size([3, 1, 100])\n",
      " gru hidden output torch.Size([1, 1, 100])\n",
      " fc output torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 3]) out torch.Size([1, 1, 18])\n",
      " input torch.Size([6, 4])\n",
      " embedding torch.Size([6, 4, 100])\n",
      " gru hidden output torch.Size([1, 4, 100])\n",
      " fc output torch.Size([1, 4, 18])\n",
      "batch in torch.Size([4, 6]) batch out torch.Size([1, 4, 18])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    names = ['adylov', 'solan', 'hard', 'san']\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_CLASSES)\n",
    "    \n",
    "    for name in names:\n",
    "        arr, _ = str2ascii_arr(name)\n",
    "        inp = torch.LongTensor([arr])\n",
    "        out = classifier(inp)\n",
    "        print(\"in\", inp.size(), \"out\", out.size())\n",
    "        \n",
    "    inputs = make_variables(names)\n",
    "    out = classifier(inputs)\n",
    "    print('batch in', inputs.size(), 'batch out', out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22bc79e",
   "metadata": {},
   "source": [
    "# rnn_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f54fa2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:27:32.123612Z",
     "start_time": "2024-02-16T09:27:32.085077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 countries\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "\n",
    "test_dataset = NameDataset(is_train_set=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_dataset = NameDataset(is_train_set=True)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                        batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "N_COUNTRIES = len(train_dataset.get_countries())\n",
    "print(N_COUNTRIES, \"countries\")\n",
    "N_CHARS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7686b3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:33:59.577916Z",
     "start_time": "2024-02-16T09:33:59.569930Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m,s)\n",
    "\n",
    "def creative_variable(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        return tensor.cuda()\n",
    "    else:\n",
    "        return tensor\n",
    "        \n",
    "def pad_sequences(vectorized_seqs, seq_lengths, countries):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "        \n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "    \n",
    "    target = countries2tensor(countries)\n",
    "    if len(countries):\n",
    "        target = target[perm_idx]\n",
    "        \n",
    "    return creative_variable(seq_tensor), creative_variable(seq_lengths), creative_variable(target)\n",
    "\n",
    "def make_variables(names, countries):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n",
    "\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ord(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "def countries2tensor(countries):\n",
    "    country_ids = [train_dataset.get_country_id(country) \n",
    "                   for country in countries]\n",
    "    return torch.LongTensor(country_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b75f4ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:33:59.842307Z",
     "start_time": "2024-02-16T09:33:59.824908Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = int(bidirectional) + 1\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                               bidirectional=bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, seq_lengths):\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "        \n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        gru_input = pack_padded_sequence(\n",
    "                    embedded, seq_lengths.data.cpu().numpy())\n",
    "        \n",
    "        self.gru.flatten_parameters()\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        \n",
    "        fc_output = self.fc(hidden[-1])\n",
    "        return fc_output\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
    "                            batch_size, self.hidden_size)\n",
    "        return creative_variable(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16f84031",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:39:26.897616Z",
     "start_time": "2024-02-16T09:39:26.882898Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_loss = 0\n",
    "    \n",
    "    for ii, (names, countries) in enumerate(train_loader, 1):\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        output = classifier(input, seq_lengths)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item() #loss.data[0]\n",
    "        \n",
    "        classifier.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ii % 10 == 0:\n",
    "            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
    "                time_since(start), epoch,  ii *\n",
    "                len(names), len(train_loader.dataset),\n",
    "                100. * ii * len(names) / len(train_loader.dataset),\n",
    "                total_loss / ii * len(names)))\n",
    "            \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ebe6f3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:42:45.681303Z",
     "start_time": "2024-02-16T09:42:45.674636Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(name=None):\n",
    "    if name:\n",
    "        input, seq_lengths, target = make_variables([name], [])\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        country_id = pred.cpu().numpy()[0][0]\n",
    "        print(name, \"is\", train_dataset.get_country(country_id))\n",
    "        return\n",
    "    \n",
    "    print('evaluating trained model ...')\n",
    "    correct = 0\n",
    "    train_data_size = len(test_loader.dataset)\n",
    "    \n",
    "    for names, countries in test_loader:\n",
    "        input, seq_lengths, target = make_variables(names, countries)\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, train_data_size, 100. * correct / train_data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96d004",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-16T09:42:46.364Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epoch...\n",
      "[0m 1s] Train Epoch: 1 [2560/13374 (19%)]\tLoss: 622.39\n",
      "[0m 2s] Train Epoch: 1 [5120/13374 (38%)]\tLoss: 544.70\n",
      "[0m 3s] Train Epoch: 1 [7680/13374 (57%)]\tLoss: 505.91\n",
      "[0m 4s] Train Epoch: 1 [10240/13374 (77%)]\tLoss: 475.77\n",
      "[0m 6s] Train Epoch: 1 [12800/13374 (96%)]\tLoss: 453.25\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 4173/6700 (62%)\n",
      "\n",
      "Sung is Arabic\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Russian\n",
      "[0m 9s] Train Epoch: 2 [2560/13374 (19%)]\tLoss: 328.70\n",
      "[0m 11s] Train Epoch: 2 [5120/13374 (38%)]\tLoss: 319.13\n",
      "[0m 13s] Train Epoch: 2 [7680/13374 (57%)]\tLoss: 311.52\n",
      "[0m 15s] Train Epoch: 2 [10240/13374 (77%)]\tLoss: 306.77\n",
      "[0m 17s] Train Epoch: 2 [12800/13374 (96%)]\tLoss: 302.25\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 4652/6700 (69%)\n",
      "\n",
      "Sung is Arabic\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 22s] Train Epoch: 3 [2560/13374 (19%)]\tLoss: 260.57\n",
      "[0m 23s] Train Epoch: 3 [5120/13374 (38%)]\tLoss: 258.24\n",
      "[0m 25s] Train Epoch: 3 [7680/13374 (57%)]\tLoss: 253.32\n",
      "[0m 27s] Train Epoch: 3 [10240/13374 (77%)]\tLoss: 248.45\n",
      "[0m 29s] Train Epoch: 3 [12800/13374 (96%)]\tLoss: 245.21\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 4993/6700 (75%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Russian\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 34s] Train Epoch: 4 [2560/13374 (19%)]\tLoss: 215.18\n",
      "[0m 35s] Train Epoch: 4 [5120/13374 (38%)]\tLoss: 212.24\n",
      "[0m 37s] Train Epoch: 4 [7680/13374 (57%)]\tLoss: 210.77\n",
      "[0m 39s] Train Epoch: 4 [10240/13374 (77%)]\tLoss: 206.97\n",
      "[0m 41s] Train Epoch: 4 [12800/13374 (96%)]\tLoss: 203.85\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5164/6700 (77%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[0m 44s] Train Epoch: 5 [2560/13374 (19%)]\tLoss: 180.67\n",
      "[0m 46s] Train Epoch: 5 [5120/13374 (38%)]\tLoss: 179.79\n",
      "[0m 47s] Train Epoch: 5 [7680/13374 (57%)]\tLoss: 178.54\n",
      "[0m 49s] Train Epoch: 5 [10240/13374 (77%)]\tLoss: 177.46\n",
      "[0m 51s] Train Epoch: 5 [12800/13374 (96%)]\tLoss: 174.74\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5303/6700 (79%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[0m 54s] Train Epoch: 6 [2560/13374 (19%)]\tLoss: 162.02\n",
      "[0m 56s] Train Epoch: 6 [5120/13374 (38%)]\tLoss: 156.24\n",
      "[0m 57s] Train Epoch: 6 [7680/13374 (57%)]\tLoss: 156.52\n",
      "[0m 59s] Train Epoch: 6 [10240/13374 (77%)]\tLoss: 154.42\n",
      "[1m 1s] Train Epoch: 6 [12800/13374 (96%)]\tLoss: 155.48\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5417/6700 (81%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[1m 4s] Train Epoch: 7 [2560/13374 (19%)]\tLoss: 140.91\n",
      "[1m 6s] Train Epoch: 7 [5120/13374 (38%)]\tLoss: 138.77\n",
      "[1m 7s] Train Epoch: 7 [7680/13374 (57%)]\tLoss: 139.04\n",
      "[1m 9s] Train Epoch: 7 [10240/13374 (77%)]\tLoss: 141.67\n",
      "[1m 11s] Train Epoch: 7 [12800/13374 (96%)]\tLoss: 140.98\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5438/6700 (81%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[1m 14s] Train Epoch: 8 [2560/13374 (19%)]\tLoss: 137.56\n",
      "[1m 16s] Train Epoch: 8 [5120/13374 (38%)]\tLoss: 133.44\n",
      "[1m 18s] Train Epoch: 8 [7680/13374 (57%)]\tLoss: 129.93\n",
      "[1m 19s] Train Epoch: 8 [10240/13374 (77%)]\tLoss: 128.71\n",
      "[1m 21s] Train Epoch: 8 [12800/13374 (96%)]\tLoss: 129.23\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5484/6700 (82%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Russian\n",
      "Nako is Japanese\n",
      "[1m 24s] Train Epoch: 9 [2560/13374 (19%)]\tLoss: 110.03\n",
      "[1m 26s] Train Epoch: 9 [5120/13374 (38%)]\tLoss: 115.50\n",
      "[1m 28s] Train Epoch: 9 [7680/13374 (57%)]\tLoss: 115.52\n",
      "[1m 29s] Train Epoch: 9 [10240/13374 (77%)]\tLoss: 116.78\n",
      "[1m 31s] Train Epoch: 9 [12800/13374 (96%)]\tLoss: 117.95\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5504/6700 (82%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[1m 34s] Train Epoch: 10 [2560/13374 (19%)]\tLoss: 105.06\n",
      "[1m 36s] Train Epoch: 10 [5120/13374 (38%)]\tLoss: 104.84\n",
      "[1m 38s] Train Epoch: 10 [7680/13374 (57%)]\tLoss: 105.86\n",
      "[1m 39s] Train Epoch: 10 [10240/13374 (77%)]\tLoss: 107.59\n",
      "[1m 41s] Train Epoch: 10 [12800/13374 (96%)]\tLoss: 107.70\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5537/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[1m 44s] Train Epoch: 11 [2560/13374 (19%)]\tLoss: 92.98\n",
      "[1m 46s] Train Epoch: 11 [5120/13374 (38%)]\tLoss: 97.13\n",
      "[1m 47s] Train Epoch: 11 [7680/13374 (57%)]\tLoss: 98.16\n",
      "[1m 49s] Train Epoch: 11 [10240/13374 (77%)]\tLoss: 98.86\n",
      "[1m 51s] Train Epoch: 11 [12800/13374 (96%)]\tLoss: 99.75\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5576/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[1m 56s] Train Epoch: 12 [2560/13374 (19%)]\tLoss: 85.69\n",
      "[1m 57s] Train Epoch: 12 [5120/13374 (38%)]\tLoss: 85.86\n",
      "[1m 59s] Train Epoch: 12 [7680/13374 (57%)]\tLoss: 88.71\n",
      "[2m 0s] Train Epoch: 12 [10240/13374 (77%)]\tLoss: 88.59\n",
      "[2m 2s] Train Epoch: 12 [12800/13374 (96%)]\tLoss: 91.48\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5586/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Russian\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[2m 5s] Train Epoch: 13 [2560/13374 (19%)]\tLoss: 89.35\n",
      "[2m 7s] Train Epoch: 13 [5120/13374 (38%)]\tLoss: 85.65\n",
      "[2m 9s] Train Epoch: 13 [7680/13374 (57%)]\tLoss: 88.11\n",
      "[2m 10s] Train Epoch: 13 [10240/13374 (77%)]\tLoss: 86.86\n",
      "[2m 12s] Train Epoch: 13 [12800/13374 (96%)]\tLoss: 86.01\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5608/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[2m 15s] Train Epoch: 14 [2560/13374 (19%)]\tLoss: 76.52\n",
      "[2m 17s] Train Epoch: 14 [5120/13374 (38%)]\tLoss: 73.31\n",
      "[2m 19s] Train Epoch: 14 [7680/13374 (57%)]\tLoss: 75.82\n",
      "[2m 20s] Train Epoch: 14 [10240/13374 (77%)]\tLoss: 76.85\n",
      "[2m 22s] Train Epoch: 14 [12800/13374 (96%)]\tLoss: 77.47\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5589/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[2m 26s] Train Epoch: 15 [2560/13374 (19%)]\tLoss: 71.32\n",
      "[2m 28s] Train Epoch: 15 [5120/13374 (38%)]\tLoss: 68.06\n",
      "[2m 29s] Train Epoch: 15 [7680/13374 (57%)]\tLoss: 71.95\n",
      "[2m 31s] Train Epoch: 15 [10240/13374 (77%)]\tLoss: 71.98\n",
      "[2m 32s] Train Epoch: 15 [12800/13374 (96%)]\tLoss: 72.36\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5621/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[2m 36s] Train Epoch: 16 [2560/13374 (19%)]\tLoss: 58.98\n",
      "[2m 38s] Train Epoch: 16 [5120/13374 (38%)]\tLoss: 62.33\n",
      "[2m 40s] Train Epoch: 16 [7680/13374 (57%)]\tLoss: 62.38\n",
      "[2m 41s] Train Epoch: 16 [10240/13374 (77%)]\tLoss: 63.61\n",
      "[2m 43s] Train Epoch: 16 [12800/13374 (96%)]\tLoss: 65.15\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5593/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[2m 47s] Train Epoch: 17 [2560/13374 (19%)]\tLoss: 59.10\n",
      "[2m 48s] Train Epoch: 17 [5120/13374 (38%)]\tLoss: 57.86\n",
      "[2m 50s] Train Epoch: 17 [7680/13374 (57%)]\tLoss: 57.67\n",
      "[2m 52s] Train Epoch: 17 [10240/13374 (77%)]\tLoss: 58.26\n",
      "[2m 53s] Train Epoch: 17 [12800/13374 (96%)]\tLoss: 59.30\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5615/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[2m 58s] Train Epoch: 18 [2560/13374 (19%)]\tLoss: 49.87\n",
      "[2m 59s] Train Epoch: 18 [5120/13374 (38%)]\tLoss: 52.56\n",
      "[3m 1s] Train Epoch: 18 [7680/13374 (57%)]\tLoss: 53.40\n",
      "[3m 3s] Train Epoch: 18 [10240/13374 (77%)]\tLoss: 52.55\n",
      "[3m 5s] Train Epoch: 18 [12800/13374 (96%)]\tLoss: 52.78\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5606/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is English\n",
      "Nako is Japanese\n",
      "[3m 8s] Train Epoch: 19 [2560/13374 (19%)]\tLoss: 46.04\n",
      "[3m 10s] Train Epoch: 19 [5120/13374 (38%)]\tLoss: 46.91\n",
      "[3m 11s] Train Epoch: 19 [7680/13374 (57%)]\tLoss: 47.88\n",
      "[3m 13s] Train Epoch: 19 [10240/13374 (77%)]\tLoss: 47.94\n",
      "[3m 15s] Train Epoch: 19 [12800/13374 (96%)]\tLoss: 48.16\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5585/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[3m 19s] Train Epoch: 20 [2560/13374 (19%)]\tLoss: 42.49\n",
      "[3m 20s] Train Epoch: 20 [5120/13374 (38%)]\tLoss: 44.39\n",
      "[3m 22s] Train Epoch: 20 [7680/13374 (57%)]\tLoss: 46.02\n",
      "[3m 23s] Train Epoch: 20 [10240/13374 (77%)]\tLoss: 46.36\n",
      "[3m 25s] Train Epoch: 20 [12800/13374 (96%)]\tLoss: 45.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5619/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[3m 29s] Train Epoch: 21 [2560/13374 (19%)]\tLoss: 44.51\n",
      "[3m 30s] Train Epoch: 21 [5120/13374 (38%)]\tLoss: 40.79\n",
      "[3m 32s] Train Epoch: 21 [7680/13374 (57%)]\tLoss: 40.87\n",
      "[3m 33s] Train Epoch: 21 [10240/13374 (77%)]\tLoss: 41.86\n",
      "[3m 35s] Train Epoch: 21 [12800/13374 (96%)]\tLoss: 41.87\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5605/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[3m 38s] Train Epoch: 22 [2560/13374 (19%)]\tLoss: 39.63\n",
      "[3m 40s] Train Epoch: 22 [5120/13374 (38%)]\tLoss: 39.35\n",
      "[3m 41s] Train Epoch: 22 [7680/13374 (57%)]\tLoss: 38.59\n",
      "[3m 43s] Train Epoch: 22 [10240/13374 (77%)]\tLoss: 39.75\n",
      "[3m 45s] Train Epoch: 22 [12800/13374 (96%)]\tLoss: 40.06\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5597/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[3m 48s] Train Epoch: 23 [2560/13374 (19%)]\tLoss: 37.39\n",
      "[3m 50s] Train Epoch: 23 [5120/13374 (38%)]\tLoss: 36.17\n",
      "[3m 52s] Train Epoch: 23 [7680/13374 (57%)]\tLoss: 35.93\n",
      "[3m 53s] Train Epoch: 23 [10240/13374 (77%)]\tLoss: 36.19\n",
      "[3m 55s] Train Epoch: 23 [12800/13374 (96%)]\tLoss: 35.88\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5588/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[3m 59s] Train Epoch: 24 [2560/13374 (19%)]\tLoss: 30.06\n",
      "[4m 0s] Train Epoch: 24 [5120/13374 (38%)]\tLoss: 29.90\n",
      "[4m 2s] Train Epoch: 24 [7680/13374 (57%)]\tLoss: 29.52\n",
      "[4m 3s] Train Epoch: 24 [10240/13374 (77%)]\tLoss: 30.14\n",
      "[4m 5s] Train Epoch: 24 [12800/13374 (96%)]\tLoss: 30.84\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5606/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Russian\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[4m 9s] Train Epoch: 25 [2560/13374 (19%)]\tLoss: 30.63\n",
      "[4m 10s] Train Epoch: 25 [5120/13374 (38%)]\tLoss: 30.76\n",
      "[4m 12s] Train Epoch: 25 [7680/13374 (57%)]\tLoss: 30.78\n",
      "[4m 13s] Train Epoch: 25 [10240/13374 (77%)]\tLoss: 31.43\n",
      "[4m 15s] Train Epoch: 25 [12800/13374 (96%)]\tLoss: 31.73\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5578/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is English\n",
      "Nako is Japanese\n",
      "[4m 19s] Train Epoch: 26 [2560/13374 (19%)]\tLoss: 28.62\n",
      "[4m 20s] Train Epoch: 26 [5120/13374 (38%)]\tLoss: 31.63\n",
      "[4m 22s] Train Epoch: 26 [7680/13374 (57%)]\tLoss: 32.18\n",
      "[4m 23s] Train Epoch: 26 [10240/13374 (77%)]\tLoss: 31.77\n",
      "[4m 25s] Train Epoch: 26 [12800/13374 (96%)]\tLoss: 32.24\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5575/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[4m 29s] Train Epoch: 27 [2560/13374 (19%)]\tLoss: 26.59\n",
      "[4m 31s] Train Epoch: 27 [5120/13374 (38%)]\tLoss: 25.50\n",
      "[4m 33s] Train Epoch: 27 [7680/13374 (57%)]\tLoss: 26.26\n",
      "[4m 34s] Train Epoch: 27 [10240/13374 (77%)]\tLoss: 26.38\n",
      "[4m 36s] Train Epoch: 27 [12800/13374 (96%)]\tLoss: 26.81\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5607/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[4m 40s] Train Epoch: 28 [2560/13374 (19%)]\tLoss: 19.68\n",
      "[4m 41s] Train Epoch: 28 [5120/13374 (38%)]\tLoss: 20.79\n",
      "[4m 43s] Train Epoch: 28 [7680/13374 (57%)]\tLoss: 21.83\n",
      "[4m 44s] Train Epoch: 28 [10240/13374 (77%)]\tLoss: 23.26\n",
      "[4m 47s] Train Epoch: 28 [12800/13374 (96%)]\tLoss: 24.54\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5589/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is English\n",
      "Nako is Japanese\n",
      "[4m 52s] Train Epoch: 29 [2560/13374 (19%)]\tLoss: 21.54\n",
      "[4m 53s] Train Epoch: 29 [5120/13374 (38%)]\tLoss: 21.40\n",
      "[4m 55s] Train Epoch: 29 [7680/13374 (57%)]\tLoss: 21.85\n",
      "[4m 58s] Train Epoch: 29 [10240/13374 (77%)]\tLoss: 22.27\n",
      "[5m 0s] Train Epoch: 29 [12800/13374 (96%)]\tLoss: 22.55\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5572/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[5m 4s] Train Epoch: 30 [2560/13374 (19%)]\tLoss: 18.00\n",
      "[5m 6s] Train Epoch: 30 [5120/13374 (38%)]\tLoss: 19.60\n",
      "[5m 8s] Train Epoch: 30 [7680/13374 (57%)]\tLoss: 20.89\n",
      "[5m 9s] Train Epoch: 30 [10240/13374 (77%)]\tLoss: 21.04\n",
      "[5m 11s] Train Epoch: 30 [12800/13374 (96%)]\tLoss: 20.91\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5581/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[5m 15s] Train Epoch: 31 [2560/13374 (19%)]\tLoss: 18.80\n",
      "[5m 17s] Train Epoch: 31 [5120/13374 (38%)]\tLoss: 18.20\n",
      "[5m 18s] Train Epoch: 31 [7680/13374 (57%)]\tLoss: 17.79\n",
      "[5m 20s] Train Epoch: 31 [10240/13374 (77%)]\tLoss: 18.40\n",
      "[5m 22s] Train Epoch: 31 [12800/13374 (96%)]\tLoss: 19.72\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5584/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[5m 25s] Train Epoch: 32 [2560/13374 (19%)]\tLoss: 17.34\n",
      "[5m 27s] Train Epoch: 32 [5120/13374 (38%)]\tLoss: 17.48\n",
      "[5m 28s] Train Epoch: 32 [7680/13374 (57%)]\tLoss: 17.82\n",
      "[5m 30s] Train Epoch: 32 [10240/13374 (77%)]\tLoss: 17.93\n",
      "[5m 32s] Train Epoch: 32 [12800/13374 (96%)]\tLoss: 18.80\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5555/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[5m 36s] Train Epoch: 33 [2560/13374 (19%)]\tLoss: 15.19\n",
      "[5m 37s] Train Epoch: 33 [5120/13374 (38%)]\tLoss: 15.73\n",
      "[5m 39s] Train Epoch: 33 [7680/13374 (57%)]\tLoss: 16.52\n",
      "[5m 40s] Train Epoch: 33 [10240/13374 (77%)]\tLoss: 17.18\n",
      "[5m 42s] Train Epoch: 33 [12800/13374 (96%)]\tLoss: 17.97\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5535/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[5m 46s] Train Epoch: 34 [2560/13374 (19%)]\tLoss: 14.41\n",
      "[5m 48s] Train Epoch: 34 [5120/13374 (38%)]\tLoss: 14.72\n",
      "[5m 49s] Train Epoch: 34 [7680/13374 (57%)]\tLoss: 15.70\n",
      "[5m 51s] Train Epoch: 34 [10240/13374 (77%)]\tLoss: 16.62\n",
      "[5m 52s] Train Epoch: 34 [12800/13374 (96%)]\tLoss: 17.21\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5557/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[5m 56s] Train Epoch: 35 [2560/13374 (19%)]\tLoss: 15.71\n",
      "[5m 58s] Train Epoch: 35 [5120/13374 (38%)]\tLoss: 17.03\n",
      "[5m 59s] Train Epoch: 35 [7680/13374 (57%)]\tLoss: 16.75\n",
      "[6m 1s] Train Epoch: 35 [10240/13374 (77%)]\tLoss: 17.35\n",
      "[6m 3s] Train Epoch: 35 [12800/13374 (96%)]\tLoss: 17.97\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5570/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[6m 7s] Train Epoch: 36 [2560/13374 (19%)]\tLoss: 13.93\n",
      "[6m 9s] Train Epoch: 36 [5120/13374 (38%)]\tLoss: 15.19\n",
      "[6m 10s] Train Epoch: 36 [7680/13374 (57%)]\tLoss: 15.19\n",
      "[6m 12s] Train Epoch: 36 [10240/13374 (77%)]\tLoss: 15.80\n",
      "[6m 14s] Train Epoch: 36 [12800/13374 (96%)]\tLoss: 16.29\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5514/6700 (82%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[6m 18s] Train Epoch: 37 [2560/13374 (19%)]\tLoss: 15.78\n",
      "[6m 20s] Train Epoch: 37 [5120/13374 (38%)]\tLoss: 16.68\n",
      "[6m 21s] Train Epoch: 37 [7680/13374 (57%)]\tLoss: 16.39\n",
      "[6m 23s] Train Epoch: 37 [10240/13374 (77%)]\tLoss: 16.86\n",
      "[6m 24s] Train Epoch: 37 [12800/13374 (96%)]\tLoss: 17.16\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5554/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is English\n",
      "Nako is Japanese\n",
      "[6m 28s] Train Epoch: 38 [2560/13374 (19%)]\tLoss: 13.98\n",
      "[6m 30s] Train Epoch: 38 [5120/13374 (38%)]\tLoss: 14.42\n",
      "[6m 32s] Train Epoch: 38 [7680/13374 (57%)]\tLoss: 15.10\n",
      "[6m 33s] Train Epoch: 38 [10240/13374 (77%)]\tLoss: 15.42\n",
      "[6m 35s] Train Epoch: 38 [12800/13374 (96%)]\tLoss: 15.43\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5565/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[6m 39s] Train Epoch: 39 [2560/13374 (19%)]\tLoss: 13.22\n",
      "[6m 40s] Train Epoch: 39 [5120/13374 (38%)]\tLoss: 12.40\n",
      "[6m 42s] Train Epoch: 39 [7680/13374 (57%)]\tLoss: 12.90\n",
      "[6m 44s] Train Epoch: 39 [10240/13374 (77%)]\tLoss: 13.56\n",
      "[6m 45s] Train Epoch: 39 [12800/13374 (96%)]\tLoss: 14.65\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5570/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[6m 49s] Train Epoch: 40 [2560/13374 (19%)]\tLoss: 13.56\n",
      "[6m 51s] Train Epoch: 40 [5120/13374 (38%)]\tLoss: 13.96\n",
      "[6m 52s] Train Epoch: 40 [7680/13374 (57%)]\tLoss: 14.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6m 54s] Train Epoch: 40 [10240/13374 (77%)]\tLoss: 15.09\n",
      "[6m 55s] Train Epoch: 40 [12800/13374 (96%)]\tLoss: 15.54\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5553/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[6m 59s] Train Epoch: 41 [2560/13374 (19%)]\tLoss: 14.15\n",
      "[7m 1s] Train Epoch: 41 [5120/13374 (38%)]\tLoss: 14.64\n",
      "[7m 2s] Train Epoch: 41 [7680/13374 (57%)]\tLoss: 16.07\n",
      "[7m 4s] Train Epoch: 41 [10240/13374 (77%)]\tLoss: 17.44\n",
      "[7m 5s] Train Epoch: 41 [12800/13374 (96%)]\tLoss: 18.65\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5538/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[7m 9s] Train Epoch: 42 [2560/13374 (19%)]\tLoss: 13.79\n",
      "[7m 11s] Train Epoch: 42 [5120/13374 (38%)]\tLoss: 15.64\n",
      "[7m 12s] Train Epoch: 42 [7680/13374 (57%)]\tLoss: 16.57\n",
      "[7m 14s] Train Epoch: 42 [10240/13374 (77%)]\tLoss: 17.04\n",
      "[7m 15s] Train Epoch: 42 [12800/13374 (96%)]\tLoss: 17.10\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5514/6700 (82%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[7m 19s] Train Epoch: 43 [2560/13374 (19%)]\tLoss: 12.91\n",
      "[7m 21s] Train Epoch: 43 [5120/13374 (38%)]\tLoss: 13.60\n",
      "[7m 22s] Train Epoch: 43 [7680/13374 (57%)]\tLoss: 14.78\n",
      "[7m 24s] Train Epoch: 43 [10240/13374 (77%)]\tLoss: 14.59\n",
      "[7m 25s] Train Epoch: 43 [12800/13374 (96%)]\tLoss: 15.32\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5564/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[7m 29s] Train Epoch: 44 [2560/13374 (19%)]\tLoss: 11.66\n",
      "[7m 30s] Train Epoch: 44 [5120/13374 (38%)]\tLoss: 12.44\n",
      "[7m 32s] Train Epoch: 44 [7680/13374 (57%)]\tLoss: 12.65\n",
      "[7m 34s] Train Epoch: 44 [10240/13374 (77%)]\tLoss: 13.43\n",
      "[7m 35s] Train Epoch: 44 [12800/13374 (96%)]\tLoss: 14.09\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5558/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[7m 39s] Train Epoch: 45 [2560/13374 (19%)]\tLoss: 13.19\n",
      "[7m 40s] Train Epoch: 45 [5120/13374 (38%)]\tLoss: 12.68\n",
      "[7m 42s] Train Epoch: 45 [7680/13374 (57%)]\tLoss: 12.52\n",
      "[7m 43s] Train Epoch: 45 [10240/13374 (77%)]\tLoss: 13.27\n",
      "[7m 45s] Train Epoch: 45 [12800/13374 (96%)]\tLoss: 13.36\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5556/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[7m 49s] Train Epoch: 46 [2560/13374 (19%)]\tLoss: 11.80\n",
      "[7m 50s] Train Epoch: 46 [5120/13374 (38%)]\tLoss: 13.05\n",
      "[7m 52s] Train Epoch: 46 [7680/13374 (57%)]\tLoss: 13.18\n",
      "[7m 53s] Train Epoch: 46 [10240/13374 (77%)]\tLoss: 13.56\n",
      "[7m 55s] Train Epoch: 46 [12800/13374 (96%)]\tLoss: 13.87\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5558/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Irish\n",
      "Nako is Japanese\n",
      "[7m 59s] Train Epoch: 47 [2560/13374 (19%)]\tLoss: 9.56\n",
      "[8m 0s] Train Epoch: 47 [5120/13374 (38%)]\tLoss: 12.16\n",
      "[8m 2s] Train Epoch: 47 [7680/13374 (57%)]\tLoss: 12.15\n",
      "[8m 3s] Train Epoch: 47 [10240/13374 (77%)]\tLoss: 12.65\n",
      "[8m 5s] Train Epoch: 47 [12800/13374 (96%)]\tLoss: 12.86\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5568/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[8m 8s] Train Epoch: 48 [2560/13374 (19%)]\tLoss: 12.95\n",
      "[8m 10s] Train Epoch: 48 [5120/13374 (38%)]\tLoss: 12.49\n",
      "[8m 12s] Train Epoch: 48 [7680/13374 (57%)]\tLoss: 12.78\n",
      "[8m 13s] Train Epoch: 48 [10240/13374 (77%)]\tLoss: 12.55\n",
      "[8m 15s] Train Epoch: 48 [12800/13374 (96%)]\tLoss: 13.08\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5578/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[8m 18s] Train Epoch: 49 [2560/13374 (19%)]\tLoss: 13.07\n",
      "[8m 20s] Train Epoch: 49 [5120/13374 (38%)]\tLoss: 12.81\n",
      "[8m 21s] Train Epoch: 49 [7680/13374 (57%)]\tLoss: 11.86\n",
      "[8m 23s] Train Epoch: 49 [10240/13374 (77%)]\tLoss: 12.05\n",
      "[8m 25s] Train Epoch: 49 [12800/13374 (96%)]\tLoss: 12.53\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5565/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[8m 28s] Train Epoch: 50 [2560/13374 (19%)]\tLoss: 12.31\n",
      "[8m 30s] Train Epoch: 50 [5120/13374 (38%)]\tLoss: 12.02\n",
      "[8m 31s] Train Epoch: 50 [7680/13374 (57%)]\tLoss: 12.40\n",
      "[8m 33s] Train Epoch: 50 [10240/13374 (77%)]\tLoss: 12.01\n",
      "[8m 34s] Train Epoch: 50 [12800/13374 (96%)]\tLoss: 12.26\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5571/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[8m 39s] Train Epoch: 51 [2560/13374 (19%)]\tLoss: 10.24\n",
      "[8m 41s] Train Epoch: 51 [5120/13374 (38%)]\tLoss: 10.25\n",
      "[8m 42s] Train Epoch: 51 [7680/13374 (57%)]\tLoss: 10.87\n",
      "[8m 44s] Train Epoch: 51 [10240/13374 (77%)]\tLoss: 11.36\n",
      "[8m 45s] Train Epoch: 51 [12800/13374 (96%)]\tLoss: 11.91\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5569/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[8m 49s] Train Epoch: 52 [2560/13374 (19%)]\tLoss: 9.86\n",
      "[8m 50s] Train Epoch: 52 [5120/13374 (38%)]\tLoss: 10.00\n",
      "[8m 52s] Train Epoch: 52 [7680/13374 (57%)]\tLoss: 10.60\n",
      "[8m 54s] Train Epoch: 52 [10240/13374 (77%)]\tLoss: 11.20\n",
      "[8m 55s] Train Epoch: 52 [12800/13374 (96%)]\tLoss: 11.72\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5561/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[8m 59s] Train Epoch: 53 [2560/13374 (19%)]\tLoss: 10.18\n",
      "[9m 0s] Train Epoch: 53 [5120/13374 (38%)]\tLoss: 10.04\n",
      "[9m 2s] Train Epoch: 53 [7680/13374 (57%)]\tLoss: 10.71\n",
      "[9m 3s] Train Epoch: 53 [10240/13374 (77%)]\tLoss: 10.78\n",
      "[9m 5s] Train Epoch: 53 [12800/13374 (96%)]\tLoss: 11.38\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5562/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[9m 9s] Train Epoch: 54 [2560/13374 (19%)]\tLoss: 9.11\n",
      "[9m 10s] Train Epoch: 54 [5120/13374 (38%)]\tLoss: 8.82\n",
      "[9m 12s] Train Epoch: 54 [7680/13374 (57%)]\tLoss: 9.66\n",
      "[9m 13s] Train Epoch: 54 [10240/13374 (77%)]\tLoss: 10.41\n",
      "[9m 15s] Train Epoch: 54 [12800/13374 (96%)]\tLoss: 11.33\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5561/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[9m 19s] Train Epoch: 55 [2560/13374 (19%)]\tLoss: 8.62\n",
      "[9m 21s] Train Epoch: 55 [5120/13374 (38%)]\tLoss: 9.54\n",
      "[9m 22s] Train Epoch: 55 [7680/13374 (57%)]\tLoss: 10.11\n",
      "[9m 24s] Train Epoch: 55 [10240/13374 (77%)]\tLoss: 10.45\n",
      "[9m 25s] Train Epoch: 55 [12800/13374 (96%)]\tLoss: 10.99\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5573/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is Chinese\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[9m 29s] Train Epoch: 56 [2560/13374 (19%)]\tLoss: 10.28\n",
      "[9m 31s] Train Epoch: 56 [5120/13374 (38%)]\tLoss: 10.36\n",
      "[9m 32s] Train Epoch: 56 [7680/13374 (57%)]\tLoss: 10.46\n",
      "[9m 34s] Train Epoch: 56 [10240/13374 (77%)]\tLoss: 11.04\n",
      "[9m 35s] Train Epoch: 56 [12800/13374 (96%)]\tLoss: 11.34\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5549/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[9m 39s] Train Epoch: 57 [2560/13374 (19%)]\tLoss: 9.25\n",
      "[9m 40s] Train Epoch: 57 [5120/13374 (38%)]\tLoss: 9.85\n",
      "[9m 42s] Train Epoch: 57 [7680/13374 (57%)]\tLoss: 10.60\n",
      "[9m 44s] Train Epoch: 57 [10240/13374 (77%)]\tLoss: 11.33\n",
      "[9m 45s] Train Epoch: 57 [12800/13374 (96%)]\tLoss: 11.53\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5549/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is English\n",
      "Nako is Japanese\n",
      "[9m 49s] Train Epoch: 58 [2560/13374 (19%)]\tLoss: 8.54\n",
      "[9m 50s] Train Epoch: 58 [5120/13374 (38%)]\tLoss: 8.85\n",
      "[9m 52s] Train Epoch: 58 [7680/13374 (57%)]\tLoss: 9.79\n",
      "[9m 53s] Train Epoch: 58 [10240/13374 (77%)]\tLoss: 10.42\n",
      "[9m 55s] Train Epoch: 58 [12800/13374 (96%)]\tLoss: 11.01\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5551/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[9m 59s] Train Epoch: 59 [2560/13374 (19%)]\tLoss: 8.81\n",
      "[10m 0s] Train Epoch: 59 [5120/13374 (38%)]\tLoss: 10.41\n",
      "[10m 2s] Train Epoch: 59 [7680/13374 (57%)]\tLoss: 10.75\n",
      "[10m 3s] Train Epoch: 59 [10240/13374 (77%)]\tLoss: 10.98\n",
      "[10m 5s] Train Epoch: 59 [12800/13374 (96%)]\tLoss: 10.95\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5559/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Irish\n",
      "Nako is Japanese\n",
      "[10m 9s] Train Epoch: 60 [2560/13374 (19%)]\tLoss: 8.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10m 10s] Train Epoch: 60 [5120/13374 (38%)]\tLoss: 9.02\n",
      "[10m 12s] Train Epoch: 60 [7680/13374 (57%)]\tLoss: 9.89\n",
      "[10m 13s] Train Epoch: 60 [10240/13374 (77%)]\tLoss: 10.75\n",
      "[10m 15s] Train Epoch: 60 [12800/13374 (96%)]\tLoss: 11.38\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5563/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[10m 18s] Train Epoch: 61 [2560/13374 (19%)]\tLoss: 9.27\n",
      "[10m 20s] Train Epoch: 61 [5120/13374 (38%)]\tLoss: 8.98\n",
      "[10m 21s] Train Epoch: 61 [7680/13374 (57%)]\tLoss: 9.98\n",
      "[10m 23s] Train Epoch: 61 [10240/13374 (77%)]\tLoss: 10.68\n",
      "[10m 25s] Train Epoch: 61 [12800/13374 (96%)]\tLoss: 11.59\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5553/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[10m 29s] Train Epoch: 62 [2560/13374 (19%)]\tLoss: 13.34\n",
      "[10m 30s] Train Epoch: 62 [5120/13374 (38%)]\tLoss: 14.74\n",
      "[10m 32s] Train Epoch: 62 [7680/13374 (57%)]\tLoss: 15.51\n",
      "[10m 33s] Train Epoch: 62 [10240/13374 (77%)]\tLoss: 18.01\n",
      "[10m 35s] Train Epoch: 62 [12800/13374 (96%)]\tLoss: 20.18\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5492/6700 (82%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[10m 38s] Train Epoch: 63 [2560/13374 (19%)]\tLoss: 29.72\n",
      "[10m 40s] Train Epoch: 63 [5120/13374 (38%)]\tLoss: 29.37\n",
      "[10m 42s] Train Epoch: 63 [7680/13374 (57%)]\tLoss: 28.97\n",
      "[10m 44s] Train Epoch: 63 [10240/13374 (77%)]\tLoss: 27.69\n",
      "[10m 46s] Train Epoch: 63 [12800/13374 (96%)]\tLoss: 26.70\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5548/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[10m 49s] Train Epoch: 64 [2560/13374 (19%)]\tLoss: 15.07\n",
      "[10m 51s] Train Epoch: 64 [5120/13374 (38%)]\tLoss: 15.17\n",
      "[10m 52s] Train Epoch: 64 [7680/13374 (57%)]\tLoss: 16.04\n",
      "[10m 54s] Train Epoch: 64 [10240/13374 (77%)]\tLoss: 16.98\n",
      "[10m 56s] Train Epoch: 64 [12800/13374 (96%)]\tLoss: 17.54\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5542/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[11m 0s] Train Epoch: 65 [2560/13374 (19%)]\tLoss: 11.95\n",
      "[11m 2s] Train Epoch: 65 [5120/13374 (38%)]\tLoss: 12.28\n",
      "[11m 4s] Train Epoch: 65 [7680/13374 (57%)]\tLoss: 13.27\n",
      "[11m 5s] Train Epoch: 65 [10240/13374 (77%)]\tLoss: 13.29\n",
      "[11m 7s] Train Epoch: 65 [12800/13374 (96%)]\tLoss: 13.59\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5579/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[11m 11s] Train Epoch: 66 [2560/13374 (19%)]\tLoss: 11.98\n",
      "[11m 12s] Train Epoch: 66 [5120/13374 (38%)]\tLoss: 11.72\n",
      "[11m 14s] Train Epoch: 66 [7680/13374 (57%)]\tLoss: 11.92\n",
      "[11m 16s] Train Epoch: 66 [10240/13374 (77%)]\tLoss: 11.65\n",
      "[11m 18s] Train Epoch: 66 [12800/13374 (96%)]\tLoss: 12.01\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5587/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[11m 21s] Train Epoch: 67 [2560/13374 (19%)]\tLoss: 7.71\n",
      "[11m 23s] Train Epoch: 67 [5120/13374 (38%)]\tLoss: 8.52\n",
      "[11m 25s] Train Epoch: 67 [7680/13374 (57%)]\tLoss: 9.42\n",
      "[11m 26s] Train Epoch: 67 [10240/13374 (77%)]\tLoss: 10.32\n",
      "[11m 28s] Train Epoch: 67 [12800/13374 (96%)]\tLoss: 11.09\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5595/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[11m 32s] Train Epoch: 68 [2560/13374 (19%)]\tLoss: 9.07\n",
      "[11m 33s] Train Epoch: 68 [5120/13374 (38%)]\tLoss: 8.89\n",
      "[11m 35s] Train Epoch: 68 [7680/13374 (57%)]\tLoss: 10.28\n",
      "[11m 38s] Train Epoch: 68 [10240/13374 (77%)]\tLoss: 10.06\n",
      "[11m 42s] Train Epoch: 68 [12800/13374 (96%)]\tLoss: 10.45\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5589/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is German\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[11m 48s] Train Epoch: 69 [2560/13374 (19%)]\tLoss: 9.04\n",
      "[11m 50s] Train Epoch: 69 [5120/13374 (38%)]\tLoss: 8.83\n",
      "[11m 52s] Train Epoch: 69 [7680/13374 (57%)]\tLoss: 9.49\n",
      "[11m 53s] Train Epoch: 69 [10240/13374 (77%)]\tLoss: 10.01\n",
      "[11m 55s] Train Epoch: 69 [12800/13374 (96%)]\tLoss: 10.34\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5595/6700 (84%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[11m 58s] Train Epoch: 70 [2560/13374 (19%)]\tLoss: 8.40\n",
      "[12m 0s] Train Epoch: 70 [5120/13374 (38%)]\tLoss: 9.34\n",
      "[12m 1s] Train Epoch: 70 [7680/13374 (57%)]\tLoss: 10.19\n",
      "[12m 3s] Train Epoch: 70 [10240/13374 (77%)]\tLoss: 10.68\n",
      "[12m 5s] Train Epoch: 70 [12800/13374 (96%)]\tLoss: 10.82\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5573/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[12m 8s] Train Epoch: 71 [2560/13374 (19%)]\tLoss: 8.51\n",
      "[12m 10s] Train Epoch: 71 [5120/13374 (38%)]\tLoss: 9.20\n",
      "[12m 11s] Train Epoch: 71 [7680/13374 (57%)]\tLoss: 10.13\n",
      "[12m 13s] Train Epoch: 71 [10240/13374 (77%)]\tLoss: 9.96\n",
      "[12m 15s] Train Epoch: 71 [12800/13374 (96%)]\tLoss: 10.47\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5573/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[12m 19s] Train Epoch: 72 [2560/13374 (19%)]\tLoss: 9.17\n",
      "[12m 20s] Train Epoch: 72 [5120/13374 (38%)]\tLoss: 9.62\n",
      "[12m 22s] Train Epoch: 72 [7680/13374 (57%)]\tLoss: 9.96\n",
      "[12m 23s] Train Epoch: 72 [10240/13374 (77%)]\tLoss: 10.34\n",
      "[12m 25s] Train Epoch: 72 [12800/13374 (96%)]\tLoss: 10.51\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5583/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[12m 29s] Train Epoch: 73 [2560/13374 (19%)]\tLoss: 8.66\n",
      "[12m 30s] Train Epoch: 73 [5120/13374 (38%)]\tLoss: 8.74\n",
      "[12m 32s] Train Epoch: 73 [7680/13374 (57%)]\tLoss: 9.03\n",
      "[12m 33s] Train Epoch: 73 [10240/13374 (77%)]\tLoss: 9.47\n",
      "[12m 35s] Train Epoch: 73 [12800/13374 (96%)]\tLoss: 10.04\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5594/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[12m 39s] Train Epoch: 74 [2560/13374 (19%)]\tLoss: 9.56\n",
      "[12m 40s] Train Epoch: 74 [5120/13374 (38%)]\tLoss: 9.36\n",
      "[12m 42s] Train Epoch: 74 [7680/13374 (57%)]\tLoss: 9.69\n",
      "[12m 43s] Train Epoch: 74 [10240/13374 (77%)]\tLoss: 9.83\n",
      "[12m 45s] Train Epoch: 74 [12800/13374 (96%)]\tLoss: 10.01\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5579/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[12m 49s] Train Epoch: 75 [2560/13374 (19%)]\tLoss: 8.83\n",
      "[12m 51s] Train Epoch: 75 [5120/13374 (38%)]\tLoss: 8.66\n",
      "[12m 53s] Train Epoch: 75 [7680/13374 (57%)]\tLoss: 8.97\n",
      "[12m 54s] Train Epoch: 75 [10240/13374 (77%)]\tLoss: 9.91\n",
      "[12m 56s] Train Epoch: 75 [12800/13374 (96%)]\tLoss: 10.05\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5592/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[13m 0s] Train Epoch: 76 [2560/13374 (19%)]\tLoss: 8.03\n",
      "[13m 1s] Train Epoch: 76 [5120/13374 (38%)]\tLoss: 8.53\n",
      "[13m 3s] Train Epoch: 76 [7680/13374 (57%)]\tLoss: 8.88\n",
      "[13m 4s] Train Epoch: 76 [10240/13374 (77%)]\tLoss: 9.82\n",
      "[13m 6s] Train Epoch: 76 [12800/13374 (96%)]\tLoss: 9.90\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5579/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[13m 10s] Train Epoch: 77 [2560/13374 (19%)]\tLoss: 9.36\n",
      "[13m 11s] Train Epoch: 77 [5120/13374 (38%)]\tLoss: 8.97\n",
      "[13m 13s] Train Epoch: 77 [7680/13374 (57%)]\tLoss: 9.45\n",
      "[13m 14s] Train Epoch: 77 [10240/13374 (77%)]\tLoss: 9.49\n",
      "[13m 16s] Train Epoch: 77 [12800/13374 (96%)]\tLoss: 9.96\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5587/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Czech\n",
      "Nako is Japanese\n",
      "[13m 20s] Train Epoch: 78 [2560/13374 (19%)]\tLoss: 8.99\n",
      "[13m 22s] Train Epoch: 78 [5120/13374 (38%)]\tLoss: 8.71\n",
      "[13m 24s] Train Epoch: 78 [7680/13374 (57%)]\tLoss: 9.16\n",
      "[13m 25s] Train Epoch: 78 [10240/13374 (77%)]\tLoss: 9.34\n",
      "[13m 27s] Train Epoch: 78 [12800/13374 (96%)]\tLoss: 9.79\n",
      "evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 5594/6700 (83%)\n",
      "\n",
      "Sung is Chinese\n",
      "Jungwoo is English\n",
      "Soojin is Dutch\n",
      "Nako is Japanese\n",
      "[13m 31s] Train Epoch: 79 [2560/13374 (19%)]\tLoss: 7.09\n",
      "[13m 33s] Train Epoch: 79 [5120/13374 (38%)]\tLoss: 8.49\n",
      "[13m 34s] Train Epoch: 79 [7680/13374 (57%)]\tLoss: 9.36\n",
      "[13m 36s] Train Epoch: 79 [10240/13374 (77%)]\tLoss: 9.67\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        classifier = torch.nn.DataParallel(classifier)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        classifier.cuda()\n",
    "        \n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    start = time.time()\n",
    "    print(\"Training for %d epoch...\" % N_EPOCHS)\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        train()\n",
    "        \n",
    "        test()\n",
    "        \n",
    "        # Testing several samples\n",
    "        test(\"Sung\")\n",
    "        test(\"Jungwoo\")\n",
    "        test(\"Soojin\")\n",
    "        test(\"Nako\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea6d51",
   "metadata": {},
   "source": [
    "# char rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "08c902de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T08:32:16.056689Z",
     "start_time": "2024-02-19T08:32:16.051703Z"
    }
   },
   "outputs": [],
   "source": [
    "from text_loader import TextDataset\n",
    "\n",
    "hidden_size = 128\n",
    "n_layers = 3\n",
    "batch_size = 3*64\n",
    "n_epochs = 100\n",
    "n_characters  = 128 #ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "da44c701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T08:32:16.315303Z",
     "start_time": "2024-02-19T08:32:16.282439Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embed = self.embedding(input.view(1,-1)) # S(=1) x I\n",
    "        embed = embed.view(1,1,-1) # S(=1) x B(=1) x I (embedding size)\n",
    "        output, hidden = self.gru(embed, hidden)\n",
    "        output = self.linear(output.view(1, -1)) # S(=1) x I\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if torch.cuda.is_available():\n",
    "            hidden = torch.zeros(self.n_layers, 1, self.hidden_size).cuda()\n",
    "        else:\n",
    "            hidden = torch.zeros(self.n_layers, 1, self.hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "87e189b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T08:32:16.517064Z",
     "start_time": "2024-02-19T08:32:16.501816Z"
    }
   },
   "outputs": [],
   "source": [
    "def str2tensor(string):\n",
    "    tensor = [ord(c) for c in string]\n",
    "    tensor = torch.LongTensor(tensor)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.cuda()\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def generate(decoder, prime_str = 'A', predict_len = 100, temperature = 0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = str2tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "    \n",
    "    for pp in range(len(prime_str) -1):\n",
    "        _, hidden = decoder(prime_input[pp], hidden)\n",
    "    \n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for pp in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        predicted_char = chr(top_i)\n",
    "        predicted += predicted_char\n",
    "        inp = str2tensor(predicted_char)\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def train_teacher_forching(line):\n",
    "    input = str2tensor(line[:-1])\n",
    "    target = str2tensor(line[1:])\n",
    "\n",
    "    hidden = decoder.init_hidden()\n",
    "    loss = 0\n",
    "\n",
    "    for cc in range(len(input)):\n",
    "        output, hidden = decoder(input[cc], hidden)\n",
    "        loss += criterion(output, target[cc])\n",
    "\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / len(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8d7c6adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T08:32:16.801538Z",
     "start_time": "2024-02-19T08:32:16.783556Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(line):\n",
    "    input = str2tensor(line[:-1])\n",
    "    target = str2tensor(line[1:])\n",
    "\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder_in = input[0]\n",
    "    loss = 0\n",
    "\n",
    "    for cc in range(len(input)):\n",
    "        output, hidden = decoder(decoder_in, hidden)\n",
    "        \n",
    "        # 모델의 출력에서 가장 높은 확률을 가진 문자의 인덱스로 변환\n",
    "        # torch.argmax 함수를 사용하여 가장 높은 값의 인덱스를 가져옴\n",
    "        predicted_char_index = torch.argmax(output)\n",
    "        \n",
    "        # 타겟 데이터는 정수 형태이므로, torch.tensor 형태로 변환\n",
    "        target_char_index = torch.tensor(target[cc])\n",
    "        \n",
    "        # 손실을 계산할 때 모델의 출력과 타겟 데이터의 크기가 일치해야 함\n",
    "        # 따라서 모델의 출력과 타겟 데이터를 각각 인덱스로 비교하여 손실을 계산\n",
    "        loss += criterion(output.view(1, -1), target_char_index.view(1))\n",
    "        \n",
    "        # 다음 입력으로 사용할 디코더의 입력 설정\n",
    "        decoder_in = target_char_index\n",
    "    \n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / len(input)  # loss.item()을 사용하여 손실 값을 가져옴\n",
    "        \n",
    "# 기존 코드 중에서 size 안 맞아서 안 되는 부분 수정 전 원본.\n",
    "#         #loss += criterion(output, target[c])\n",
    "#         decoder_in = output.max(1)[1]\n",
    "\n",
    "#     decoder.zero_grad()\n",
    "#     loss.backward()\n",
    "#     decoder_optimizer.step()\n",
    "\n",
    "#     return loss.data[0] / len(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "30510bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T09:00:50.358562Z",
     "start_time": "2024-02-19T08:32:17.199806Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs...\n",
      "[(1 1%) loss: 4.8559]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10331\\AppData\\Local\\Temp/ipykernel_16068/1995632507.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_char_index = torch.tensor(target[cc])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh3\u0005t;\u0000Dts\u000b",
      ":\u001b\t/\n",
      "a5$j\u00030\u0003\u001c",
      "o3swq\u0012Z\u0006!qF}CP\u001e",
      "ez\n",
      "vY)n\u000b",
      "szi\u001d",
      "\u0014%cQDV p)jz\u0014@Rl/S!&|Rbb\u0003Y\u001c",
      "(Omf\f",
      "n>z3~!\u00177)cjO;^0)P*u8 \n",
      "\n",
      "[(1 1%) loss: 2.8737]\n",
      "Whtmhdsranirostritescsaseerwysirosraswteaa'enrdg,ss,raorieeeedarryaleosmsoahsoeemwdddeathlanhvreeatdta \n",
      "\n",
      "[(2 2%) loss: 3.0422]\n",
      "Whisiiaottehderh-wynatnennnosentaotsosaesaithsorayneeyere.aidslreatiaedt'blshvceoomhsetalmaeerhelau:t: \n",
      "\n",
      "[(2 2%) loss: 3.3414]\n",
      "Wheroawineatroecrawerteowomebnler-msfrtodo,urhltosrkenskeesmenmotorthe:t!iemrryetoslnrelettiausnastsod \n",
      "\n",
      "[(3 3%) loss: 2.8198]\n",
      "Whimbarvesledsesyyhetmrothouisvayononhearesenmobris,oterthhbnesrpavtafaans!thylepethathrebetothirowhem \n",
      "\n",
      "[(3 3%) loss: 2.7395]\n",
      "Whmatheondyaonlamhendlestyouslendetedananther,entendethindlenwhelvedstufbesthenthondnttofredenc,plalsi \n",
      "\n",
      "[(4 4%) loss: 2.9374]\n",
      "Whartnesewi;yo!imcoreawfemeveleirselbeounand.nedaa:tenwedeshen'teneundthastheun,othee;enereshenelthocr \n",
      "\n",
      "[(4 4%) loss: 2.7550]\n",
      "Wheelsizeorsenitinanthwenogarperethearbalgarnyiyhenesadeswindalbsousthalnam.esdoinandnouswindomouc,ege \n",
      "\n",
      "[(5 5%) loss: 2.3447]\n",
      "Wheiunsweenteill,thinek,menesil'lrbomeyolthateindutheatstheangenasnote:nothagemarsolethoufefildhenthan \n",
      "\n",
      "[(5 5%) loss: 2.4850]\n",
      "Wheareforbe,lytheshesil,vovesforhouthoureasingiythoudthacethefrothmhincallfifheshoursarswonthemerbiwht \n",
      "\n",
      "[(6 6%) loss: 2.4556]\n",
      "Whandfabandidallordweran?taryourisfa:tredumutmethowetindaldangsserasheatwhvputherswethe,incentiniondri \n",
      "\n",
      "[(6 6%) loss: 2.6626]\n",
      "Wherellnitteresthendherterelein'theiradousingadeveldaswersas:de,terstheanceyerbe,,aralverondsory,iripo \n",
      "\n",
      "[(7 7%) loss: 2.9094]\n",
      "Whengeng,andwif,ifskecocfheeinglard,someadimewsilngacdursherele,whenchorlor,fordatenmresentilseicormeo \n",
      "\n",
      "[(7 7%) loss: 2.6681]\n",
      "Wheckare,noidtheallsirseltheyhaysedsillblourhyoudagapyourshertendisenferenherandbroteherybrlelond,anle \n",
      "\n",
      "[(8 8%) loss: 2.5736]\n",
      "Whatissarteiurfolandbe,swyhainthimelassthethesofthears,youmethenofthatheafyouriettheemrounjerendsuy,so \n",
      "\n",
      "[(8 8%) loss: 2.1684]\n",
      "Whelasttitheredfordedsounceoniseepopertogoobatemedared,younderedmeprelypaswoothnotscrofwekencetherosey \n",
      "\n",
      "[(9 9%) loss: 2.6513]\n",
      "Whel''llobestle.kall,wer,wourlork.,norckofeamemeandworcie,brenfatiseapar,ortswahirenlensseacher,andnod \n",
      "\n",
      "[(9 9%) loss: 2.4449]\n",
      "Whisandhacling:thi::forofthoudblowthouardforesakebapesand'cathi:dipiegh:thingfaresarstomeres:sotfto-?s \n",
      "\n",
      "[(10 10%) loss: 2.0729]\n",
      "Whereawerhereiifmimeremycunsfore,ifengther,couselewicethattekisaybyoutirchintententeglethe,whereleandh \n",
      "\n",
      "[(10 10%) loss: 2.6978]\n",
      "Wheermymetherstoftengesthewereestoothegrimethichisethihour,hesellyould,,hene.,ththeres,berpathesetoher \n",
      "\n",
      "[(11 11%) loss: 2.3972]\n",
      "Whis:meningmeinsonerthithercourein,mrenceinhardyceismylord.myouremunttenle?sontmeesbendofthatb.breofda \n",
      "\n",
      "[(11 11%) loss: 2.3696]\n",
      "Whounthenepthegeasfentilpreagessofrordstill'dyourestthonaseproteyifmeanmydeard.,noyoursifenomosmie,goe \n",
      "\n",
      "[(12 12%) loss: 2.2148]\n",
      "Whathandandinehesadwaremylestlightyouvenshickingbuttalsonoftheredtaryamermytheagles.thehakes.ifusoyhou \n",
      "\n",
      "[(12 12%) loss: 2.2264]\n",
      "Whelovesphardyourihapletsyournicould,wellofsyus,ser.heat,reworktobtouts:homsnockmepronfortirpurtustont \n",
      "\n",
      "[(13 13%) loss: 2.4543]\n",
      "Wheretherindaycoreheresadnowtomemingthentifememingtowe,thanferesasent.conlore:would.ifandoferold,fomer \n",
      "\n",
      "[(13 13%) loss: 2.0974]\n",
      "Wheerdyoubest.laidtheeby'toflikenay,shecomnoutnour.donady:wellbutthenoster,there;serstothenothered,cou \n",
      "\n",
      "[(14 14%) loss: 2.1244]\n",
      "Whathing:bour,worthoughtofolid,seromegriousimenovelnowthevereansthathed'd.wheround,anddasta?forehather \n",
      "\n",
      "[(14 14%) loss: 1.4725]\n",
      "Whowyloldsovewardsullensirfollis.goreandyourrothershoursssalstsbeinetistoyoutletsart,myday,;brofesight \n",
      "\n",
      "[(15 15%) loss: 2.0143]\n",
      "Wherthetamcestitssharderdosethereenofcondrithtertohylove,thatesfortisroveaveshewearce,andmincedinditen \n",
      "\n",
      "[(15 15%) loss: 1.8542]\n",
      "Whimtansomeanot,ingthen,whatthemie,andsofensreamin,myformictanrger;your,sonotonthingtobearcelfeather,n \n",
      "\n",
      "[(16 16%) loss: 2.4216]\n",
      "Whofthewannewthenhemillamtosethereandstillssiltthyousalancalliellnem,illei.ry.thehthenmylouldnositen,l \n",
      "\n",
      "[(16 16%) loss: 2.5252]\n",
      "Whemerin:i:utillnotlivehe,thephingdaceenothiss,meakthesardthoudoud,nother,nothouthere,younwit'twards,i \n",
      "\n",
      "[(17 17%) loss: 2.4659]\n",
      "Whenchandarewound,--fork:filings.youtooherenel:firs:iftiserolfordingyoumukenothard.vereingmich,aftnott \n",
      "\n",
      "[(17 17%) loss: 1.3412]\n",
      "Whyconsweredadum:ofrurcharnius,courestinmytingslord.,bearyourstorli;estertogresed:mustthathewinofthowt \n",
      "\n",
      "[(18 18%) loss: 1.7364]\n",
      "Whaticameismalbesivesterry;mylovearsojoyhaveforaveandyourtoffraugh!you.afagiandbedome,inasthoughardowh \n",
      "\n",
      "[(18 18%) loss: 2.0411]\n",
      "Whowpery;lownthemberetrus:ifpirairtroane'st,tothewaringrisbebutisnotbuen;you,londhering,andworld.froth \n",
      "\n",
      "[(19 19%) loss: 2.4110]\n",
      "Whowmehiswhichardshardofwithicountofolluke,been;spepcons,stendrardiustlen.andiprickingedtherewome.shom \n",
      "\n",
      "[(19 19%) loss: 2.2080]\n",
      "Whistingoandthisstellbetoometheststhethesyouremetrotersinttheclays;then,whecullouteinourwhessay,coon.w \n",
      "\n",
      "[(20 20%) loss: 1.2597]\n",
      "Whone,selveansto'thissone,andgreak'sgyou,mysyestetruke.sersyou,astoodersofmentodespeveretit,rothallbep \n",
      "\n",
      "[(20 20%) loss: 2.3726]\n",
      "Where,ipringovermemytothiskourtent'stromegremeyetonghinkifhisseehorting,dorio:,iflomeofthou,nowthetold \n",
      "\n",
      "[(21 21%) loss: 2.1112]\n",
      "Whewouldner.:mylordneepnanderand.thalfeofuspead't,sir,ifanchink,orcallingtheycalis,nowstoome,thereoffo \n",
      "\n",
      "[(21 21%) loss: 2.6358]\n",
      "Whouphandthoudother,ilifeathelfaryswerfurehischandswouldstoskwiththehows,where,shallsthereartistheweal \n",
      "\n",
      "[(22 22%) loss: 2.5673]\n",
      "Whothhisforthemork,sames,thetherewillandsarcestouthdearthaporasbyshallandcomemeselfloos.thelearhadsing \n",
      "\n",
      "[(22 22%) loss: 2.0316]\n",
      "Whard,ismarintying,irfillow;him,and,iandtheyyou,awastbyyouforethat,henrenbed,ere.thastheaceledwiththes \n",
      "\n",
      "[(23 23%) loss: 2.4349]\n",
      "Whewerengceith:brebesingthisto?enince,letreathdream.hasselfleenshisaptcondistedthink'sregitem.contothi \n",
      "\n",
      "[(23 23%) loss: 2.3626]\n",
      "Whethenway,asandpromdeidive,suchishoullyou;fortheheimenwithshedtheebuthendy.forrowhipsanfeverseeture.! \n",
      "\n",
      "[(24 24%) loss: 2.2878]\n",
      "Wheirer:sobourtitjello'strenefrome,sillague.theromorepostofhisurbushopent.----i;perthey,butmoreenmydor \n",
      "\n",
      "[(24 24%) loss: 2.0368]\n",
      "Where,itsay,heavefreapter,serve,ishalli'dofforeasinmelay.theunthewillwhees,icometothealtboseading,ifhe \n",
      "\n",
      "[(25 25%) loss: 1.8955]\n",
      "Whenfeendershowthatprasmear.thinherefeedoo,myde-diestsrans;ifpersshalland,jeechyours.ipano.godingpeari \n",
      "\n",
      "[(25 25%) loss: 2.1616]\n",
      "Whavetherewasesiffaid:ihavetheare,sosilesandbecrox,thouthemightwife;ightslestthatdetodring.havewordsto \n",
      "\n",
      "[(26 26%) loss: 2.3182]\n",
      "Whellobuck:asitipetheyhallbetheysay,shewithhere.panstay?ityear?iraveep,isherderedthatgethisdies:butyou \n",
      "\n",
      "[(26 26%) loss: 2.1791]\n",
      "Wheirhimthischnotme,itman..me,tosomyseher,mylord.soyourevinger,bycomeshimsmingwhosegethistru.edrrowasg \n",
      "\n",
      "[(27 27%) loss: 2.3059]\n",
      "Wheavewend,whatebidangedringmanpeiting.whatandman,whomeihadloweldwan:mysethem?manyoftheseathershegeswe \n",
      "\n",
      "[(27 27%) loss: 2.8227]\n",
      "Whath'ds,thesaresmathoushalon.cuthere'tisthey,let.thereyoursaciousmakesrome,andtonus,andthat.nohavenow \n",
      "\n",
      "[(28 28%) loss: 1.9160]\n",
      "Wherebetorestoftowouldthesomethemy,asondicestesseetrought,weresamethattianetheedonceforasire,asitakenm \n",
      "\n",
      "[(28 28%) loss: 2.6752]\n",
      "Whavegideneviseofarrose,ihavedyousovethysoveir.buttruchssweseipandgofetomainty.noralltall.mokeinortsen \n",
      "\n",
      "[(29 28%) loss: 2.1556]\n",
      "Whatyourcestingtakeshindear.hayforkingthese,madinchavestrander.selfauthers,ofthetoatarestesyalter,buti \n",
      "\n",
      "[(29 28%) loss: 2.1364]\n",
      "Whathreagead.theslovedingdeath.somylordge,forethle,thomustfealthere,andlivebe,shallfouldwithherremeamo \n",
      "\n",
      "[(30 30%) loss: 1.7903]\n",
      "Whourlancesternojuldicarthehalssonthatbutatathehpaster,howsternevere,grownother'pronandsofhamanmandoft \n",
      "\n",
      "[(30 30%) loss: 2.2852]\n",
      "Wheverdiangeses:inthyintowrands,lethatthatispeturespinesmylord.iplander;ence,andwhichwidown,togenthout \n",
      "\n",
      "[(31 31%) loss: 2.4336]\n",
      "Whytheirmorebether,poosasastostignsofpirmost.goodtheirthank.thathasdoaneeward,whoshourance.tthoughtthe \n",
      "\n",
      "[(31 31%) loss: 2.4413]\n",
      "Whereampanourfears,thathow,thecallaster.thencursfaisingirenisinareheremeheare.whatmayy,thustenceswithe \n",
      "\n",
      "[(32 32%) loss: 2.2079]\n",
      "Wheardie:i'dwiltlybewilltheandthemantrunme.notthemanoledwithatthatmyghenhyours;tellthouchark!butmyduke \n",
      "\n",
      "[(32 32%) loss: 2.1889]\n",
      "Wherebead,andmyk.goandnotbetomakeshallyoumare.heallbetheragesladyens,ello,thererenotthecay!themincontm \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(33 33%) loss: 2.0404]\n",
      "Whousinsayforthedeathereavephere,norsedwouldbetsbeing,ishallselfyourand--hardonotlords,letter,iray,asi \n",
      "\n",
      "[(33 33%) loss: 2.1928]\n",
      "Whisslord;butiganditthemberescontedletistoreashiveanddidone,itforthemanased.mymadysayaftisqueenemi,lig \n",
      "\n",
      "[(34 34%) loss: 2.2879]\n",
      "Whenrytoandclageoflordareswoundasidonder:butyeswarceishetrucioussother,pindyes,doded'sdeartthemasterce \n",
      "\n",
      "[(34 34%) loss: 0.7664]\n",
      "Wheruswerethestofelfandtheking,donesgraagath,theysockandinmy,andyoumindmage,butinowtobro!through,kingl \n",
      "\n",
      "[(35 35%) loss: 2.2551]\n",
      "Whuswannedmither!werevenceloughsomeshoresome,makeroughtitheram,withyourand?ieredosed,thenandthoughstoh \n",
      "\n",
      "[(35 35%) loss: 2.2611]\n",
      "Whereadusconderselfhingpidssinourofourgengtothat,---xeaitemsofherition,aptersedgeathandallaccess,tooth \n",
      "\n",
      "[(36 36%) loss: 1.7744]\n",
      "Whowhislord;andtheyouway,wherebywelceswellatandtheryoursomeohouldschold,wither;andthe,hownottheeshi'll \n",
      "\n",
      "[(36 36%) loss: 1.7294]\n",
      "Whathershallyoucoun'd,ifmystrangatensofmysone?alingeidandhathereadstrivers.whatthygoodnotthee.world.fa \n",
      "\n",
      "[(37 37%) loss: 1.8320]\n",
      "Whathsightandflaterone'sarderedthencetake,andthathewacherwell.whichhevanscand,tellweettheehath,hearthe \n",
      "\n",
      "[(37 37%) loss: 2.1843]\n",
      "Whereabrivedinsofhim,hisdriecy!himabemare.beyou,shalldids;father,sir,fallion;tobechartionmy,theewerese \n",
      "\n",
      "[(38 38%) loss: 1.8905]\n",
      "Whathandaming:liveatthesupt,inthatsaunstwithyoursaprespeed.,whomakelikewesatoretotheyarmydein,butthouw \n",
      "\n",
      "[(38 38%) loss: 1.1747]\n",
      "Who:myblook,ihavehereashing.sir,when,whileherevinthearthatinmythesicinius:whichinthemperiwithsir,witht \n",
      "\n",
      "[(39 39%) loss: 2.0995]\n",
      "Whingofmill,andtheythathavenobrought,simethatisarestearstreatheardtheirdhissack.andheasethefory,thatge \n",
      "\n",
      "[(39 39%) loss: 2.7710]\n",
      "Where'sgoodandlordandclaremwishmyhasheispolid,faidrine:nomordself.goodneme.nown.thewarginent,theyduked \n",
      "\n",
      "[(40 40%) loss: 2.2030]\n",
      "Whimsuchprowst;imenoclardrunt.ogedmentmythee,hebutface,burned.andsay,thregetposeensholdthistignhers,po \n",
      "\n",
      "[(40 40%) loss: 2.0529]\n",
      "Whaskingcompitoftolookthathence,say,forgete,theirservedowll'd;mutbeherace,andtiknanengle.sege,whil;myl \n",
      "\n",
      "[(41 41%) loss: 2.4600]\n",
      "Whimaple,iwordthemono'thefeedmylord,andthoufor?-'terare:awakone,away,asmytillmethe,myletingementlepaig \n",
      "\n",
      "[(41 41%) loss: 1.9306]\n",
      "Whousofrurghingreperis:inthatnotallked.beenthatwumeoftobetheythegall?look:themone.inmykingreamourthere \n",
      "\n",
      "[(42 42%) loss: 2.4929]\n",
      "Whenrybiestion:giveyet,ifyouwillbidestrentiommothandisfairhallswastitprened,ifitakethatihave;butdothot \n",
      "\n",
      "[(42 42%) loss: 2.1713]\n",
      "Whenhisitcomayyoudeadandsiactyourgandstreatsbeyouasspartsoareofthat.sarefore'dops,thehardieantheyagepa \n",
      "\n",
      "[(43 43%) loss: 2.1469]\n",
      "Whistooiningasbeandstosfullainsatate:andhaveningthefile;todeatthesetrousedourman?at,insour,theshalt'sn \n",
      "\n",
      "[(43 43%) loss: 1.1285]\n",
      "Whisprovesbutdidishastthatnow,aswrongswiththeagoddukestingheldbetobation'dhearthe'scourt;henrie,ofuniv \n",
      "\n",
      "[(44 44%) loss: 2.2695]\n",
      "Whowselfwhatthesire,good,good'sbtthem.thoughhimshehase.forathourhavesmelly.tokinghope,notsiknow,byyour \n",
      "\n",
      "[(44 44%) loss: 1.9488]\n",
      "Whenryfatherselfather,hand,asfasterrong.goodhathandcalus:iwillnot.formygapelsedthatsaythereofthroughan \n",
      "\n",
      "[(45 45%) loss: 1.9509]\n",
      "Whalandinmyfordistay.athloldgod,themakebygrood.isterwell,andnot?andter,whoseofthybut,come.andyourgreav \n",
      "\n",
      "[(45 45%) loss: 2.2449]\n",
      "Whengentyour:tocoriofsthegrecteatded.captionarted.here,vold,mydeadbad?theirsarethatisfear;thewillbeldf \n",
      "\n",
      "[(46 46%) loss: 2.4764]\n",
      "Whimranceshereichardfincedlony,andthee,yourprouseshallshall.hekee'ssealeck'sand,myself.they,hethatsoon \n",
      "\n",
      "[(46 46%) loss: 2.8065]\n",
      "Whesabetteringhimaltthesadwithmyhim,thebreathishespeaven,andbrightferslook.foriclareyourfromthather'sw \n",
      "\n",
      "[(47 47%) loss: 0.8617]\n",
      "Whimwithinthesepetrequeenviartionoblelevenyourforromine,whichareyou,tell'shoulditisleverliviscursendin \n",
      "\n",
      "[(47 47%) loss: 2.6899]\n",
      "Whish'dwere,thouweremethatthurthatwowdriem.an-amay,notwhencome'sthatyourselftoroureyouwamastasonthetit \n",
      "\n",
      "[(48 48%) loss: 1.7945]\n",
      "Whatsharce'simelasparciusparehathhensecetaretobeleatethepuliet?mourthee,youhavemorethepomthem.why,lid' \n",
      "\n",
      "[(48 48%) loss: 2.5046]\n",
      "Whearsonoftodisforterland:betoken:but,sirvice.mylord.naboyofmysomeanogauntvingtherelied.whentitisis.il \n",
      "\n",
      "[(49 49%) loss: 1.8315]\n",
      "Whewhopome,putionisald,ifidestheyasionisdiestion,sirsquethatisal'slague,counderhislive,willyou,bies,an \n",
      "\n",
      "[(49 49%) loss: 2.1428]\n",
      "Whereismostincentio:buse,letnotmemore.briedo:forethebooly.--hussomemanres,andthebirashthemanytentoohos \n",
      "\n",
      "[(50 50%) loss: 1.7392]\n",
      "Whenefealperandasour,ihowis-'toch'dcalli'too.idock,--whatthatthereneatywithto'nesmay,themearnows'caste \n",
      "\n",
      "[(50 50%) loss: 1.8280]\n",
      "Whavesoshesweetwouldgodsthetringshesaswellard,andhistly.'grovengethee,menotherfather,sirngess;andand'n \n",
      "\n",
      "[(51 51%) loss: 1.4374]\n",
      "Whearpoorelife,thedibuckingandbackyourgrade.themaster.loods,idonour.brookeintome,andcallhere.compinged \n",
      "\n",
      "[(51 51%) loss: 2.0289]\n",
      "Whatthatmyspeetchard!shallyourshlady.letbuthingbutyouunderethassme,andmyguireofthedangers.heelongedtho \n",
      "\n",
      "[(52 52%) loss: 1.8049]\n",
      "Whehavemusty:stay.theetruchioled,mylord,made.andthemayshall?andtakemestenry;geness,thougherthought,the \n",
      "\n",
      "[(52 52%) loss: 1.9161]\n",
      "Whathergod;whereishesavemoughthearall.iwardtoo,'tis,waridwhatscompet,irabellmayeagain!asyetweareafnowf \n",
      "\n",
      "[(53 53%) loss: 2.2265]\n",
      "Whoursupondissone'mfortue?wasnotupanstherevintofherei'thetheroubobear,andmaslandfrewedisaman:mythouska \n",
      "\n",
      "[(53 53%) loss: 1.5807]\n",
      "Whatsadvantagaintheyhangstreadappessasiwithyousomeep,thereinstheplestasd,andthekingly,oursed,anstand,g \n",
      "\n",
      "[(54 54%) loss: 1.9785]\n",
      "Where'themupondeisofthetillmuchwith.thouaremuchmes'shall'dletthecametoraft.thereforblothenlyclailt.pau \n",
      "\n",
      "[(54 54%) loss: 1.7793]\n",
      "Whimbescondstrisugh,that'stheather:forsuchio,peedfortheminetotissed,theyhourladysad,titedwhichordanged \n",
      "\n",
      "[(55 55%) loss: 1.9521]\n",
      "Whenceinaling.say,lether,iwillself;indayson,myliet'swiththetobeandnewstagexenhim:liceshim;andmadreestr \n",
      "\n",
      "[(55 55%) loss: 2.2408]\n",
      "Whowthesearofsomeofyouarewhy,thatbearofhowats,tothereingso,nomore?---frompart,ifyournoccomionardsthouf \n",
      "\n",
      "[(56 56%) loss: 2.1656]\n",
      "Wheeaveperneling,istheprockengavein,thereforesteapandseefthislovelate,andthehavebol'tabastion,theerine \n",
      "\n",
      "[(56 56%) loss: 2.0756]\n",
      "Wheredidgooddandnotfield,andthenright!youknownoflack;anddsay,sir.herabars:stillofthebai.thatfinthee,li \n",
      "\n",
      "[(57 56%) loss: 2.0645]\n",
      "Whourbalt:lordsinsweels,lattilllefuling,heaven,lacetheytoshouldsomedater:heraligalsomethissisthesehehe \n",
      "\n",
      "[(57 56%) loss: 2.3872]\n",
      "Whereother,yours,whichtheclordssonesjuteafers!whishouldwer'swithsir,willbe!whowswearthands,theyaregone \n",
      "\n",
      "[(58 57%) loss: 1.9294]\n",
      "Whamuphysumped,whosewconsendsedwiththeedangelonibestlort,thymer'sgillandmastaring.andsomes,would,soome \n",
      "\n",
      "[(58 57%) loss: 2.2960]\n",
      "Whimetameweatthetofyouhadservingfart,sir?wherewell?isabarn,fearteasureyou.ather,goodtheedeserve.atofma \n",
      "\n",
      "[(59 59%) loss: 2.1349]\n",
      "Whereisabarducuch'lofminethepriseisthouaranceimfailto'tthenife,'tistillhimparestaintsbrook'd,lethislea \n",
      "\n",
      "[(59 59%) loss: 0.8268]\n",
      "Whowherthereforehishadmywilftheeandhounder:forhisstakeaway,beinghardiesthysasthee.whichyoursirelikeofa \n",
      "\n",
      "[(60 60%) loss: 2.0833]\n",
      "Whisshortofartthen,andinceneniostandme,ifthishadas,ikandavillenameannorwithdeeyou;iamnottowithsare,let \n",
      "\n",
      "[(60 60%) loss: 2.0221]\n",
      "Wheresomurderbrontesswhich,madeinglyayes,ityourend,butwhensigh'dhisbardshephouseibeboutofyou.stoknowba \n",
      "\n",
      "[(61 61%) loss: 2.1068]\n",
      "Whiver'd:thatihaveinmyrivingallyouhax.mineconservinhistofelbo.mygroogy:think'tsthepaltbouts.tormethemr \n",
      "\n",
      "[(61 61%) loss: 2.8141]\n",
      "Whereminess'tisunnept,sonders.howoftofullfare,thelowdischiestthewhombleptingle.shemaynepistobe,hadspos \n",
      "\n",
      "[(62 62%) loss: 1.8900]\n",
      "Whowneareheyou,bewith,wholepassands,asinaplareloacius,bydiethechillofkeep-blesens,thoublesepsenthisabe \n",
      "\n",
      "[(62 62%) loss: 1.7673]\n",
      "Wherelittlemythee,nother,thedukean,diether,whatwarwick.bybreath,myday,thence'seralmen;ihaveattobeadyha \n",
      "\n",
      "[(63 63%) loss: 1.7835]\n",
      "Whelordstrumeswellofherandtheyarenotyourslantheysecondcomes-surest.nowindoverlife.butposedkill'dtomean \n",
      "\n",
      "[(63 63%) loss: 2.1200]\n",
      "Whatwellthoughionorthisamandinhisbe.notmydaveheirsworthoutwill,andtheunticantoregoses,moreherandgented \n",
      "\n",
      "[(64 64%) loss: 0.4222]\n",
      "Whostforthiscourbrother;asthoushallaway!orerescaniohismart.orly,mylord.ihavewedoseddidyoufellasareaty; \n",
      "\n",
      "[(64 64%) loss: 1.8173]\n",
      "Whereupapprance.pailey?whatcallakenoffail,thetrue,toliveme.youofyork!therefortaseyearethenserl;ifyesof \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(65 65%) loss: 2.5215]\n",
      "Whatsweethisbeetchhim.thedrow.giseyoucome.sir?come,criendcallyeadthegod?witharinius:andhe'llofgrock'df \n",
      "\n",
      "[(65 65%) loss: 2.2371]\n",
      "Whowwelltoputityou,doyouwouldthearetheirfriends;i'llasedsweertthisson?thisso.thetatter'sthee,thechildd \n",
      "\n",
      "[(66 66%) loss: 1.7363]\n",
      "Wherestedwardapjess,withbepursesparts,withhereath,ipropenkingrichardredonowmadefather.whattheseparitit \n",
      "\n",
      "[(66 66%) loss: 2.0808]\n",
      "Wherkindied:henries'tisasormysir!tohewarculeturneadsmothreath.themile--sim,andtheir,whatisintheirsepli \n",
      "\n",
      "[(67 67%) loss: 2.5443]\n",
      "Wheboonshethankthebrotherenrustproveshe'shalladvershespeak,suchbe'smer'dthishandsttheel.themse,thereav \n",
      "\n",
      "[(67 67%) loss: 2.2199]\n",
      "Whethatimerhoisethatdome-astheircom'dedward,itbeing.corao'aboutthouhadding.butimerknight,hedaure,strac \n",
      "\n",
      "[(68 68%) loss: 1.5623]\n",
      "Whygravesterssedwithmensenent.givencersofmiserendevershalltheirstandfirsts.i'llknow,isavestathtodidedd \n",
      "\n",
      "[(68 68%) loss: 2.5398]\n",
      "Whewereisethingsosuchardtoo:thisnowshethebeargreat.wouldgrenpriciledsttobecitizerstheneconwithion,him: \n",
      "\n",
      "[(69 69%) loss: 1.0526]\n",
      "Whathalltothisdreather:butthecas'dscourethisbutthensel,thecome,saimsonstodeadoth,broebefaellamble,hiss \n",
      "\n",
      "[(69 69%) loss: 2.2134]\n",
      "Whismakesa.issoulwithlove,theseheheadeyandcallyoulovedmetheearof?cameinfordoftheirouce,plousulet;butth \n",
      "\n",
      "[(70 70%) loss: 2.0410]\n",
      "Whymoneirdeatherbenotsonandmybrothers,iworkingnotconsaptowerd.thirdinhimsome,andcomeinhearserk;andthei \n",
      "\n",
      "[(70 70%) loss: 1.5600]\n",
      "Whenkyourself.mylord.----heistherebeensufjust.-gringmoved,nowle.sell,callisentshusboutthatiswreth.my's \n",
      "\n",
      "[(71 71%) loss: 1.7991]\n",
      "Whenchulsence:thesevietony:there,and,intronghackinghathamlea,mostcontol:toourmonger:thatmysagainstmysi \n",
      "\n",
      "[(71 71%) loss: 1.1992]\n",
      "Whathaveness,down,servetoweitsaygodnessingafeach!brungerofandyouwethedstobegenence.diebedmygoodmostahy \n",
      "\n",
      "[(72 72%) loss: 2.0953]\n",
      "Whebecannotlyman,beyou'llseenflot'shethatlacktoher.isay'nbradonhim.cursed,adidoalingbanss.whichards,ad \n",
      "\n",
      "[(72 72%) loss: 2.2000]\n",
      "Whoughhishame,yourgodier,iknow?'greatorstandagromeo,theytherewhereinenothavinceme.whatison!shavehisthe \n",
      "\n",
      "[(73 73%) loss: 2.3103]\n",
      "Whostohenthe,inkisnessfriend,forinthisonsciarou.whatcunstervatembland,thatyourlife,ifyourforstealth.-- \n",
      "\n",
      "[(73 73%) loss: 2.6394]\n",
      "Whatamorethecity'dasknocking!icannotglopsingintheprincedneme,tothat?hashard!thatwereofhengandthelet--b \n",
      "\n",
      "[(74 74%) loss: 2.1521]\n",
      "Whereyourhisdead.andthenarethewaservantofthemaster:butwhatpresion,andpoorthem'dandtobe?thesehathself.i \n",
      "\n",
      "[(74 74%) loss: 0.8192]\n",
      "Whatlove,sidisetheyspeak.ifforhaveintheyloursiel:butitispasseinthou.thealoftheirstrue?ungersonyourgood \n",
      "\n",
      "[(75 75%) loss: 1.9219]\n",
      "Whisdarthalpeak,our'tsanytouch,--theircomesleep,asshard,braw,mysofthyholm.'tshouldsacter?iwilltheirgod \n",
      "\n",
      "[(75 75%) loss: 2.0060]\n",
      "Whoseaconsereyes:mysondereroftowere'smyland.why,what'smedrant:formyself?oasiredamy.that;isitmysoveryga \n",
      "\n",
      "[(76 76%) loss: 2.3453]\n",
      "Where,andhim,oremeithere:ifandidobesoning!audeforevendoys,andwilldoplese.canshorsenotfangerofsir,and,a \n",
      "\n",
      "[(76 76%) loss: 2.0700]\n",
      "Whavenowingthereadbewithoutistrest'sgrown,tohicknow,mycappertoftheprame,wines!whatknow'sgideartheedete \n",
      "\n",
      "[(77 77%) loss: 1.6066]\n",
      "Whebhatenohenmayofyouandsus,settervance:whichcorionohardion;werelybcies:heerfool;mylavesityoudowyouret \n",
      "\n",
      "[(77 77%) loss: 2.0697]\n",
      "Whimgoodnagoulourgrazelusbethecords,belovester,northembrothishisfresper,gield,andunthousins,beensesins \n",
      "\n",
      "[(78 78%) loss: 2.3030]\n",
      "Whencelyprovenatourwines.thesequirentbesottarenotmylord.sinking.whow,maybeingbutsheknowmustme:seemedhi \n",
      "\n",
      "[(78 78%) loss: 2.8972]\n",
      "Whousate,werenotswithmenenius,goodclumio:margaukestertoyourdothey,inombercurse.with.--whereyoumore,and \n",
      "\n",
      "[(79 79%) loss: 2.4680]\n",
      "Whereinlyenofandandssencemenursedertheknece.youbetochancemight.tothesknow'scharmaloa.assueliebrie.bute \n",
      "\n",
      "[(79 79%) loss: 1.7550]\n",
      "Wheward'sttheclealthisgrievegoodunthepart,andcright:rewar!gearines,whenfromabastle,thyheart,sutalterth \n",
      "\n",
      "[(80 80%) loss: 2.1147]\n",
      "Whesoverywomessoors,strackuntraich'dconderstencedatmen.thewell,whatlowns.iwill.what,thouhasttale,nothi \n",
      "\n",
      "[(80 80%) loss: 2.1180]\n",
      "Whatchamportheringward:forcenseyou'mereath.whatwence,ifall;whenthanthebloodyoudoyouhavetoaglowneralban \n",
      "\n",
      "[(81 81%) loss: 1.6441]\n",
      "Whisseedthejeam,fairers!butthesetwereinbemymejustidyourperr.sourceldshallwarrid!houseonesshewouldtodes \n",
      "\n",
      "[(81 81%) loss: 1.1682]\n",
      "Whencespirtthereceastthecouss,wheresthereisswellthelery,death,thoulord;itrueperdie?topack,snown;buttob \n",
      "\n",
      "[(82 82%) loss: 2.0141]\n",
      "Whereinorcaponthechotherbegurdforty:heart,butkneels'inthespatityhavethatthershategodbylivethekingin;bu \n",
      "\n",
      "[(82 82%) loss: 0.2222]\n",
      "Whatharetisnightthenbasedandtheplotfromtheserackate,butchiomohenmyheremeans;sir,youthanthat:northence. \n",
      "\n",
      "[(83 83%) loss: 1.8377]\n",
      "Whelanded,andhereafineedher;forthink,youwiltbearthineedness,been'shermandtheeandsorheart?butthat'scalu \n",
      "\n",
      "[(83 83%) loss: 1.8431]\n",
      "Where:ofethoufriendsallyandshewasnexsforicontengricharddiditoldgives,andto'timesomeright.hewilltosign' \n",
      "\n",
      "[(84 84%) loss: 1.8539]\n",
      "Whearenot,fort,letwell.haves-kinglyverycallacient:hevingtheperunthecomestermenenelycinemetherefear:who \n",
      "\n",
      "[(84 84%) loss: 1.8946]\n",
      "Whatdediculinethislikefullauther,letthepasstbenot,i'llgod?heisthoucantion.ismethinkshallinhisprinca:in \n",
      "\n",
      "[(85 85%) loss: 1.7980]\n",
      "Whow,letheramesweredoourprospermandhimsecondtowhatertogehereoplethathangalended:andthatparts,conterous \n",
      "\n",
      "[(85 85%) loss: 2.0834]\n",
      "Whead,orhismeansonandsofthim.whitharwouldgroseyes,lookingthee,mygullofrie:dearthatmakemrom!ourdothmell \n",
      "\n",
      "[(86 86%) loss: 1.8786]\n",
      "Wheregraysoino,forking.atings.which,butwillthere'smaning,there'sthemme:yets,oreresmonofgrey:comemylord \n",
      "\n",
      "[(86 86%) loss: 1.9366]\n",
      "Whourgessandficenorwither.isayerer;andsoshallmake'look.and,itsheasaforgethes,doyoubeandtheeous,whatmun \n",
      "\n",
      "[(87 87%) loss: 1.9821]\n",
      "Whevedidwehenryviok,notnearthatinherd.tillheretheyabringsofthewondince.hespeatetheeheandthekingablow?t \n",
      "\n",
      "[(87 87%) loss: 1.7588]\n",
      "Wheaverselvesinthetotherely.butwhotour,asbyyouwalkyouandrevenish,ilieves.thantellyourwithmarcius:where \n",
      "\n",
      "[(88 88%) loss: 2.0146]\n",
      "Whatchedsinginvoliots!prustcomymanserviciofthyricharddow!hothespeace;poosedmyclown,mywellingmade?itmas \n",
      "\n",
      "[(88 88%) loss: 0.7282]\n",
      "Wheirmanomyball,and,there'sbeyouwastedwewilllove.thatdissoulseheart?--less,here;thehorisbothanwithmywi \n",
      "\n",
      "[(89 89%) loss: 2.3369]\n",
      "Whomone,myman'sbeleadandtoalbomybragenot;friends,andnewsofyork,nooveramystershim,andgood,thembedtofiso \n",
      "\n",
      "[(89 89%) loss: 2.4912]\n",
      "Whicerous'd,andmaylive,weit.i'llbedone'sfathench,'er.iwould?thoughts.i'lltheseand:andgive,hall.andhesi \n",
      "\n",
      "[(90 90%) loss: 1.8424]\n",
      "Whehaveprovourstorinewesetismanyinroughte.him,andptomyself.ofthelanenstthoubestrone,onthemouttebe,neve \n",
      "\n",
      "[(90 90%) loss: 0.9563]\n",
      "Whatherfullow,throwsuchamachingforthylight.have'sbeackthythyname,andloveclarychamillowhibliftersewingr \n",
      "\n",
      "[(91 91%) loss: 2.0536]\n",
      "Whewillpeourbegunrotheriguentwithdeneschdearyourfortheiclaudio!'tisatain.stoursuchargeemberdereliebarn \n",
      "\n",
      "[(91 91%) loss: 1.7438]\n",
      "Whomandthsoulsay,tosigniorstarriexder,ansallsomorrisetotosme,thenwill;ithereasstalses,theyallhaved,iba \n",
      "\n",
      "[(92 92%) loss: 2.5533]\n",
      "Whaveisay,andingcrateliewouldheretohave.andtomehismustgrengysuredisparc,sir,hishand,myloves,what'tisbu \n",
      "\n",
      "[(92 92%) loss: 1.5175]\n",
      "Wheart,andtheoreofssentastherefasttruelygremio,apusy.here?where,asforbleedgreadst.allsuchforyourfromar \n",
      "\n",
      "[(93 93%) loss: 2.2606]\n",
      "Wheareinfiresmavemadeamoess.hespoorthecunpieltofromall.lethim:butihavetherethatatthatfirerfromhim.onth \n",
      "\n",
      "[(93 93%) loss: 0.1975]\n",
      "Whatharonius:yetmakessowasheartthanbearblamepreatenttowerecome,tellfehavearenotaninsmakemore.notthen,a \n",
      "\n",
      "[(94 94%) loss: 1.7673]\n",
      "Whathcouronthee,andfromtheseform,andalsofaukingflustduke;wouldinthoushalt.housedtherewewelthee?that'st \n",
      "\n",
      "[(94 94%) loss: 1.5960]\n",
      "Whortetherwarwick,tillashersoftither;inthequeen.ifyouhaveforgodmure,totheydesert.thee,ofwithsici,letre \n",
      "\n",
      "[(95 95%) loss: 2.1432]\n",
      "Whastingstasmiminesland.iwillorshandawaytheirlday,wemedytheming?atereinsmeinsurpowere,intheworld.well, \n",
      "\n",
      "[(95 95%) loss: 2.3767]\n",
      "Wherefore,beaglifeofthydiditroth.shallnot,andheaven,andsuchagain,myme.nowhave;andshenameyoursofe,andth \n",
      "\n",
      "[(96 96%) loss: 2.3833]\n",
      "Whenrybirestance.i'lethenrywithyouness'ds,andthisdogloucestonerance,andinareyouknowmyparghing,mylord,a \n",
      "\n",
      "[(96 96%) loss: 2.0431]\n",
      "Whisthouhareboint?taketheirpostwack'stless?bygiveshouldhearmorefore.butbeing.hathbeen;ihadgives;meinso \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(97 97%) loss: 2.4432]\n",
      "Whatharingofcoriolanus:andyourshichheirad,thenrypart,mymoptay'smoalwhat,yousunconteintiltofleastall,th \n",
      "\n",
      "[(97 97%) loss: 1.6251]\n",
      "Whownownoogeche,thathisplige.howthematilyofthissofyork:hownownownandgragion?whom.now,thatvicthentothem \n",
      "\n",
      "[(98 98%) loss: 2.3944]\n",
      "Whiss,ifthemadashallhermhowthesrunsedamanstthenandherast,thathavethenanourformine-profoutisthyme.andfl \n",
      "\n",
      "[(98 98%) loss: 2.2321]\n",
      "Whespitionelustopformorivistain.andtoherablehisseyetwell.iamforce:her'swould!killbeingbrokesoursthrour \n",
      "\n",
      "[(99 99%) loss: 1.7998]\n",
      "Whouson,eed;yoursedbyandewest,sinderthyorthere:yourhomeconthere--god;asyouarewontentnool:made?forthere \n",
      "\n",
      "[(99 99%) loss: 2.2056]\n",
      "Wherearther,likeinshearttomakeintheheart.heiheandmylibe,orhaveourhouseoffrincentio.ashairyoushallheeda \n",
      "\n",
      "[(100 100%) loss: 2.0953]\n",
      "Whousettoourinperispartthatfore.hewould;thatapassforgoodforchome.inmine;thatbegone?withallwhosit,ors'd \n",
      "\n",
      "[(100 100%) loss: 2.0964]\n",
      "Whavenovenforgodhouse.soreyeto'toppetruchio?hewouldbehapendamandenishesiit.wedeemontheswell.'thatiswit \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "    if torch.cuda.is_available():\n",
    "        decoder.cuda()\n",
    "\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(dataset=TextDataset(),\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "\n",
    "    print(\"Training for %d epochs...\" % n_epochs)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for i, (lines, _) in enumerate(train_loader):\n",
    "            loss = train(lines[0])  # Batch size is 1\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('[(%d %d%%) loss: %.4f]' %\n",
    "                      (epoch, epoch / n_epochs * 100, loss))\n",
    "                print(generate(decoder, 'Wh', 100), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57fe8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f63974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c221bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa19d9eb",
   "metadata": {},
   "source": [
    "# pack_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "92f74fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:48:46.472897Z",
     "start_time": "2024-02-19T05:48:46.463741Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fb8bb90d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:48:46.821421Z",
     "start_time": "2024-02-19T05:48:46.807838Z"
    }
   },
   "outputs": [],
   "source": [
    "seqs = ['ghatmasala', 'nicela', 'chutpakodas']\n",
    "\n",
    "vocab = ['<pad>'] + sorted(list(set(flatten(seqs))))\n",
    "\n",
    "embedding_size = 3\n",
    "\n",
    "embed = torch.nn.Embedding(len(vocab), embedding_size)\n",
    "lstm = torch.nn.LSTM(embedding_size, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75ddb946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:48:47.104657Z",
     "start_time": "2024-02-19T05:48:47.091140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized seqs [[5, 6, 1, 15, 10, 1, 14, 1, 9, 1], [11, 7, 2, 4, 9, 1], [2, 6, 16, 15, 13, 1, 8, 12, 3, 1, 14]]\n",
      "[10, 6, 11]\n"
     ]
    }
   ],
   "source": [
    "vectorized_seqs = [[vocab.index(tok) for tok in seq] for seq in seqs]\n",
    "print('vectorized seqs', vectorized_seqs)\n",
    "\n",
    "print([x for x in map(len, vectorized_seqs)])\n",
    "\n",
    "seq_lengths = torch.LongTensor([x for x in map(len, vectorized_seqs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a151cf0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:48:47.387272Z",
     "start_time": "2024-02-19T05:48:47.371050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_tensor tensor([[ 5,  6,  1, 15, 10,  1, 14,  1,  9,  1,  0],\n",
      "        [11,  7,  2,  4,  9,  1,  0,  0,  0,  0,  0],\n",
      "        [ 2,  6, 16, 15, 13,  1,  8, 12,  3,  1, 14]])\n"
     ]
    }
   ],
   "source": [
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "\n",
    "seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "print(\"seq_tensor\", seq_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "490142c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:48:47.653352Z",
     "start_time": "2024-02-19T05:48:47.637728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_tensor after sorting tensor([[ 2,  6, 16, 15, 13,  1,  8, 12,  3,  1, 14],\n",
      "        [ 5,  6,  1, 15, 10,  1, 14,  1,  9,  1,  0],\n",
      "        [11,  7,  2,  4,  9,  1,  0,  0,  0,  0,  0]])\n",
      "seq_tensor after transporting torch.Size([11, 3]) tensor([[ 2,  5, 11],\n",
      "        [ 6,  6,  7],\n",
      "        [16,  1,  2],\n",
      "        [15, 15,  4],\n",
      "        [13, 10,  9],\n",
      "        [ 1,  1,  1],\n",
      "        [ 8, 14,  0],\n",
      "        [12,  1,  0],\n",
      "        [ 3,  9,  0],\n",
      "        [ 1,  1,  0],\n",
      "        [14,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "print(\"seq_tensor after sorting\", seq_tensor)\n",
    "\n",
    "seq_tensor = seq_tensor.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n",
    "print(\"seq_tensor after transporting\", seq_tensor.size(), seq_tensor.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cb7ee1ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:48:48.127296Z",
     "start_time": "2024-02-19T05:48:48.120313Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_tensor after embedding torch.Size([11, 3, 3]) tensor([[[-0.6058, -1.2204,  0.4031],\n",
      "         [ 0.3229, -1.0837,  0.9217],\n",
      "         [ 0.4397,  0.3536, -0.3924]],\n",
      "\n",
      "        [[ 0.6011,  0.6126, -0.0748],\n",
      "         [ 0.6011,  0.6126, -0.0748],\n",
      "         [ 0.8720,  1.5821,  0.7780]],\n",
      "\n",
      "        [[-1.1258, -0.3440, -0.1232],\n",
      "         [ 1.0625,  0.4981, -1.5001],\n",
      "         [-0.6058, -1.2204,  0.4031]],\n",
      "\n",
      "        [[ 1.5140,  0.9574,  0.9148],\n",
      "         [ 1.5140,  0.9574,  0.9148],\n",
      "         [ 0.3444,  1.0493,  1.7402]],\n",
      "\n",
      "        [[-2.9877,  0.2516,  0.3182],\n",
      "         [ 0.5330,  0.5235,  1.0946],\n",
      "         [-0.8676,  0.2975, -0.6976]],\n",
      "\n",
      "        [[ 1.0625,  0.4981, -1.5001],\n",
      "         [ 1.0625,  0.4981, -1.5001],\n",
      "         [ 1.0625,  0.4981, -1.5001]],\n",
      "\n",
      "        [[-2.1000,  0.9419,  1.6039],\n",
      "         [ 0.1195, -0.3871,  0.6173],\n",
      "         [-0.8361,  0.8061,  0.5474]],\n",
      "\n",
      "        [[-0.0511, -0.2240,  0.9062],\n",
      "         [ 1.0625,  0.4981, -1.5001],\n",
      "         [-0.8361,  0.8061,  0.5474]],\n",
      "\n",
      "        [[ 0.6097, -0.0877, -0.0644],\n",
      "         [-0.8676,  0.2975, -0.6976],\n",
      "         [-0.8361,  0.8061,  0.5474]],\n",
      "\n",
      "        [[ 1.0625,  0.4981, -1.5001],\n",
      "         [ 1.0625,  0.4981, -1.5001],\n",
      "         [-0.8361,  0.8061,  0.5474]],\n",
      "\n",
      "        [[ 0.1195, -0.3871,  0.6173],\n",
      "         [-0.8361,  0.8061,  0.5474],\n",
      "         [-0.8361,  0.8061,  0.5474]]])\n"
     ]
    }
   ],
   "source": [
    "embeded_seq_tensor = embed(seq_tensor)\n",
    "print(\"seq_tensor after embedding\",embeded_seq_tensor.size(), embeded_seq_tensor.data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f615edcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:48:58.949256Z",
     "start_time": "2024-02-19T05:48:58.936603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm output torch.Size([11, 3, 5]) tensor([[[-0.0147,  0.1987,  0.0190,  0.1467, -0.1479],\n",
      "         [-0.0806,  0.1208, -0.0613,  0.0827, -0.1047],\n",
      "         [-0.0882, -0.0659,  0.0325,  0.0088,  0.0346]],\n",
      "\n",
      "        [[-0.0968,  0.0414,  0.0229,  0.0077, -0.0263],\n",
      "         [-0.1508,  0.0109, -0.0024, -0.0138, -0.0017],\n",
      "         [-0.1468, -0.0880, -0.0033, -0.0987,  0.0280]],\n",
      "\n",
      "        [[-0.0292,  0.2070,  0.0846,  0.0966, -0.0941],\n",
      "         [-0.2447, -0.1290,  0.0407,  0.0186,  0.0506],\n",
      "         [-0.0873,  0.1538,  0.0093,  0.0602, -0.0864]],\n",
      "\n",
      "        [[-0.1720,  0.0318, -0.0142, -0.0554, -0.0166],\n",
      "         [-0.2623, -0.1253, -0.0321, -0.0968,  0.0479],\n",
      "         [-0.0887,  0.1855, -0.0321, -0.0992, -0.0891]],\n",
      "\n",
      "        [[-0.0328,  0.4967,  0.0819,  0.0512, -0.2463],\n",
      "         [-0.2062, -0.0206, -0.0431, -0.1191,  0.0267],\n",
      "         [-0.0388,  0.1905,  0.0595,  0.0092, -0.0239]],\n",
      "\n",
      "        [[-0.1609,  0.1150,  0.1299,  0.0099, -0.0183],\n",
      "         [-0.2868, -0.1463,  0.0194,  0.0043,  0.0608],\n",
      "         [-0.1756, -0.0341,  0.0702,  0.0128,  0.0398]],\n",
      "\n",
      "        [[-0.0041,  0.4944,  0.0301, -0.0497, -0.2561],\n",
      "         [-0.2001, -0.0183, -0.0227,  0.0625,  0.0293],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0227,  0.3471,  0.0149, -0.0795, -0.2385],\n",
      "         [-0.2938, -0.1473,  0.0299,  0.0368,  0.0563],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.1041,  0.1838,  0.0218, -0.0458, -0.0793],\n",
      "         [-0.1982, -0.1296,  0.0797,  0.0920,  0.0646],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.2189, -0.0109,  0.0534, -0.0010,  0.0306],\n",
      "         [-0.2691, -0.2024,  0.0692,  0.0655,  0.0645],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.1644,  0.0729, -0.0139,  0.0368, -0.0131],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "packed_input = pack_padded_sequence(embeded_seq_tensor,\n",
    "                                   seq_lengths.cpu().numpy())\n",
    "\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "\n",
    "output, _ = pad_packed_sequence(packed_output)\n",
    "print('lstm output', output.size(), output.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da779a80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T05:49:01.648972Z",
     "start_time": "2024-02-19T05:49:01.633346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last output torch.Size([3, 5]) tensor([[-0.1644,  0.0729, -0.0139,  0.0368, -0.0131],\n",
      "        [-0.2691, -0.2024,  0.0692,  0.0655,  0.0645],\n",
      "        [-0.1756, -0.0341,  0.0702,  0.0128,  0.0398]])\n"
     ]
    }
   ],
   "source": [
    "print('last output', ht[-1].size(), ht[-1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c3d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
