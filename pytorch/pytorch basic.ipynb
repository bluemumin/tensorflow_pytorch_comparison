{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232f24db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T04:08:28.556703Z",
     "start_time": "2024-02-07T04:08:23.710056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n",
      "torch 2.2.0+cpu\n",
      "numpy 1.19.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print('torch', torch.__version__)\n",
    "print('numpy', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5712bc54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T05:26:43.527370Z",
     "start_time": "2024-02-06T05:26:41.043798Z"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets\n",
    "# from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4c7a3",
   "metadata": {},
   "source": [
    "# 기초 operation (tensorflow -> pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a931a",
   "metadata": {},
   "source": [
    "숫자 입력은 torch.tensor([숫자 값1, 숫자값 2]) 이용\n",
    "\n",
    "negative만 neg로 표현되고, 나머지는 동일하게 표현됨.\n",
    "\n",
    "오히려 tensorflow에서 일부 math를 붙여야되는 표현이 없음\n",
    "\n",
    "torch.add : 덧셈, subtract : 뺄셈, multiply : 곱셈, divide : 나눗셈,\n",
    "\n",
    "pow : n(3)=제곱 torch.pow(x, 3), neg : 음수부호 추가\n",
    "\n",
    "abs : 절대값, round : 반올림, ceil : 올림, floor : 내림,\n",
    "\n",
    "square : 제곱, sqrt 제곱근, maximum 최대값, minimum : 최소값\n",
    "\n",
    "cumsum(x, dim=0) : 누적합, cumprod(x, dim=0) : 누적곱, log : 로그값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a187aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T05:40:21.014828Z",
     "start_time": "2024-02-06T05:40:21.002860Z"
    }
   },
   "outputs": [],
   "source": [
    "# 덧셈\n",
    "torch_add = torch.add(torch.tensor([1.0, 2.0]), torch.tensor([3.0, 4.0]))\n",
    "\n",
    "# 뺄셈\n",
    "torch_subtract = torch.subtract(torch.tensor([5.0, 6.0]), torch.tensor([2.0, 1.0]))\n",
    "\n",
    "# 곱셈\n",
    "torch_multiply = torch.multiply(torch.tensor([2.0, 3.0]), torch.tensor([4.0, 5.0]))\n",
    "\n",
    "# 나눗셈\n",
    "torch_divide = torch.divide(torch.tensor([10.0, 15.0]), torch.tensor([2.0, 3.0]))\n",
    "\n",
    "# n-제곱\n",
    "torch_pow = torch.pow(torch.tensor([2.0, 3.0]), 3)\n",
    "\n",
    "# 음수 부호\n",
    "torch_negative = torch.neg(torch.tensor([4.0, -5.0]))\n",
    "\n",
    "# 절대값\n",
    "torch_abs = torch.abs(torch.tensor([-2.0, 3.0]))\n",
    "\n",
    "# 부호\n",
    "torch_sign = torch.sign(torch.tensor([-2.0, 3.0]))\n",
    "\n",
    "# 반올림\n",
    "torch_round = torch.round(torch.tensor([1.6, 2.4]))\n",
    "\n",
    "# 올림\n",
    "torch_ceil = torch.ceil(torch.tensor([1.6, 2.4]))\n",
    "\n",
    "# 내림\n",
    "torch_floor = torch.floor(torch.tensor([1.6, 2.4]))\n",
    "\n",
    "# 제곱\n",
    "torch_square = torch.square(torch.tensor([2.0, 3.0]))\n",
    "\n",
    "# 제곱근\n",
    "torch_sqrt = torch.sqrt(torch.tensor([4.0, 9.0]))\n",
    "\n",
    "#최대값\n",
    "torch_maximum = torch.maximum(torch.tensor([1.0, 2.0]), torch.tensor([3.0, 1.0]))\n",
    "\n",
    "# 최소값\n",
    "torch_minimum = torch.minimum(torch.tensor([1.0, 2.0]), torch.tensor([3.0, 1.0]))\n",
    "\n",
    "\n",
    "x = torch.tensor([2.0, 3.0, 4.0])\n",
    "\n",
    "result_cumsum = torch.cumsum(x, dim=0) # 누적합\n",
    "result_cumprod = torch.cumprod(x, dim=0) #누적곱\n",
    "result_log = torch.log(x) #로그값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ea80e9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T06:09:54.528317Z",
     "start_time": "2024-02-06T06:09:54.521081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "node1 = torch.tensor(3.0, dtype=torch.float32)\n",
    "node2 = torch.tensor(4.0, dtype=torch.float32)\n",
    "\n",
    "def forward():\n",
    "    return node1 + node2\n",
    "\n",
    "out_a = forward()\n",
    "print(out_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3ad8351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T06:10:10.574324Z",
     "start_time": "2024-02-06T06:10:10.569337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e20291b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T06:10:58.099995Z",
     "start_time": "2024-02-06T06:10:58.094011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7., 9.])\n"
     ]
    }
   ],
   "source": [
    "node3 = torch.tensor([3.0,4.0], dtype=torch.float32)\n",
    "node4 = torch.tensor([4.0,5.0], dtype=torch.float32)\n",
    "\n",
    "def adder(a,b):\n",
    "    return a + b\n",
    "\n",
    "print(adder(node3, node4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3c45a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T06:11:27.520267Z",
     "start_time": "2024-02-06T06:11:27.513298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5.])\n",
      "tensor([3., 4.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.maximum(node3, node4))\n",
    "\n",
    "print(torch.minimum(node3, node4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297ffa1",
   "metadata": {},
   "source": [
    "# gradient (PyTorchZeroToAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285a9b6",
   "metadata": {},
   "source": [
    "## manual gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "60981606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:55:20.467169Z",
     "start_time": "2024-02-06T08:55:20.458579Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (before training) 4 8.0\n",
      "\tgrad:  1.0 2.0 -0.8\n",
      "\tgrad:  2.0 4.0 -3.14\n",
      "\tgrad:  3.0 6.0 -6.49\n",
      "progress: 0 w= 1.7 loss= 0.79\n",
      "\tgrad:  1.0 2.0 -0.59\n",
      "\tgrad:  2.0 4.0 -2.32\n",
      "\tgrad:  3.0 6.0 -4.8\n",
      "progress: 1 w= 1.78 loss= 0.43\n",
      "\tgrad:  1.0 2.0 -0.44\n",
      "\tgrad:  2.0 4.0 -1.71\n",
      "\tgrad:  3.0 6.0 -3.55\n",
      "progress: 2 w= 1.84 loss= 0.24\n",
      "\tgrad:  1.0 2.0 -0.32\n",
      "\tgrad:  2.0 4.0 -1.27\n",
      "\tgrad:  3.0 6.0 -2.62\n",
      "progress: 3 w= 1.88 loss= 0.13\n",
      "\tgrad:  1.0 2.0 -0.24\n",
      "\tgrad:  2.0 4.0 -0.94\n",
      "\tgrad:  3.0 6.0 -1.94\n",
      "progress: 4 w= 1.91 loss= 0.07\n",
      "predicted score (after training) 4 hours of studying :  7.646606335189437\n"
     ]
    }
   ],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.6\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "def loss(y_pred, y_val):\n",
    "    return (y_pred - y_val) ** 2\n",
    "\n",
    "def gradient(x, y): # d_loss/d_w\n",
    "    return 2 * x * (x * w - y)\n",
    "\n",
    "print(\"Prediction (before training)\",  4, forward(5))\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01 * grad\n",
    "        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
    "        epoch_loss = loss(forward(x_val), y_val)\n",
    "    \n",
    "    print('progress:', epoch, \"w=\", round(w,2), \"loss=\", round(epoch_loss,2))\n",
    "    \n",
    "print(\"predicted score (after training)\", \"4 hours of studying : \", forward(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433a02e",
   "metadata": {},
   "source": [
    "## auto_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b62152e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:55:29.448478Z",
     "start_time": "2024-02-06T08:55:29.427708Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (before training) 4.0 6.400000095367432\n",
      "\tgrad:  1.0 2.0 -0.7999999523162842\n",
      "\tgrad:  2.0 4.0 -3.1359996795654297\n",
      "\tgrad:  3.0 6.0 -6.491518020629883\n",
      "Epoch: 0 | Loss: 1.1705502271652222\n",
      "\tgrad:  1.0 2.0 -0.591449499130249\n",
      "\tgrad:  2.0 4.0 -2.3184823989868164\n",
      "\tgrad:  3.0 6.0 -4.799260139465332\n",
      "Epoch: 1 | Loss: 0.6398026943206787\n",
      "\tgrad:  1.0 2.0 -0.43726587295532227\n",
      "\tgrad:  2.0 4.0 -1.7140817642211914\n",
      "\tgrad:  3.0 6.0 -3.5481491088867188\n",
      "Epoch: 2 | Loss: 0.3497045040130615\n",
      "\tgrad:  1.0 2.0 -0.3232758045196533\n",
      "\tgrad:  2.0 4.0 -1.2672414779663086\n",
      "\tgrad:  3.0 6.0 -2.6231889724731445\n",
      "Epoch: 3 | Loss: 0.1911422312259674\n",
      "\tgrad:  1.0 2.0 -0.23900175094604492\n",
      "\tgrad:  2.0 4.0 -0.9368867874145508\n",
      "\tgrad:  3.0 6.0 -1.9393558502197266\n",
      "Epoch: 4 | Loss: 0.10447502881288528\n",
      "Prediction (after training) 4 7.6466064453125\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "w = torch.tensor([1.6], requires_grad=True)\n",
    "\n",
    "print(\"Prediction (before training)\",  4.0, forward(4.0).item())\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        y_pred = forward(x_val)\n",
    "        epoch_loss = loss(y_pred, y_val)\n",
    "        epoch_loss.backward() #requires_grad로 backward 수행 가능\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n",
    "        w.data = w.data - 0.01 * w.grad.item()\n",
    "        \n",
    "        w.grad.data.zero_() # Manually zero the gradients after updating weights\n",
    "    \n",
    "    print(f\"Epoch: {epoch} | Loss: {epoch_loss.item()}\")\n",
    "    \n",
    "print(\"Prediction (after training)\",  4, forward(4).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc7927",
   "metadata": {},
   "source": [
    "# cost function (tensorflow -> pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8928e669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T06:46:58.818793Z",
     "start_time": "2024-02-06T06:46:58.803812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |     74.667\n",
      "-2.429 |     54.857\n",
      "-1.857 |     38.095\n",
      "-1.286 |     24.381\n",
      "-0.714 |     13.714\n",
      "-0.143 |     6.0952\n",
      " 0.429 |     1.5238\n",
      " 1.000 |        0.0\n",
      " 1.571 |     1.5238\n",
      " 2.143 |     6.0952\n",
      " 2.714 |     13.714\n",
      " 3.286 |     24.381\n",
      " 3.857 |     38.095\n",
      " 4.429 |     54.857\n",
      " 5.000 |     74.667\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "X = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "Y = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "\n",
    "def cost_func(W, X, Y):\n",
    "    c = 0\n",
    "    for ii in range(len(X)):\n",
    "        c += (W * X[ii] - Y[ii]) ** 2  # 제곱 누적합\n",
    "    return c / len(X)  # 평균\n",
    "\n",
    "# 실제 W 값에 따른 cost 값 변화\n",
    "for feed_W in np.linspace(-3, 5, num=15):\n",
    "    feed_W = torch.tensor(feed_W, dtype=torch.float32)\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    print(\"{:6.3f} | {:10.5}\".format(feed_W.item(), curr_cost.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b07b454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T06:48:36.278547Z",
     "start_time": "2024-02-06T06:48:36.263215Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "alpha = 0.01\n",
    "gradient = torch.mean((W * X - Y) * X)  # alpha 제외한 식\n",
    "descent = W - alpha * gradient  # alpha를 뒤의 식에 곱함.\n",
    "W = descent.clone().detach()  # W를 업데이트함. (PyTorch에서는 assign 대신 직접 대입)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5baf2bb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T06:49:34.155259Z",
     "start_time": "2024-02-06T06:49:34.080866Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 17521.0000 |  46.375000\n",
      "   10 |  3684.7576 |  22.169115\n",
      "   20 |   775.0278 |  11.068727\n",
      "   30 |   163.1182 |   5.978286\n",
      "   40 |    34.4350 |   3.643899\n",
      "   50 |     7.3732 |   2.573390\n",
      "   60 |     1.6822 |   2.082474\n",
      "   70 |     0.4854 |   1.857349\n",
      "   80 |     0.2337 |   1.754110\n",
      "   90 |     0.1808 |   1.706767\n",
      "  100 |     0.1696 |   1.685056\n",
      "  110 |     0.1673 |   1.675099\n",
      "  120 |     0.1668 |   1.670534\n",
      "  130 |     0.1667 |   1.668440\n",
      "  140 |     0.1667 |   1.667480\n",
      "  150 |     0.1667 |   1.667040\n",
      "  160 |     0.1667 |   1.666838\n",
      "  170 |     0.1667 |   1.666745\n",
      "  180 |     0.1667 |   1.666703\n",
      "  190 |     0.1667 |   1.666683\n",
      "  200 |     0.1667 |   1.666674\n",
      "  210 |     0.1667 |   1.666670\n",
      "  220 |     0.1667 |   1.666668\n",
      "  230 |     0.1667 |   1.666667\n",
      "  240 |     0.1667 |   1.666667\n",
      "  250 |     0.1667 |   1.666667\n",
      "  260 |     0.1667 |   1.666667\n",
      "  270 |     0.1667 |   1.666667\n",
      "  280 |     0.1667 |   1.666667\n",
      "  290 |     0.1667 |   1.666667\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "x_data = torch.tensor([1., 2., 3., 4.])\n",
    "y_data = torch.tensor([1., 3., 5., 7.])\n",
    "\n",
    "W = torch.tensor([50.0], requires_grad=True)  # 임의 값 1개 설정\n",
    "for step in range(300):\n",
    "    hyp = W * x_data\n",
    "    cost = torch.mean((hyp - y_data)**2)\n",
    "\n",
    "    alpha = 0.01\n",
    "    gradient = torch.mean((W * x_data - y_data) * x_data)\n",
    "    descent = W - alpha * gradient\n",
    "    W.data = descent.clone().detach() \n",
    "    #descent 복사본 생성 -> 그래디언트 계속 추적 중 \n",
    "    # -> detach()로 추적 중단. 파라미터 업데이트 할 때, 새로운 텐서에 대한 추적 방지.\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.item(), W.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12d2f7",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc07e3c",
   "metadata": {},
   "source": [
    "## in poytorchzeroall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fdcee938",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T09:13:28.877799Z",
     "start_time": "2024-02-06T09:13:28.582503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 39.7831916809082 \n",
      "Epoch: 100 | Loss: 0.20084184408187866 \n",
      "Epoch: 200 | Loss: 0.047225479036569595 \n",
      "Epoch: 300 | Loss: 0.011104447767138481 \n",
      "Epoch: 400 | Loss: 0.002611098112538457 \n",
      "Prediction (after training) 4 7.971309185028076\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1,1) #one input and one output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "hour_var = torch.tensor([[4.0]])\n",
    "y_pred = model(hour_var)\n",
    "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa0160",
   "metadata": {},
   "source": [
    "## in tensorflow -> pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7512bf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T06:19:52.190465Z",
     "start_time": "2024-02-06T06:19:52.180492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.8967, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "y_train = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "\n",
    "# 초기 값 지정\n",
    "W = torch.tensor(2.9, requires_grad=True) #기울기를 추적할것인지 나타내는 매개변수\n",
    "b = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "# 가설 함수\n",
    "hyp = x_train * W + b\n",
    "\n",
    "# 비용 함수\n",
    "cost = torch.mean((hyp - y_train)**2)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f18e9bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T06:19:52.627299Z",
     "start_time": "2024-02-06T06:19:52.561528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|     2.703|     0.414| 20.896669\n",
      "   10|     1.608|  -0.05965|  1.999508\n",
      "   20|     1.268|   -0.2013|  0.199676\n",
      "   30|     1.161|   -0.2407|  0.027861\n",
      "   40|     1.126|   -0.2486|  0.011085\n",
      "   50|     1.114|   -0.2469|  0.009091\n",
      "   60|     1.108|   -0.2423|  0.008524\n",
      "   70|     1.105|    -0.237|  0.008110\n",
      "   80|     1.102|   -0.2315|  0.007727\n",
      "   90|     1.099|    -0.226|  0.007364\n"
     ]
    }
   ],
   "source": [
    "# 학습률\n",
    "lr = 0.01\n",
    "\n",
    "# 반복 학습\n",
    "for ii in range(100):\n",
    "    # 가설 함수\n",
    "    hyp = W * x_train + b\n",
    "\n",
    "    # 비용 함수\n",
    "    cost = torch.mean((hyp - y_train)**2)\n",
    "\n",
    "    # 그래디언트 계산 autograd.grad\n",
    "    W_grad, b_grad = torch.autograd.grad(cost, [W, b])\n",
    "\n",
    "    # 파라미터 업데이트 #직접 업데이트\n",
    "    #  with torch.no_grad(): PyTorch에서 파라미터를 업데이트할 때 그래디언트를 추적하지 않도록 설정하는 블록.\n",
    "    # 모델의 파라미터 값을 테스트하거나 평가할 때는 그래디언트를 추적할 필요가 없음.\n",
    "    with torch.no_grad(): \n",
    "        W -= lr * W_grad\n",
    "        b -= lr * b_grad\n",
    "\n",
    "    if ii % 10 == 0:\n",
    "        print(\"{:5}|{:10.4}|{:10.4}|{:10.6f}\".format(ii, W.item(), b.item(), cost.item()))\n",
    "        \n",
    "print(W * 5 + b)\n",
    "print(W * 2.5 + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84783d85",
   "metadata": {},
   "source": [
    "## 참고용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "772df57a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T06:23:47.297896Z",
     "start_time": "2024-02-06T06:23:47.180137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15.783783 [[-0.37167037]] [-0.6065724]\n",
      "20 0.14310804 [[0.8703934]] [-0.0600373]\n",
      "40 0.0012983615 [[0.98861474]] [-0.00788943]\n",
      "60 1.2526984e-05 [[0.9998268]] [-0.00282183]\n",
      "80 7.996239e-07 [[1.0008515]] [-0.002242]\n",
      "100 6.300895e-07 [[1.0009084]] [-0.00209402]\n",
      "120 5.714221e-07 [[1.0008748]] [-0.00199151]\n",
      "140 5.1887304e-07 [[1.0008342]] [-0.00189737]\n",
      "160 4.7132872e-07 [[1.0007957]] [-0.00180818]\n",
      "180 4.281654e-07 [[1.0007583]] [-0.00172331]\n",
      "200 3.8887822e-07 [[1.0007225]] [-0.0016425]\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# X and Y data\n",
    "x_train = [[1], [2], [3]]\n",
    "y_train = [[1], [2], [3]]\n",
    "X = Variable(torch.Tensor(x_train))\n",
    "Y = Variable(torch.Tensor(y_train))\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "model = torch.nn.Linear(1, 1, bias=True)\n",
    "\n",
    "#cost criterion\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Minimize\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for step in range(201):\n",
    "    optimizer.zero_grad()\n",
    "    # Our hypothesis\n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost.data.numpy(), model.weight.data.numpy(), model.bias.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa0b9a",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514cfd96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T01:17:11.517887Z",
     "start_time": "2024-02-07T01:17:10.952029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 | Loss: 1.2255\n",
      "Epoch 101/1000 | Loss: 0.6416\n",
      "Epoch 201/1000 | Loss: 0.6060\n",
      "Epoch 301/1000 | Loss: 0.5825\n",
      "Epoch 401/1000 | Loss: 0.5608\n",
      "Epoch 501/1000 | Loss: 0.5405\n",
      "Epoch 601/1000 | Loss: 0.5217\n",
      "Epoch 701/1000 | Loss: 0.5040\n",
      "Epoch 801/1000 | Loss: 0.4876\n",
      "Epoch 901/1000 | Loss: 0.4722\n",
      "\n",
      "Let's predict the hours need to score above 50%\n",
      "==================================================\n",
      "Prediction after 1 hour of training: 0.3829 | Above 50%: False\n",
      "Prediction after 7 hours of training: 0.9700 | Above 50%: True\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Training data and ground truth\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_data = torch.tensor([[0.], [0.], [1.], [1.]])\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# After training\n",
    "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
    "hour_var = model(torch.tensor([[1.0]]))\n",
    "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
    "hour_var = model(torch.tensor([[7.0]]))\n",
    "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192d49c",
   "metadata": {},
   "source": [
    "## using diabetes data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa58b8",
   "metadata": {},
   "source": [
    "### normal version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe3578d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T01:29:12.357598Z",
     "start_time": "2024-02-07T01:29:12.222296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n",
      "Epoch 1/100 | Loss: 0.8613\n",
      "Epoch 11/100 | Loss: 0.8436\n",
      "Epoch 21/100 | Loss: 0.8272\n",
      "Epoch 31/100 | Loss: 0.8122\n",
      "Epoch 41/100 | Loss: 0.7984\n",
      "Epoch 51/100 | Loss: 0.7857\n",
      "Epoch 61/100 | Loss: 0.7740\n",
      "Epoch 71/100 | Loss: 0.7633\n",
      "Epoch 81/100 | Loss: 0.7535\n",
      "Epoch 91/100 | Loss: 0.7445\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter = ',', dtype=np.float32)\n",
    "\n",
    "x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "y_data = torch.from_numpy(xy[:, [-1]])\n",
    "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8,6)\n",
    "        self.l2 = torch.nn.Linear(6,4)\n",
    "        self.l3 = torch.nn.Linear(4,1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred  = self.sigmoid(self.l3(out2))\n",
    "        return y_pred \n",
    "    \n",
    "model = Model()\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}/100 | Loss: {loss.item():.4f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d5321",
   "metadata": {},
   "source": [
    "### dataset loader version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02815ee8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T02:14:43.616343Z",
     "start_time": "2024-02-07T02:14:43.539717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch: 1 | Loss: 0.6886\n",
      "Epoch 1 | Batch: 6 | Loss: 0.6863\n",
      "Epoch 1 | Batch: 11 | Loss: 0.6872\n",
      "Epoch 1 | Batch: 16 | Loss: 0.6876\n",
      "Epoch 1 | Batch: 21 | Loss: 0.6883\n",
      "Epoch 2 | Batch: 1 | Loss: 0.6830\n",
      "Epoch 2 | Batch: 6 | Loss: 0.6791\n",
      "Epoch 2 | Batch: 11 | Loss: 0.6804\n",
      "Epoch 2 | Batch: 16 | Loss: 0.6689\n",
      "Epoch 2 | Batch: 21 | Loss: 0.6747\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=32,\n",
    "                         shuffle=True, num_workers=0,\n",
    "                          #pin_memory=True, timeout=10\n",
    "                         )\n",
    "\n",
    "#5-1-1에서 사용한 모델 코드 그대로 사용.\n",
    "model = Model()\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for epoch in range(2):\n",
    "    for ii, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        y_pred = model(inputs)\n",
    "        \n",
    "        loss = criterion(y_pred, labels)\n",
    "        if ii % 5 == 0:\n",
    "            print(f'Epoch {epoch + 1} | Batch: {ii+1} | Loss: {loss.item():.4f}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc9623",
   "metadata": {},
   "source": [
    "# softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f893a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T04:36:11.898240Z",
     "start_time": "2024-02-07T04:36:11.880858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1: 0.3567\n",
      "Loss2: 2.3026\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([1, 0, 0])\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
    "print(f'Loss1: {np.sum(-Y * np.log(Y_pred1)):.4f}')\n",
    "print(f'Loss2: {np.sum(-Y * np.log(Y_pred2)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73fa561a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T04:42:42.422834Z",
     "start_time": "2024-02-07T04:42:42.414719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Loss1: 0.4170 \n",
      "PyTorch Loss2: 1.8406\n",
      "Y_pred1: 0\n",
      "Y_pred2: 1\n"
     ]
    }
   ],
   "source": [
    "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.tensor([0], requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred2 = torch.tensor([[0.5, 2.0, 0.3]])\n",
    "\n",
    "# 크로스 엔트로피 손실 계산\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(f'PyTorch Loss1: {l1.item():.4f} \\nPyTorch Loss2: {l2.item():.4f}')\n",
    "print(f'Y_pred1: {torch.argmax(Y_pred1, dim=1).item()}')  # 가장 높은 로짓을 갖는 클래스 선택\n",
    "print(f'Y_pred2: {torch.argmax(Y_pred2, dim=1).item()}')  # 가장 높은 로짓을 갖는 클래스 선택\n",
    "\n",
    "# print(f'Y_pred1: {torch.max(Y_pred1.data, 1)[1].item()}')\n",
    "# print(f'Y_pred2: {torch.max(Y_pred2.data, 1)[1].item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5a24cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T04:44:49.988321Z",
     "start_time": "2024-02-07T04:44:49.971030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss1 : 0.496635 \n",
      "batch loss2: 1.2389\n"
     ]
    }
   ],
   "source": [
    "Y = torch.tensor([2, 0, 1], requires_grad=False)\n",
    "\n",
    "Y_pred1 = torch.tensor([[0.1, 0.2, 0.9],\n",
    "                  [1.1, 0.1, 0.2],\n",
    "                  [0.2, 2.1, 0.1]])\n",
    "\n",
    "Y_pred2 = torch.tensor([[0.8, 0.2, 0.3],\n",
    "                  [0.2, 0.3, 0.5],\n",
    "                  [0.2, 0.2, 0.5]])\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(f'Batch loss1 : {l1.item():4f} \\nbatch loss2: {l2.data:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144e3bc",
   "metadata": {},
   "source": [
    "## using mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4089c714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T05:27:26.206544Z",
     "start_time": "2024-02-07T05:27:26.201587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MNIST Model on cpu\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Training settings\n",
    "batch_size = 256\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb6c9ffb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T05:27:26.651180Z",
     "start_time": "2024-02-07T05:27:26.560591Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train = True,\n",
    "                              transform = transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor()\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "179af453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T05:27:27.123151Z",
     "start_time": "2024-02-07T05:27:27.118164Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72b9f7ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T05:30:04.131841Z",
     "start_time": "2024-02-07T05:27:27.479312Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.301599\n",
      "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.302497\n",
      "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.300550\n",
      "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.309551\n",
      "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.298288\n",
      "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.305865\n",
      "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.300394\n",
      "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.298675\n",
      "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.298419\n",
      "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.300614\n",
      "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.297462\n",
      "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.297824\n",
      "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.299121\n",
      "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.298132\n",
      "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.297862\n",
      "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.298193\n",
      "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.299576\n",
      "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.294394\n",
      "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.290569\n",
      "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 2.301976\n",
      "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 2.296663\n",
      "Train Epoch: 1 | Batch Status: 53760/60000 (89%) | Loss: 2.297996\n",
      "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 2.296736\n",
      "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 2.295124\n",
      "Training time: 0m 12s\n",
      "===========================\n",
      "Test set: Average loss: 0.0092, Accuracy: 1311/10000 (13%)\n",
      "testing time: 0m 14s\n",
      "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 2.290129\n",
      "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 2.293703\n",
      "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 2.290844\n",
      "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 2.287204\n",
      "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 2.291272\n",
      "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 2.291653\n",
      "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 2.294217\n",
      "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 2.287987\n",
      "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 2.287676\n",
      "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 2.284128\n",
      "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 2.287158\n",
      "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 2.288033\n",
      "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 2.282482\n",
      "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 2.285003\n",
      "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 2.276928\n",
      "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 2.281589\n",
      "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 2.281295\n",
      "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 2.277537\n",
      "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 2.277109\n",
      "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 2.278669\n",
      "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 2.275052\n",
      "Train Epoch: 2 | Batch Status: 53760/60000 (89%) | Loss: 2.274924\n",
      "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 2.274372\n",
      "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 2.276230\n",
      "Training time: 0m 17s\n",
      "===========================\n",
      "Test set: Average loss: 0.0091, Accuracy: 2182/10000 (22%)\n",
      "testing time: 0m 19s\n",
      "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 2.270854\n",
      "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 2.270855\n",
      "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 2.270729\n",
      "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 2.268780\n",
      "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 2.257547\n",
      "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 2.265144\n",
      "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 2.257198\n",
      "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 2.261031\n",
      "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 2.257650\n",
      "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 2.249557\n",
      "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 2.245029\n",
      "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 2.241036\n",
      "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 2.236666\n",
      "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 2.230699\n",
      "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 2.234864\n",
      "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 2.222006\n",
      "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 2.219646\n",
      "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 2.208868\n",
      "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 2.208371\n",
      "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 2.187310\n",
      "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 2.166345\n",
      "Train Epoch: 3 | Batch Status: 53760/60000 (89%) | Loss: 2.164737\n",
      "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 2.157332\n",
      "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 2.125572\n",
      "Training time: 0m 15s\n",
      "===========================\n",
      "Test set: Average loss: 0.0085, Accuracy: 3504/10000 (35%)\n",
      "testing time: 0m 17s\n",
      "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 2.123755\n",
      "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 2.085020\n",
      "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 2.085641\n",
      "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 2.048595\n",
      "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 2.025062\n",
      "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 2.000913\n",
      "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 2.004439\n",
      "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 1.902948\n",
      "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 1.913360\n",
      "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 1.859592\n",
      "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 1.834706\n",
      "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 1.830626\n",
      "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 1.725624\n",
      "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 1.690877\n",
      "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 1.662453\n",
      "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 1.599468\n",
      "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 1.592659\n",
      "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 1.520160\n",
      "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 1.539430\n",
      "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 1.492199\n",
      "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 1.469807\n",
      "Train Epoch: 4 | Batch Status: 53760/60000 (89%) | Loss: 1.414405\n",
      "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 1.386842\n",
      "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 1.259568\n",
      "Training time: 0m 15s\n",
      "===========================\n",
      "Test set: Average loss: 0.0051, Accuracy: 5978/10000 (60%)\n",
      "testing time: 0m 17s\n",
      "Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 1.291104\n",
      "Train Epoch: 5 | Batch Status: 2560/60000 (4%) | Loss: 1.247359\n",
      "Train Epoch: 5 | Batch Status: 5120/60000 (9%) | Loss: 1.208224\n",
      "Train Epoch: 5 | Batch Status: 7680/60000 (13%) | Loss: 1.222177\n",
      "Train Epoch: 5 | Batch Status: 10240/60000 (17%) | Loss: 1.118892\n",
      "Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 1.105199\n",
      "Train Epoch: 5 | Batch Status: 15360/60000 (26%) | Loss: 1.114573\n",
      "Train Epoch: 5 | Batch Status: 17920/60000 (30%) | Loss: 1.031048\n",
      "Train Epoch: 5 | Batch Status: 20480/60000 (34%) | Loss: 1.048340\n",
      "Train Epoch: 5 | Batch Status: 23040/60000 (38%) | Loss: 1.042659\n",
      "Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 1.087805\n",
      "Train Epoch: 5 | Batch Status: 28160/60000 (47%) | Loss: 1.067372\n",
      "Train Epoch: 5 | Batch Status: 30720/60000 (51%) | Loss: 0.985707\n",
      "Train Epoch: 5 | Batch Status: 33280/60000 (55%) | Loss: 0.998192\n",
      "Train Epoch: 5 | Batch Status: 35840/60000 (60%) | Loss: 0.994045\n",
      "Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 1.032221\n",
      "Train Epoch: 5 | Batch Status: 40960/60000 (68%) | Loss: 0.924399\n",
      "Train Epoch: 5 | Batch Status: 43520/60000 (72%) | Loss: 1.039882\n",
      "Train Epoch: 5 | Batch Status: 46080/60000 (77%) | Loss: 0.851183\n",
      "Train Epoch: 5 | Batch Status: 48640/60000 (81%) | Loss: 0.845017\n",
      "Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.852278\n",
      "Train Epoch: 5 | Batch Status: 53760/60000 (89%) | Loss: 0.885003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 | Batch Status: 56320/60000 (94%) | Loss: 0.884717\n",
      "Train Epoch: 5 | Batch Status: 58880/60000 (98%) | Loss: 0.761799\n",
      "Training time: 0m 15s\n",
      "===========================\n",
      "Test set: Average loss: 0.0033, Accuracy: 7447/10000 (74%)\n",
      "testing time: 0m 17s\n",
      "Train Epoch: 6 | Batch Status: 0/60000 (0%) | Loss: 0.860306\n",
      "Train Epoch: 6 | Batch Status: 2560/60000 (4%) | Loss: 0.753798\n",
      "Train Epoch: 6 | Batch Status: 5120/60000 (9%) | Loss: 0.800101\n",
      "Train Epoch: 6 | Batch Status: 7680/60000 (13%) | Loss: 0.862491\n",
      "Train Epoch: 6 | Batch Status: 10240/60000 (17%) | Loss: 0.759862\n",
      "Train Epoch: 6 | Batch Status: 12800/60000 (21%) | Loss: 0.704415\n",
      "Train Epoch: 6 | Batch Status: 15360/60000 (26%) | Loss: 0.786005\n",
      "Train Epoch: 6 | Batch Status: 17920/60000 (30%) | Loss: 0.729218\n",
      "Train Epoch: 6 | Batch Status: 20480/60000 (34%) | Loss: 0.762951\n",
      "Train Epoch: 6 | Batch Status: 23040/60000 (38%) | Loss: 0.760521\n",
      "Train Epoch: 6 | Batch Status: 25600/60000 (43%) | Loss: 0.712234\n",
      "Train Epoch: 6 | Batch Status: 28160/60000 (47%) | Loss: 0.635592\n",
      "Train Epoch: 6 | Batch Status: 30720/60000 (51%) | Loss: 0.689408\n",
      "Train Epoch: 6 | Batch Status: 33280/60000 (55%) | Loss: 0.747825\n",
      "Train Epoch: 6 | Batch Status: 35840/60000 (60%) | Loss: 0.660894\n",
      "Train Epoch: 6 | Batch Status: 38400/60000 (64%) | Loss: 0.679857\n",
      "Train Epoch: 6 | Batch Status: 40960/60000 (68%) | Loss: 0.540243\n",
      "Train Epoch: 6 | Batch Status: 43520/60000 (72%) | Loss: 0.710765\n",
      "Train Epoch: 6 | Batch Status: 46080/60000 (77%) | Loss: 0.618208\n",
      "Train Epoch: 6 | Batch Status: 48640/60000 (81%) | Loss: 0.693749\n",
      "Train Epoch: 6 | Batch Status: 51200/60000 (85%) | Loss: 0.628558\n",
      "Train Epoch: 6 | Batch Status: 53760/60000 (89%) | Loss: 0.662828\n",
      "Train Epoch: 6 | Batch Status: 56320/60000 (94%) | Loss: 0.519096\n",
      "Train Epoch: 6 | Batch Status: 58880/60000 (98%) | Loss: 0.571172\n",
      "Training time: 0m 15s\n",
      "===========================\n",
      "Test set: Average loss: 0.0023, Accuracy: 8331/10000 (83%)\n",
      "testing time: 0m 17s\n",
      "Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.603678\n",
      "Train Epoch: 7 | Batch Status: 2560/60000 (4%) | Loss: 0.482682\n",
      "Train Epoch: 7 | Batch Status: 5120/60000 (9%) | Loss: 0.535932\n",
      "Train Epoch: 7 | Batch Status: 7680/60000 (13%) | Loss: 0.580749\n",
      "Train Epoch: 7 | Batch Status: 10240/60000 (17%) | Loss: 0.589930\n",
      "Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.633757\n",
      "Train Epoch: 7 | Batch Status: 15360/60000 (26%) | Loss: 0.616121\n",
      "Train Epoch: 7 | Batch Status: 17920/60000 (30%) | Loss: 0.534472\n",
      "Train Epoch: 7 | Batch Status: 20480/60000 (34%) | Loss: 0.592435\n",
      "Train Epoch: 7 | Batch Status: 23040/60000 (38%) | Loss: 0.521089\n",
      "Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.504578\n",
      "Train Epoch: 7 | Batch Status: 28160/60000 (47%) | Loss: 0.582961\n",
      "Train Epoch: 7 | Batch Status: 30720/60000 (51%) | Loss: 0.576018\n",
      "Train Epoch: 7 | Batch Status: 33280/60000 (55%) | Loss: 0.567309\n",
      "Train Epoch: 7 | Batch Status: 35840/60000 (60%) | Loss: 0.428034\n",
      "Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.511743\n",
      "Train Epoch: 7 | Batch Status: 40960/60000 (68%) | Loss: 0.564069\n",
      "Train Epoch: 7 | Batch Status: 43520/60000 (72%) | Loss: 0.454127\n",
      "Train Epoch: 7 | Batch Status: 46080/60000 (77%) | Loss: 0.507411\n",
      "Train Epoch: 7 | Batch Status: 48640/60000 (81%) | Loss: 0.403593\n",
      "Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.501693\n",
      "Train Epoch: 7 | Batch Status: 53760/60000 (89%) | Loss: 0.495047\n",
      "Train Epoch: 7 | Batch Status: 56320/60000 (94%) | Loss: 0.423115\n",
      "Train Epoch: 7 | Batch Status: 58880/60000 (98%) | Loss: 0.403921\n",
      "Training time: 0m 15s\n",
      "===========================\n",
      "Test set: Average loss: 0.0018, Accuracy: 8686/10000 (87%)\n",
      "testing time: 0m 17s\n",
      "Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.450330\n",
      "Train Epoch: 8 | Batch Status: 2560/60000 (4%) | Loss: 0.468235\n",
      "Train Epoch: 8 | Batch Status: 5120/60000 (9%) | Loss: 0.484073\n",
      "Train Epoch: 8 | Batch Status: 7680/60000 (13%) | Loss: 0.413481\n",
      "Train Epoch: 8 | Batch Status: 10240/60000 (17%) | Loss: 0.459258\n",
      "Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.390777\n",
      "Train Epoch: 8 | Batch Status: 15360/60000 (26%) | Loss: 0.512317\n",
      "Train Epoch: 8 | Batch Status: 17920/60000 (30%) | Loss: 0.482145\n",
      "Train Epoch: 8 | Batch Status: 20480/60000 (34%) | Loss: 0.625200\n",
      "Train Epoch: 8 | Batch Status: 23040/60000 (38%) | Loss: 0.449078\n",
      "Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.407686\n",
      "Train Epoch: 8 | Batch Status: 28160/60000 (47%) | Loss: 0.521769\n",
      "Train Epoch: 8 | Batch Status: 30720/60000 (51%) | Loss: 0.384184\n",
      "Train Epoch: 8 | Batch Status: 33280/60000 (55%) | Loss: 0.490951\n",
      "Train Epoch: 8 | Batch Status: 35840/60000 (60%) | Loss: 0.390510\n",
      "Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.471719\n",
      "Train Epoch: 8 | Batch Status: 40960/60000 (68%) | Loss: 0.396530\n",
      "Train Epoch: 8 | Batch Status: 43520/60000 (72%) | Loss: 0.420921\n",
      "Train Epoch: 8 | Batch Status: 46080/60000 (77%) | Loss: 0.399761\n",
      "Train Epoch: 8 | Batch Status: 48640/60000 (81%) | Loss: 0.570176\n",
      "Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.414589\n",
      "Train Epoch: 8 | Batch Status: 53760/60000 (89%) | Loss: 0.394925\n",
      "Train Epoch: 8 | Batch Status: 56320/60000 (94%) | Loss: 0.487664\n",
      "Train Epoch: 8 | Batch Status: 58880/60000 (98%) | Loss: 0.382860\n",
      "Training time: 0m 15s\n",
      "===========================\n",
      "Test set: Average loss: 0.0017, Accuracy: 8796/10000 (88%)\n",
      "testing time: 0m 17s\n",
      "Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.403228\n",
      "Train Epoch: 9 | Batch Status: 2560/60000 (4%) | Loss: 0.437129\n",
      "Train Epoch: 9 | Batch Status: 5120/60000 (9%) | Loss: 0.467520\n",
      "Train Epoch: 9 | Batch Status: 7680/60000 (13%) | Loss: 0.399673\n",
      "Train Epoch: 9 | Batch Status: 10240/60000 (17%) | Loss: 0.314317\n",
      "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.361014\n",
      "Train Epoch: 9 | Batch Status: 15360/60000 (26%) | Loss: 0.440232\n",
      "Train Epoch: 9 | Batch Status: 17920/60000 (30%) | Loss: 0.358154\n",
      "Train Epoch: 9 | Batch Status: 20480/60000 (34%) | Loss: 0.544382\n",
      "Train Epoch: 9 | Batch Status: 23040/60000 (38%) | Loss: 0.389201\n",
      "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.338148\n",
      "Train Epoch: 9 | Batch Status: 28160/60000 (47%) | Loss: 0.385019\n",
      "Train Epoch: 9 | Batch Status: 30720/60000 (51%) | Loss: 0.352150\n",
      "Train Epoch: 9 | Batch Status: 33280/60000 (55%) | Loss: 0.334101\n",
      "Train Epoch: 9 | Batch Status: 35840/60000 (60%) | Loss: 0.470332\n",
      "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.363154\n",
      "Train Epoch: 9 | Batch Status: 40960/60000 (68%) | Loss: 0.345192\n",
      "Train Epoch: 9 | Batch Status: 43520/60000 (72%) | Loss: 0.362613\n",
      "Train Epoch: 9 | Batch Status: 46080/60000 (77%) | Loss: 0.442867\n",
      "Train Epoch: 9 | Batch Status: 48640/60000 (81%) | Loss: 0.393149\n",
      "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.406050\n",
      "Train Epoch: 9 | Batch Status: 53760/60000 (89%) | Loss: 0.384841\n",
      "Train Epoch: 9 | Batch Status: 56320/60000 (94%) | Loss: 0.327758\n",
      "Train Epoch: 9 | Batch Status: 58880/60000 (98%) | Loss: 0.397813\n",
      "Training time: 0m 18s\n",
      "===========================\n",
      "Test set: Average loss: 0.0015, Accuracy: 8948/10000 (89%)\n",
      "testing time: 0m 20s\n",
      "Total Time: 2m 37s\n",
      "Model was trained on cpu!\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(784,520)\n",
    "        self.l2 = torch.nn.Linear(520,320)\n",
    "        self.l3 = torch.nn.Linear(320,240)\n",
    "        self.l4 = torch.nn.Linear(240,120)\n",
    "        self.l5 = torch.nn.Linear(120,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784) # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "    \n",
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                batch_idx * len(data), \n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item())\n",
    "                 )\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1,10):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'testing time: {m:.0f}m {s:.0f}s')\n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
